{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from NN_Thesis.nn_classes import *\n",
    "from NN_Thesis.trainer import *\n",
    "from NN_Thesis.helper_functions import *\n",
    "from NN_Thesis.adapters import *\n",
    "from NN_Thesis.helper_functions import *\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "\n",
    "\n",
    "from torchvision.transforms import transforms\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "import random\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import BinaryNet.models as BNN_models\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from NN_Thesis.dataset import cifar_n_dataset,get_split_dataset,cifar_100_split\n",
    "from torch.utils.data import DataLoader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_channels(data):\n",
    "    #We have a nxCxWxH array\n",
    "    d = data\n",
    "    d = torch.flatten(data,2,-1)\n",
    "    mean= torch.mean(d,dim = [0,2])\n",
    "    std = torch.std(d,dim = [0,2])\n",
    "    return mean,std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.5034, 0.4852, 0.4427]), tensor([0.2684, 0.2562, 0.2766]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train80_path = r'data\\cifar-100\\cifar-80\\train'\n",
    "test80_path =r'data\\cifar-100\\cifar-80\\train'\n",
    "cifar80_path = r'data\\cifar-100\\cifar-80'\n",
    "\n",
    "\n",
    "train_80 = cifar_100_split(cifar80_path)\n",
    "# train_80=apply_transforms(train_80)\n",
    "\n",
    "test_80 = cifar_100_split(cifar80_path,train = False)\n",
    "# test_80=apply_transforms(test_80,train = False)\n",
    "mean,std,=normalize_channels(train_80.data)\n",
    "mean,std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar20_path = r'data\\cifar-100\\cifar-20'\n",
    "\n",
    "\n",
    "train_20 = cifar_100_split(cifar20_path)\n",
    "test_20 = cifar_100_split(cifar20_path,train = False)\n",
    "\n",
    "\n",
    "transform_train = transforms.Compose([  #resises the image so it can be perfect for our model.\n",
    "                                      transforms.RandomCrop(32, padding=4),\n",
    "                                      transforms.RandomHorizontalFlip(), # FLips the image w.r.t horizontal axis\n",
    "                                      transforms.Normalize(mean,std) #Normalize all the images\n",
    "                               ])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "transform_test = transforms.Compose([  #resises the image so it can be perfect for our model.\n",
    "                                      transforms.Normalize(mean,std) #Normalize all the images\n",
    "                               ])\n",
    "\n",
    "train_20.transform = transform_train\n",
    "test_20.transform = transform_test\n",
    "train20_DL = DataLoader(train_20,batch_size=64,shuffle= True,num_workers=4)\n",
    "test20_DL = DataLoader(test_20,batch_size=64,shuffle= False,num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class placeholder(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_elem(net:nn.Module,per_layer = False,show=False):\n",
    "    bin_total = 0\n",
    "    for n,m in net.named_modules():\n",
    "        if isinstance(m,(BinarizeConv2d,BinarizeLinear)):\n",
    "            bin_total+= sum([p.numel() for p in m.parameters()])\n",
    "    print(bin_total)\n",
    "    total = 0\n",
    "    for name,p in net.named_parameters():\n",
    "        if per_layer:\n",
    "            print(name,p.numel())\n",
    "        total += p.numel()\n",
    "    print(total)\n",
    "    print(total - bin_total)\n",
    "    return total,bin_total,total-bin_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44000.0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "11_000_000*32/8000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4330165\n",
      "4336415\n",
      "6250\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.036060496692725326"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def weight_pct(weights,classes = 5):\n",
    "    x,y,z = total_elem(resnet18_adapt(classes))\n",
    "\n",
    "    base = (y+z*32)/8000\n",
    "\n",
    "    return weights*32/8000/base\n",
    "\n",
    "weight_pct(5105)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'SavedModels\\UniAdapt_Cifar100_Base\\ResNet18_BNN_Baseline_2023-05-09 13-19-37\\ResNet18_BNN_Baseline_best_acc.pth'\n",
    "\n",
    "state = torch.load(path,map_location='cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No lr scheduler\n",
      "Optimizer Set to New: {'state': {}, 'param_groups': [{'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': False, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54]}]}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:xkw7cs7o) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Current Best Acc</td><td>▁▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█████████████████████</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>epoch time (s)</td><td>▁█▇▆▆▆▆▇▆▇▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆</td></tr><tr><td>lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_accuracy</td><td>▁▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇██▇███████████████████</td></tr><tr><td>test_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>training_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Best Accuracy</td><td>0.7495</td></tr><tr><td>Current Best Acc</td><td>0.7495</td></tr><tr><td>Total Time (hours)</td><td>0.55309</td></tr><tr><td>epoch</td><td>75</td></tr><tr><td>epoch time (s)</td><td>13.10636</td></tr><tr><td>lr</td><td>0.001</td></tr><tr><td>test_accuracy</td><td>0.744</td></tr><tr><td>test_loss</td><td>0.78967</td></tr><tr><td>training_loss</td><td>0.78588</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">avid-snowflake-1</strong>: <a href=\"https://wandb.ai/johnny_suu/FINAL_CIFAR100_Regular/runs/xkw7cs7o\" target=\"_blank\">https://wandb.ai/johnny_suu/FINAL_CIFAR100_Regular/runs/xkw7cs7o</a><br/>Synced 5 W&B file(s), 1 media file(s), 1 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230512_120434-xkw7cs7o\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:xkw7cs7o). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6783628137cf49af95baa3738b235243",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016933333332902596, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\John Su\\Downloads\\SydneyUni\\2022\\thesis\\Thesis\\wandb\\run-20230512_122122-82abokms</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/johnny_suu/FINAL_CIFAR100_Regular/runs/82abokms\" target=\"_blank\">flowing-wave-2</a></strong> to <a href=\"https://wandb.ai/johnny_suu/FINAL_CIFAR100_Regular\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "Project Name: FINAL_CIFAR100_Regular, Run Name Head Only \n",
      "\n",
      "\n",
      "Run Start : 2023-05-12 12-21-22\n",
      "start_epoch : 0\n",
      "initial_lr : 0.001\n",
      "batch_size : 64\n",
      "epochs : 75\n",
      "epoch_chkpts : []\n",
      "device : cuda:0\n",
      "criterion : CrossEntropyLoss()\n",
      "optimizer : Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: False\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "seed : 123\n",
      "model_architecture : <class 'NN_Thesis.nn_classes.BNN_Resnet_UniAdapt'>\n",
      "binerised_training : True\n",
      "Number of Elements : 4380700\n",
      "Initial accuracy:\n",
      "Test Accuracy : 4.2%, Test Loss: 34.74617124666833\n",
      "best_acc.pth saved!\n",
      "Test Accuracy : 4.7%, Test Loss: 34.512225687503815\n",
      "best_acc.pth saved!\n",
      "epoch: 1 average loss: 2.019\n",
      "Test Accuracy : 59.0%, Test Loss: 1.5337960049510002\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 7.84 seconds\n",
      "epoch: 2 average loss: 1.563\n",
      "Test Accuracy : 62.6%, Test Loss: 1.4103223346173763\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 7.77 seconds\n",
      "epoch: 3 average loss: 1.473\n",
      "Test Accuracy : 62.1%, Test Loss: 1.348892755806446\n",
      "Epoch Time (Training + Test) = 7.73 seconds\n",
      "epoch: 4 average loss: 1.432\n",
      "Test Accuracy : 63.8%, Test Loss: 1.3146961443126202\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 7.76 seconds\n",
      "epoch: 5 average loss: 1.391\n",
      "Test Accuracy : 63.0%, Test Loss: 1.2853534929454327\n",
      "Epoch Time (Training + Test) = 7.74 seconds\n",
      "epoch: 6 average loss: 1.368\n",
      "Test Accuracy : 64.6%, Test Loss: 1.2420081421732903\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 7.76 seconds\n",
      "epoch: 7 average loss: 1.339\n",
      "Test Accuracy : 63.4%, Test Loss: 1.2226638346910477\n",
      "Epoch Time (Training + Test) = 7.81 seconds\n",
      "epoch: 8 average loss: 1.318\n",
      "Test Accuracy : 64.6%, Test Loss: 1.1969427280128002\n",
      "Epoch Time (Training + Test) = 7.70 seconds\n",
      "epoch: 9 average loss: 1.314\n",
      "Test Accuracy : 62.4%, Test Loss: 1.2325945310294628\n",
      "Epoch Time (Training + Test) = 7.74 seconds\n",
      "epoch: 10 average loss: 1.306\n",
      "Test Accuracy : 64.0%, Test Loss: 1.1991678141057491\n",
      "Epoch Time (Training + Test) = 7.75 seconds\n",
      "epoch: 11 average loss: 1.299\n",
      "Test Accuracy : 64.3%, Test Loss: 1.1889881119132042\n",
      "Epoch Time (Training + Test) = 7.76 seconds\n",
      "epoch: 12 average loss: 1.299\n",
      "Test Accuracy : 63.8%, Test Loss: 1.1952635198831558\n",
      "Epoch Time (Training + Test) = 7.68 seconds\n",
      "epoch: 13 average loss: 1.273\n",
      "Test Accuracy : 64.6%, Test Loss: 1.1785005163401365\n",
      "Epoch Time (Training + Test) = 7.75 seconds\n",
      "epoch: 14 average loss: 1.271\n",
      "Test Accuracy : 63.9%, Test Loss: 1.1709367986768484\n",
      "Epoch Time (Training + Test) = 7.66 seconds\n",
      "epoch: 15 average loss: 1.262\n",
      "Test Accuracy : 65.0%, Test Loss: 1.1448384411633015\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 7.77 seconds\n",
      "epoch: 16 average loss: 1.262\n",
      "Test Accuracy : 62.2%, Test Loss: 1.1725906357169151\n",
      "Epoch Time (Training + Test) = 7.73 seconds\n",
      "epoch: 17 average loss: 1.251\n",
      "Test Accuracy : 64.1%, Test Loss: 1.1537408269941807\n",
      "Epoch Time (Training + Test) = 7.68 seconds\n",
      "epoch: 18 average loss: 1.254\n",
      "Test Accuracy : 63.5%, Test Loss: 1.1462257504463196\n",
      "Epoch Time (Training + Test) = 7.71 seconds\n",
      "epoch: 19 average loss: 1.257\n",
      "Test Accuracy : 64.3%, Test Loss: 1.1343591157346964\n",
      "Epoch Time (Training + Test) = 7.69 seconds\n",
      "epoch: 20 average loss: 1.243\n",
      "Test Accuracy : 63.6%, Test Loss: 1.1579628307372332\n",
      "Epoch Time (Training + Test) = 7.91 seconds\n",
      "epoch: 21 average loss: 1.250\n",
      "Test Accuracy : 65.1%, Test Loss: 1.1361801996827126\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 7.75 seconds\n",
      "epoch: 22 average loss: 1.232\n",
      "Test Accuracy : 64.3%, Test Loss: 1.125417321920395\n",
      "Epoch Time (Training + Test) = 7.60 seconds\n",
      "epoch: 23 average loss: 1.231\n",
      "Test Accuracy : 64.2%, Test Loss: 1.1352930888533592\n",
      "Epoch Time (Training + Test) = 7.65 seconds\n",
      "epoch: 24 average loss: 1.245\n",
      "Test Accuracy : 63.3%, Test Loss: 1.1358540914952755\n",
      "Epoch Time (Training + Test) = 7.60 seconds\n",
      "epoch: 25 average loss: 1.223\n",
      "Test Accuracy : 63.1%, Test Loss: 1.1282536257058382\n",
      "Epoch Time (Training + Test) = 7.69 seconds\n",
      "epoch: 26 average loss: 1.230\n",
      "Test Accuracy : 65.0%, Test Loss: 1.1273980792611837\n",
      "Epoch Time (Training + Test) = 7.61 seconds\n",
      "epoch: 27 average loss: 1.223\n",
      "Test Accuracy : 63.7%, Test Loss: 1.1151935122907162\n",
      "Epoch Time (Training + Test) = 7.65 seconds\n",
      "epoch: 28 average loss: 1.221\n",
      "Test Accuracy : 63.3%, Test Loss: 1.1383970994502306\n",
      "Epoch Time (Training + Test) = 7.61 seconds\n",
      "epoch: 29 average loss: 1.227\n",
      "Test Accuracy : 63.8%, Test Loss: 1.1137226819992065\n",
      "Epoch Time (Training + Test) = 7.66 seconds\n",
      "epoch: 30 average loss: 1.232\n",
      "Test Accuracy : 64.5%, Test Loss: 1.134462982416153\n",
      "Epoch Time (Training + Test) = 7.51 seconds\n",
      "epoch: 31 average loss: 1.221\n",
      "Test Accuracy : 64.6%, Test Loss: 1.1142495535314083\n",
      "Epoch Time (Training + Test) = 7.61 seconds\n",
      "epoch: 32 average loss: 1.222\n",
      "Test Accuracy : 64.5%, Test Loss: 1.1174789164215326\n",
      "Epoch Time (Training + Test) = 7.60 seconds\n",
      "epoch: 33 average loss: 1.211\n",
      "Test Accuracy : 64.2%, Test Loss: 1.1050544623285532\n",
      "Epoch Time (Training + Test) = 7.65 seconds\n",
      "epoch: 34 average loss: 1.226\n",
      "Test Accuracy : 64.6%, Test Loss: 1.0993141736835241\n",
      "Epoch Time (Training + Test) = 7.55 seconds\n",
      "epoch: 35 average loss: 1.222\n",
      "Test Accuracy : 64.2%, Test Loss: 1.1240524053573608\n",
      "Epoch Time (Training + Test) = 7.59 seconds\n",
      "epoch: 36 average loss: 1.224\n",
      "Test Accuracy : 64.2%, Test Loss: 1.1264020204544067\n",
      "Epoch Time (Training + Test) = 7.59 seconds\n",
      "epoch: 37 average loss: 1.217\n",
      "Test Accuracy : 63.8%, Test Loss: 1.115593634545803\n",
      "Epoch Time (Training + Test) = 7.65 seconds\n",
      "epoch: 38 average loss: 1.220\n",
      "Test Accuracy : 64.5%, Test Loss: 1.1128999795764685\n",
      "Epoch Time (Training + Test) = 7.64 seconds\n",
      "epoch: 39 average loss: 1.217\n",
      "Test Accuracy : 63.2%, Test Loss: 1.1141850780695677\n",
      "Epoch Time (Training + Test) = 7.54 seconds\n",
      "epoch: 40 average loss: 1.216\n",
      "Test Accuracy : 62.1%, Test Loss: 1.1376057472079992\n",
      "Epoch Time (Training + Test) = 7.63 seconds\n",
      "epoch: 41 average loss: 1.219\n",
      "Test Accuracy : 64.4%, Test Loss: 1.115000780671835\n",
      "Epoch Time (Training + Test) = 7.58 seconds\n",
      "epoch: 42 average loss: 1.217\n",
      "Test Accuracy : 65.3%, Test Loss: 1.078127022832632\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 7.65 seconds\n",
      "epoch: 43 average loss: 1.206\n",
      "Test Accuracy : 63.7%, Test Loss: 1.1061156429350376\n",
      "Epoch Time (Training + Test) = 7.64 seconds\n",
      "epoch: 44 average loss: 1.232\n",
      "Test Accuracy : 63.5%, Test Loss: 1.0935564525425434\n",
      "Epoch Time (Training + Test) = 7.62 seconds\n",
      "epoch: 45 average loss: 1.214\n",
      "Test Accuracy : 65.5%, Test Loss: 1.0970370825380087\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 7.67 seconds\n",
      "epoch: 46 average loss: 1.215\n",
      "Test Accuracy : 62.8%, Test Loss: 1.1286773812025785\n",
      "Epoch Time (Training + Test) = 7.63 seconds\n",
      "epoch: 47 average loss: 1.202\n",
      "Test Accuracy : 62.5%, Test Loss: 1.1099844984710217\n",
      "Epoch Time (Training + Test) = 7.65 seconds\n",
      "epoch: 48 average loss: 1.205\n",
      "Test Accuracy : 64.9%, Test Loss: 1.0985945221036673\n",
      "Epoch Time (Training + Test) = 7.65 seconds\n",
      "epoch: 49 average loss: 1.219\n",
      "Test Accuracy : 63.2%, Test Loss: 1.116857174783945\n",
      "Epoch Time (Training + Test) = 7.52 seconds\n",
      "epoch: 50 average loss: 1.205\n",
      "Test Accuracy : 65.5%, Test Loss: 1.086138455197215\n",
      "Epoch Time (Training + Test) = 7.63 seconds\n",
      "epoch: 51 average loss: 1.214\n",
      "Test Accuracy : 64.2%, Test Loss: 1.1060306876897812\n",
      "Epoch Time (Training + Test) = 7.59 seconds\n",
      "epoch: 52 average loss: 1.209\n",
      "Test Accuracy : 64.7%, Test Loss: 1.1073377188295126\n",
      "Epoch Time (Training + Test) = 7.66 seconds\n",
      "epoch: 53 average loss: 1.208\n",
      "Test Accuracy : 65.5%, Test Loss: 1.0980645343661308\n",
      "Epoch Time (Training + Test) = 7.69 seconds\n",
      "epoch: 54 average loss: 1.211\n",
      "Test Accuracy : 65.0%, Test Loss: 1.122750060632825\n",
      "Epoch Time (Training + Test) = 7.62 seconds\n",
      "epoch: 55 average loss: 1.212\n",
      "Test Accuracy : 63.6%, Test Loss: 1.1092046331614256\n",
      "Epoch Time (Training + Test) = 7.61 seconds\n",
      "epoch: 56 average loss: 1.212\n",
      "Test Accuracy : 63.0%, Test Loss: 1.125928122550249\n",
      "Epoch Time (Training + Test) = 7.64 seconds\n",
      "epoch: 57 average loss: 1.201\n",
      "Test Accuracy : 62.8%, Test Loss: 1.140610558912158\n",
      "Epoch Time (Training + Test) = 7.64 seconds\n",
      "epoch: 58 average loss: 1.211\n",
      "Test Accuracy : 65.1%, Test Loss: 1.107467582449317\n",
      "Epoch Time (Training + Test) = 7.67 seconds\n",
      "epoch: 59 average loss: 1.208\n",
      "Test Accuracy : 64.4%, Test Loss: 1.0956959836184978\n",
      "Epoch Time (Training + Test) = 7.61 seconds\n",
      "epoch: 60 average loss: 1.221\n",
      "Test Accuracy : 64.4%, Test Loss: 1.087446365505457\n",
      "Epoch Time (Training + Test) = 7.64 seconds\n",
      "epoch: 61 average loss: 1.199\n",
      "Test Accuracy : 62.6%, Test Loss: 1.1264616772532463\n",
      "Epoch Time (Training + Test) = 7.67 seconds\n",
      "epoch: 62 average loss: 1.214\n",
      "Test Accuracy : 65.2%, Test Loss: 1.0676141064614058\n",
      "Epoch Time (Training + Test) = 7.61 seconds\n",
      "epoch: 63 average loss: 1.212\n",
      "Test Accuracy : 65.0%, Test Loss: 1.1103855948895216\n",
      "Epoch Time (Training + Test) = 7.64 seconds\n",
      "epoch: 64 average loss: 1.207\n",
      "Test Accuracy : 63.7%, Test Loss: 1.1063313968479633\n",
      "Epoch Time (Training + Test) = 7.55 seconds\n",
      "epoch: 65 average loss: 1.210\n",
      "Test Accuracy : 63.8%, Test Loss: 1.121987959370017\n",
      "Epoch Time (Training + Test) = 7.58 seconds\n",
      "epoch: 66 average loss: 1.205\n",
      "Test Accuracy : 65.2%, Test Loss: 1.0949033591896296\n",
      "Epoch Time (Training + Test) = 7.62 seconds\n",
      "epoch: 67 average loss: 1.201\n",
      "Test Accuracy : 63.4%, Test Loss: 1.0860960744321346\n",
      "Epoch Time (Training + Test) = 7.57 seconds\n",
      "epoch: 68 average loss: 1.211\n",
      "Test Accuracy : 64.3%, Test Loss: 1.0807004198431969\n",
      "Epoch Time (Training + Test) = 7.56 seconds\n",
      "epoch: 69 average loss: 1.193\n",
      "Test Accuracy : 64.0%, Test Loss: 1.1024896763265133\n",
      "Epoch Time (Training + Test) = 7.65 seconds\n",
      "epoch: 70 average loss: 1.197\n",
      "Test Accuracy : 65.1%, Test Loss: 1.0858027394860983\n",
      "Epoch Time (Training + Test) = 7.52 seconds\n",
      "epoch: 71 average loss: 1.210\n",
      "Test Accuracy : 64.1%, Test Loss: 1.1082544606179\n",
      "Epoch Time (Training + Test) = 7.61 seconds\n",
      "epoch: 72 average loss: 1.203\n",
      "Test Accuracy : 64.8%, Test Loss: 1.102026367560029\n",
      "Epoch Time (Training + Test) = 7.57 seconds\n",
      "epoch: 73 average loss: 1.204\n",
      "Test Accuracy : 64.6%, Test Loss: 1.1093490235507488\n",
      "Epoch Time (Training + Test) = 7.66 seconds\n",
      "epoch: 74 average loss: 1.207\n",
      "Test Accuracy : 64.4%, Test Loss: 1.067682994529605\n",
      "Epoch Time (Training + Test) = 7.61 seconds\n",
      "epoch: 75 average loss: 1.203\n",
      "Test Accuracy : 63.9%, Test Loss: 1.1318990606814623\n",
      "Epoch Time (Training + Test) = 7.66 seconds\n",
      "Data Saved to Head Only.csv\n",
      "Finished Training: \n",
      "Total Time 0.318898 hours\n",
      " Average Time Per Epoch 15.31 seconds\n"
     ]
    }
   ],
   "source": [
    "BNN = BNN_Resnet_UniAdapt(80)\n",
    "BNN.load_state_dict(state,strict=False)\n",
    "BNN.freeze()\n",
    "BNN.cuda()\n",
    "\n",
    "trainer = Trainer(model = BNN,seed = 123,model_name = f'Head Only',project_name = 'FINAL_CIFAR100_Regular',classes = train_20.classes,binarise= True)\n",
    "m = trainer.model\n",
    "m.to(trainer.device)\n",
    "m.fc = BinarizeLinear(320,20)\n",
    "m.bn3 = nn.BatchNorm1d(20)\n",
    "# for layer in [m.fc,m.bn3]:\n",
    "#     for p in layer.parameters():\n",
    "#         p.requires_grad = True\n",
    "#     layer.train()\n",
    "trainer.lr = 1e-3\n",
    "trainer.batch_size = 64\n",
    "trainer.epochs =75\n",
    "trainer.epoch_chkpts = []\n",
    "trainer.start_epoch = 0\n",
    "trainer.set_scheduler(None)\n",
    "# adpt = getattr(m,after_layer)[-1]\n",
    "# print(adpt)\n",
    "# print(m.fc.weight[0,0])\n",
    "# params = nn.ModuleList([m.fc,m.bn3,adpt]).parameters()\n",
    "trainer.set_optimizer(torch.optim.Adam)\n",
    "trainer.train(train20_DL,test20_DL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No lr scheduler\n",
      "Optimizer Set to New: {'state': {}, 'param_groups': [{'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': False, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54]}]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjohnny_suu\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\John Su\\Downloads\\SydneyUni\\2022\\thesis\\Thesis\\wandb\\run-20230512_120434-xkw7cs7o</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/johnny_suu/FINAL_CIFAR100_Regular/runs/xkw7cs7o\" target=\"_blank\">avid-snowflake-1</a></strong> to <a href=\"https://wandb.ai/johnny_suu/FINAL_CIFAR100_Regular\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "Project Name: FINAL_CIFAR100_Regular, Run Name Regular_Finetune \n",
      "\n",
      "\n",
      "Run Start : 2023-05-12 12-04-32\n",
      "start_epoch : 0\n",
      "initial_lr : 0.001\n",
      "batch_size : 64\n",
      "epochs : 75\n",
      "epoch_chkpts : []\n",
      "device : cuda:0\n",
      "criterion : CrossEntropyLoss()\n",
      "optimizer : Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: False\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "seed : 123\n",
      "model_architecture : <class 'NN_Thesis.nn_classes.BNN_Resnet_UniAdapt'>\n",
      "binerised_training : True\n",
      "Number of Elements : 4380700\n",
      "Initial accuracy:\n",
      "Test Accuracy : 6.2%, Test Loss: 30.395600713742006\n",
      "best_acc.pth saved!\n",
      "Test Accuracy : 6.2%, Test Loss: 30.302964508533478\n",
      "epoch: 1 average loss: 1.993\n",
      "Test Accuracy : 57.1%, Test Loss: 1.5776673816144466\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 14.16 seconds\n",
      "epoch: 2 average loss: 1.547\n",
      "Test Accuracy : 62.2%, Test Loss: 1.4067317582666874\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 13.95 seconds\n",
      "epoch: 3 average loss: 1.453\n",
      "Test Accuracy : 63.1%, Test Loss: 1.3372744023799896\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 13.61 seconds\n",
      "epoch: 4 average loss: 1.398\n",
      "Test Accuracy : 63.9%, Test Loss: 1.282293226569891\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 13.18 seconds\n",
      "epoch: 5 average loss: 1.343\n",
      "Test Accuracy : 64.3%, Test Loss: 1.23249002546072\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 13.28 seconds\n",
      "epoch: 6 average loss: 1.311\n",
      "Test Accuracy : 64.8%, Test Loss: 1.2164589129388332\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 13.25 seconds\n",
      "epoch: 7 average loss: 1.295\n",
      "Test Accuracy : 65.8%, Test Loss: 1.1630990542471409\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 13.31 seconds\n",
      "epoch: 8 average loss: 1.257\n",
      "Test Accuracy : 65.2%, Test Loss: 1.1640449073165655\n",
      "Epoch Time (Training + Test) = 13.26 seconds\n",
      "epoch: 9 average loss: 1.249\n",
      "Test Accuracy : 65.8%, Test Loss: 1.1451055947691202\n",
      "Epoch Time (Training + Test) = 13.11 seconds\n",
      "epoch: 10 average loss: 1.222\n",
      "Test Accuracy : 67.3%, Test Loss: 1.1129242945462465\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 13.31 seconds\n",
      "epoch: 11 average loss: 1.212\n",
      "Test Accuracy : 66.2%, Test Loss: 1.129596769809723\n",
      "Epoch Time (Training + Test) = 13.21 seconds\n",
      "epoch: 12 average loss: 1.201\n",
      "Test Accuracy : 65.0%, Test Loss: 1.1167860589921474\n",
      "Epoch Time (Training + Test) = 13.16 seconds\n",
      "epoch: 13 average loss: 1.192\n",
      "Test Accuracy : 66.3%, Test Loss: 1.1182468552142382\n",
      "Epoch Time (Training + Test) = 13.38 seconds\n",
      "epoch: 14 average loss: 1.179\n",
      "Test Accuracy : 66.6%, Test Loss: 1.0927111152559519\n",
      "Epoch Time (Training + Test) = 13.17 seconds\n",
      "epoch: 15 average loss: 1.178\n",
      "Test Accuracy : 65.2%, Test Loss: 1.1173279043287039\n",
      "Epoch Time (Training + Test) = 13.27 seconds\n",
      "epoch: 16 average loss: 1.160\n",
      "Test Accuracy : 67.4%, Test Loss: 1.050534913316369\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 13.28 seconds\n",
      "epoch: 17 average loss: 1.154\n",
      "Test Accuracy : 66.5%, Test Loss: 1.0541862826794386\n",
      "Epoch Time (Training + Test) = 13.40 seconds\n",
      "epoch: 18 average loss: 1.158\n",
      "Test Accuracy : 66.3%, Test Loss: 1.080865852534771\n",
      "Epoch Time (Training + Test) = 13.16 seconds\n",
      "epoch: 19 average loss: 1.143\n",
      "Test Accuracy : 65.5%, Test Loss: 1.069452028721571\n",
      "Epoch Time (Training + Test) = 13.24 seconds\n",
      "epoch: 20 average loss: 1.135\n",
      "Test Accuracy : 66.5%, Test Loss: 1.0679232031106949\n",
      "Epoch Time (Training + Test) = 13.28 seconds\n",
      "epoch: 21 average loss: 1.126\n",
      "Test Accuracy : 65.8%, Test Loss: 1.0482492223381996\n",
      "Epoch Time (Training + Test) = 13.29 seconds\n",
      "epoch: 22 average loss: 1.128\n",
      "Test Accuracy : 65.6%, Test Loss: 1.0710080396384\n",
      "Epoch Time (Training + Test) = 13.17 seconds\n",
      "epoch: 23 average loss: 1.111\n",
      "Test Accuracy : 66.0%, Test Loss: 1.0469578597694635\n",
      "Epoch Time (Training + Test) = 13.26 seconds\n",
      "epoch: 24 average loss: 1.118\n",
      "Test Accuracy : 67.8%, Test Loss: 1.024178635329008\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 13.14 seconds\n",
      "epoch: 25 average loss: 1.112\n",
      "Test Accuracy : 68.2%, Test Loss: 1.0079032741487026\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 13.26 seconds\n",
      "epoch: 26 average loss: 1.107\n",
      "Test Accuracy : 68.2%, Test Loss: 1.0217947382479906\n",
      "Epoch Time (Training + Test) = 13.22 seconds\n",
      "epoch: 27 average loss: 1.083\n",
      "Test Accuracy : 67.5%, Test Loss: 1.0153582021594048\n",
      "Epoch Time (Training + Test) = 13.24 seconds\n",
      "epoch: 28 average loss: 1.099\n",
      "Test Accuracy : 68.5%, Test Loss: 1.0111049339175224\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 13.20 seconds\n",
      "epoch: 29 average loss: 1.091\n",
      "Test Accuracy : 65.9%, Test Loss: 1.023499010130763\n",
      "Epoch Time (Training + Test) = 13.28 seconds\n",
      "epoch: 30 average loss: 1.080\n",
      "Test Accuracy : 68.4%, Test Loss: 0.9983746111392975\n",
      "Epoch Time (Training + Test) = 13.24 seconds\n",
      "epoch: 31 average loss: 1.060\n",
      "Test Accuracy : 68.5%, Test Loss: 0.9715660903602839\n",
      "Epoch Time (Training + Test) = 13.24 seconds\n",
      "epoch: 32 average loss: 1.044\n",
      "Test Accuracy : 68.2%, Test Loss: 0.9915120918303728\n",
      "Epoch Time (Training + Test) = 13.32 seconds\n",
      "epoch: 33 average loss: 1.045\n",
      "Test Accuracy : 68.8%, Test Loss: 0.963479632511735\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 13.28 seconds\n",
      "epoch: 34 average loss: 1.040\n",
      "Test Accuracy : 69.9%, Test Loss: 0.9409974329173565\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 13.20 seconds\n",
      "epoch: 35 average loss: 1.033\n",
      "Test Accuracy : 69.2%, Test Loss: 0.9611369613558054\n",
      "Epoch Time (Training + Test) = 13.26 seconds\n",
      "epoch: 36 average loss: 1.021\n",
      "Test Accuracy : 70.5%, Test Loss: 0.9479104056954384\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 13.33 seconds\n",
      "epoch: 37 average loss: 1.010\n",
      "Test Accuracy : 70.8%, Test Loss: 0.9179661255329847\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 13.29 seconds\n",
      "epoch: 38 average loss: 1.017\n",
      "Test Accuracy : 69.7%, Test Loss: 0.9375592805445194\n",
      "Epoch Time (Training + Test) = 13.20 seconds\n",
      "epoch: 39 average loss: 1.008\n",
      "Test Accuracy : 71.0%, Test Loss: 0.9209380596876144\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 13.33 seconds\n",
      "epoch: 40 average loss: 0.997\n",
      "Test Accuracy : 71.8%, Test Loss: 0.9231956023722887\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 13.30 seconds\n",
      "epoch: 41 average loss: 0.985\n",
      "Test Accuracy : 70.2%, Test Loss: 0.9349185042083263\n",
      "Epoch Time (Training + Test) = 13.27 seconds\n",
      "epoch: 42 average loss: 0.979\n",
      "Test Accuracy : 71.8%, Test Loss: 0.905543964356184\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 13.30 seconds\n",
      "epoch: 43 average loss: 0.965\n",
      "Test Accuracy : 70.2%, Test Loss: 0.9046175181865692\n",
      "Epoch Time (Training + Test) = 13.21 seconds\n",
      "epoch: 44 average loss: 0.978\n",
      "Test Accuracy : 71.2%, Test Loss: 0.8835216760635376\n",
      "Epoch Time (Training + Test) = 13.17 seconds\n",
      "epoch: 45 average loss: 0.964\n",
      "Test Accuracy : 72.2%, Test Loss: 0.9071047492325306\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 13.30 seconds\n",
      "epoch: 46 average loss: 0.951\n",
      "Test Accuracy : 72.0%, Test Loss: 0.898117084056139\n",
      "Epoch Time (Training + Test) = 13.23 seconds\n",
      "epoch: 47 average loss: 0.936\n",
      "Test Accuracy : 70.9%, Test Loss: 0.9001341797411442\n",
      "Epoch Time (Training + Test) = 13.27 seconds\n",
      "epoch: 48 average loss: 0.953\n",
      "Test Accuracy : 73.6%, Test Loss: 0.856315715238452\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 13.27 seconds\n",
      "epoch: 49 average loss: 0.935\n",
      "Test Accuracy : 72.4%, Test Loss: 0.8652171790599823\n",
      "Epoch Time (Training + Test) = 13.17 seconds\n",
      "epoch: 50 average loss: 0.918\n",
      "Test Accuracy : 71.5%, Test Loss: 0.8705010358244181\n",
      "Epoch Time (Training + Test) = 13.30 seconds\n",
      "epoch: 51 average loss: 0.924\n",
      "Test Accuracy : 72.3%, Test Loss: 0.8712815959006548\n",
      "Epoch Time (Training + Test) = 13.23 seconds\n",
      "epoch: 52 average loss: 0.904\n",
      "Test Accuracy : 73.5%, Test Loss: 0.8544944915920496\n",
      "Epoch Time (Training + Test) = 13.21 seconds\n",
      "epoch: 53 average loss: 0.893\n",
      "Test Accuracy : 72.9%, Test Loss: 0.8355724923312664\n",
      "Epoch Time (Training + Test) = 13.21 seconds\n",
      "epoch: 54 average loss: 0.895\n",
      "Test Accuracy : 73.2%, Test Loss: 0.8335917722433805\n",
      "Epoch Time (Training + Test) = 13.21 seconds\n",
      "epoch: 55 average loss: 0.899\n",
      "Test Accuracy : 72.2%, Test Loss: 0.8765698317438364\n",
      "Epoch Time (Training + Test) = 13.22 seconds\n",
      "epoch: 56 average loss: 0.885\n",
      "Test Accuracy : 72.6%, Test Loss: 0.8411413729190826\n",
      "Epoch Time (Training + Test) = 13.30 seconds\n",
      "epoch: 57 average loss: 0.872\n",
      "Test Accuracy : 72.8%, Test Loss: 0.8409855365753174\n",
      "Epoch Time (Training + Test) = 13.28 seconds\n",
      "epoch: 58 average loss: 0.873\n",
      "Test Accuracy : 72.5%, Test Loss: 0.835998672991991\n",
      "Epoch Time (Training + Test) = 13.20 seconds\n",
      "epoch: 59 average loss: 0.869\n",
      "Test Accuracy : 72.8%, Test Loss: 0.8587446473538876\n",
      "Epoch Time (Training + Test) = 13.27 seconds\n",
      "epoch: 60 average loss: 0.857\n",
      "Test Accuracy : 72.6%, Test Loss: 0.8510196451097727\n",
      "Epoch Time (Training + Test) = 13.24 seconds\n",
      "epoch: 61 average loss: 0.867\n",
      "Test Accuracy : 74.6%, Test Loss: 0.7938081976026297\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 13.17 seconds\n",
      "epoch: 62 average loss: 0.852\n",
      "Test Accuracy : 74.1%, Test Loss: 0.8058414813131094\n",
      "Epoch Time (Training + Test) = 13.25 seconds\n",
      "epoch: 63 average loss: 0.853\n",
      "Test Accuracy : 73.6%, Test Loss: 0.8175712786614895\n",
      "Epoch Time (Training + Test) = 13.21 seconds\n",
      "epoch: 64 average loss: 0.841\n",
      "Test Accuracy : 73.2%, Test Loss: 0.8219167850911617\n",
      "Epoch Time (Training + Test) = 13.30 seconds\n",
      "epoch: 65 average loss: 0.827\n",
      "Test Accuracy : 71.9%, Test Loss: 0.8477269895374775\n",
      "Epoch Time (Training + Test) = 13.23 seconds\n",
      "epoch: 66 average loss: 0.818\n",
      "Test Accuracy : 73.7%, Test Loss: 0.8097285386174917\n",
      "Epoch Time (Training + Test) = 13.28 seconds\n",
      "epoch: 67 average loss: 0.816\n",
      "Test Accuracy : 74.4%, Test Loss: 0.8021628670394421\n",
      "Epoch Time (Training + Test) = 13.30 seconds\n",
      "epoch: 68 average loss: 0.820\n",
      "Test Accuracy : 73.6%, Test Loss: 0.8366723973304033\n",
      "Epoch Time (Training + Test) = 13.17 seconds\n",
      "epoch: 69 average loss: 0.790\n",
      "Test Accuracy : 73.1%, Test Loss: 0.8113322164863348\n",
      "Epoch Time (Training + Test) = 13.19 seconds\n",
      "epoch: 70 average loss: 0.805\n",
      "Test Accuracy : 74.9%, Test Loss: 0.7891736663877964\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 13.28 seconds\n",
      "epoch: 71 average loss: 0.786\n",
      "Test Accuracy : 74.4%, Test Loss: 0.8038190845400095\n",
      "Epoch Time (Training + Test) = 13.30 seconds\n",
      "epoch: 72 average loss: 0.784\n",
      "Test Accuracy : 74.3%, Test Loss: 0.7945079933851957\n",
      "Epoch Time (Training + Test) = 13.27 seconds\n",
      "epoch: 73 average loss: 0.795\n",
      "Test Accuracy : 74.6%, Test Loss: 0.7796124797314405\n",
      "Epoch Time (Training + Test) = 13.26 seconds\n",
      "epoch: 74 average loss: 0.778\n",
      "Test Accuracy : 75.0%, Test Loss: 0.7785487230867147\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 13.38 seconds\n",
      "epoch: 75 average loss: 0.786\n",
      "Test Accuracy : 74.4%, Test Loss: 0.7896676640957594\n",
      "Epoch Time (Training + Test) = 13.11 seconds\n",
      "Data Saved to Regular_Finetune.csv\n",
      "Finished Training: \n",
      "Total Time 0.553092 hours\n",
      " Average Time Per Epoch 26.55 seconds\n"
     ]
    }
   ],
   "source": [
    "BNN = BNN_Resnet_UniAdapt(80)\n",
    "BNN.load_state_dict(state,strict=False)\n",
    "BNN.cuda()\n",
    "\n",
    "trainer = Trainer(model = BNN,seed = 123,model_name = f'Regular_Finetune',project_name = 'FINAL_CIFAR100_Regular',classes = train_20.classes,binarise= True)\n",
    "m = trainer.model\n",
    "m.to(trainer.device)\n",
    "m.fc = BinarizeLinear(320,20)\n",
    "m.bn3 = nn.BatchNorm1d(20)\n",
    "# for layer in [m.fc,m.bn3]:\n",
    "#     for p in layer.parameters():\n",
    "#         p.requires_grad = True\n",
    "#     layer.train()\n",
    "trainer.lr = 1e-3\n",
    "trainer.batch_size = 64\n",
    "trainer.epochs =75\n",
    "trainer.epoch_chkpts = []\n",
    "trainer.start_epoch = 0\n",
    "trainer.set_scheduler(None)\n",
    "# adpt = getattr(m,after_layer)[-1]\n",
    "# print(adpt)\n",
    "# print(m.fc.weight[0,0])\n",
    "# params = nn.ModuleList([m.fc,m.bn3,adpt]).parameters()\n",
    "trainer.set_optimizer(torch.optim.Adam)\n",
    "trainer.train(train20_DL,test20_DL)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UniAdapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'SavedModels\\UniAdapt_Cifar100_Base\\ResNet18_BNN_Baseline_2023-05-09 13-19-37\\ResNet18_BNN_Baseline_best_acc.pth'\n",
    "\n",
    "state = torch.load(path,map_location='cuda:0')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BNN_Resnet_UniAdapt(\n",
      "  (conv1): BinarizeConv2d(3, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (bn1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "  (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(80, 160, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): BinarizeConv2d(80, 160, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(160, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): BinarizeConv2d(160, 320, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)\n",
      "  (bn2): BatchNorm1d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (bn3): BatchNorm1d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (logsoftmax): LogSoftmax(dim=-1)\n",
      "  (fc): BinarizeLinear(in_features=320, out_features=80, bias=True)\n",
      "  (adapter_dict): ModuleDict()\n",
      "  (head_bn): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (head): BinarizeLinear(in_features=640, out_features=20, bias=True)\n",
      "  (encoder_net): placeholder()\n",
      "  (uniAdaptNet): Sequential(\n",
      "    (0): uniAdapt_Net(\n",
      "      (layer1): thinBlock3(\n",
      "        (conv2): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=80, bias=False)\n",
      "        (bn2): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(80, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (layer2): thinBlock3(\n",
      "        (conv2): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=160, bias=False)\n",
      "        (bn2): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(160, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (layer3): thinBlock3(\n",
      "        (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=320, bias=False)\n",
      "        (bn2): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (pool1): AdaptiveAvgPool2d(output_size=16)\n",
      "      (pool2): AdaptiveAvgPool2d(output_size=8)\n",
      "      (pool3): AdaptiveAvgPool2d(output_size=1)\n",
      "    )\n",
      "    (1): Flatten(start_dim=1, end_dim=-1)\n",
      "  )\n",
      ")\n",
      "No lr scheduler\n",
      "Optimizer Set to New: {'state': {}, 'param_groups': [{'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': False, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]}]}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1d7fl2lm) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ca5036b2df74d6a93dbb96553a380ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.055 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.013325…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Current Best Acc</td><td>▁▇█████</td></tr><tr><td>epoch</td><td>▁▂▃▅▆▇█</td></tr><tr><td>epoch time (s)</td><td>▁▆▇▇█▇▆</td></tr><tr><td>lr</td><td>▁▁▁▁▁▁▁</td></tr><tr><td>test_accuracy</td><td>▁▇█████</td></tr><tr><td>test_loss</td><td>█▁▁▁▁▁▁</td></tr><tr><td>training_loss</td><td>█▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Current Best Acc</td><td>0.72</td></tr><tr><td>epoch</td><td>6</td></tr><tr><td>epoch time (s)</td><td>9.13606</td></tr><tr><td>lr</td><td>0.001</td></tr><tr><td>test_accuracy</td><td>0.695</td></tr><tr><td>test_loss</td><td>1.06689</td></tr><tr><td>training_loss</td><td>1.16685</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">crisp-frog-2</strong>: <a href=\"https://wandb.ai/johnny_suu/FINAL_PRUNING_CIFAR100/runs/1d7fl2lm\" target=\"_blank\">https://wandb.ai/johnny_suu/FINAL_PRUNING_CIFAR100/runs/1d7fl2lm</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230510_103843-1d7fl2lm\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1d7fl2lm). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6f4fa69ca1d46199ca09718daf19357",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016933333330477276, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\John Su\\Downloads\\SydneyUni\\2022\\thesis\\Thesis\\wandb\\run-20230510_104010-390fe0c9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/johnny_suu/FINAL_PRUNING_CIFAR100/runs/390fe0c9\" target=\"_blank\">tough-salad-3</a></strong> to <a href=\"https://wandb.ai/johnny_suu/FINAL_PRUNING_CIFAR100\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "Project Name: FINAL_PRUNING_CIFAR100, Run Name Cifar80_Prune_0.0% \n",
      "\n",
      "\n",
      "Run Start : 2023-05-10 10-40-10\n",
      "start_epoch : 0\n",
      "initial_lr : 0.001\n",
      "batch_size : 64\n",
      "epochs : 75\n",
      "epoch_chkpts : []\n",
      "device : cuda:0\n",
      "criterion : CrossEntropyLoss()\n",
      "optimizer : Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: False\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "seed : None\n",
      "model_architecture : <class 'NN_Thesis.nn_classes.BNN_Resnet_UniAdapt'>\n",
      "binerised_training : True\n",
      "Number of Elements : 4547660\n",
      "Initial accuracy:\n",
      "Test Accuracy : 4.9%, Test Loss: 41.67728718071227\n",
      "best_acc.pth saved!\n",
      "Test Accuracy : 4.5%, Test Loss: 41.409974575042725\n",
      "epoch: 1 average loss: 1.618\n",
      "Test Accuracy : 67.7%, Test Loss: 1.310125943273306\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.41 seconds\n",
      "epoch: 2 average loss: 1.356\n",
      "Test Accuracy : 69.3%, Test Loss: 1.2049964014440775\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.34 seconds\n",
      "epoch: 3 average loss: 1.250\n",
      "Test Accuracy : 70.6%, Test Loss: 1.1308149807155132\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.58 seconds\n",
      "epoch: 4 average loss: 1.194\n",
      "Test Accuracy : 71.4%, Test Loss: 1.0880937688052654\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.34 seconds\n",
      "epoch: 5 average loss: 1.129\n",
      "Test Accuracy : 71.8%, Test Loss: 1.0235167425125837\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.47 seconds\n",
      "epoch: 6 average loss: 1.082\n",
      "Test Accuracy : 71.2%, Test Loss: 1.0084316097199917\n",
      "Epoch Time (Training + Test) = 9.17 seconds\n",
      "epoch: 7 average loss: 1.049\n",
      "Test Accuracy : 72.0%, Test Loss: 0.9993484262377024\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.46 seconds\n",
      "epoch: 8 average loss: 1.026\n",
      "Test Accuracy : 72.9%, Test Loss: 0.9271793067455292\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.16 seconds\n",
      "epoch: 9 average loss: 0.987\n",
      "Test Accuracy : 72.5%, Test Loss: 0.9520444013178349\n",
      "Epoch Time (Training + Test) = 9.37 seconds\n",
      "epoch: 10 average loss: 0.953\n",
      "Test Accuracy : 71.7%, Test Loss: 0.9559564348310232\n",
      "Epoch Time (Training + Test) = 9.24 seconds\n",
      "epoch: 11 average loss: 0.941\n",
      "Test Accuracy : 72.6%, Test Loss: 0.9185819271951914\n",
      "Epoch Time (Training + Test) = 9.09 seconds\n",
      "epoch: 12 average loss: 0.921\n",
      "Test Accuracy : 72.9%, Test Loss: 0.9069972820580006\n",
      "Epoch Time (Training + Test) = 9.00 seconds\n",
      "epoch: 13 average loss: 0.908\n",
      "Test Accuracy : 73.2%, Test Loss: 0.8784953206777573\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.12 seconds\n",
      "epoch: 14 average loss: 0.878\n",
      "Test Accuracy : 74.5%, Test Loss: 0.8568391762673855\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.10 seconds\n",
      "epoch: 15 average loss: 0.870\n",
      "Test Accuracy : 74.1%, Test Loss: 0.8536130357533693\n",
      "Epoch Time (Training + Test) = 9.04 seconds\n",
      "epoch: 16 average loss: 0.839\n",
      "Test Accuracy : 73.6%, Test Loss: 0.8514340948313475\n",
      "Epoch Time (Training + Test) = 8.99 seconds\n",
      "epoch: 17 average loss: 0.823\n",
      "Test Accuracy : 72.9%, Test Loss: 0.8418033812195063\n",
      "Epoch Time (Training + Test) = 9.06 seconds\n",
      "epoch: 18 average loss: 0.820\n",
      "Test Accuracy : 74.1%, Test Loss: 0.8495848160237074\n",
      "Epoch Time (Training + Test) = 9.05 seconds\n",
      "epoch: 19 average loss: 0.804\n",
      "Test Accuracy : 73.9%, Test Loss: 0.8518995214253664\n",
      "Epoch Time (Training + Test) = 9.05 seconds\n",
      "epoch: 20 average loss: 0.805\n",
      "Test Accuracy : 74.8%, Test Loss: 0.8227680493146181\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.03 seconds\n",
      "epoch: 21 average loss: 0.787\n",
      "Test Accuracy : 73.3%, Test Loss: 0.8326668702065945\n",
      "Epoch Time (Training + Test) = 9.15 seconds\n",
      "epoch: 22 average loss: 0.764\n",
      "Test Accuracy : 73.6%, Test Loss: 0.8192987721413374\n",
      "Epoch Time (Training + Test) = 9.27 seconds\n",
      "epoch: 23 average loss: 0.775\n",
      "Test Accuracy : 74.8%, Test Loss: 0.8087021391838789\n",
      "Epoch Time (Training + Test) = 9.04 seconds\n",
      "epoch: 24 average loss: 0.761\n",
      "Test Accuracy : 75.4%, Test Loss: 0.8161825314164162\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.07 seconds\n",
      "epoch: 25 average loss: 0.734\n",
      "Test Accuracy : 74.0%, Test Loss: 0.8366918247193098\n",
      "Epoch Time (Training + Test) = 9.02 seconds\n",
      "epoch: 26 average loss: 0.734\n",
      "Test Accuracy : 75.2%, Test Loss: 0.7966451570391655\n",
      "Epoch Time (Training + Test) = 9.09 seconds\n",
      "epoch: 27 average loss: 0.729\n",
      "Test Accuracy : 75.2%, Test Loss: 0.8184251170605421\n",
      "Epoch Time (Training + Test) = 9.03 seconds\n",
      "epoch: 28 average loss: 0.719\n",
      "Test Accuracy : 74.9%, Test Loss: 0.7940898686647415\n",
      "Epoch Time (Training + Test) = 9.06 seconds\n",
      "epoch: 29 average loss: 0.711\n",
      "Test Accuracy : 75.7%, Test Loss: 0.7944215908646584\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.09 seconds\n",
      "epoch: 30 average loss: 0.704\n",
      "Test Accuracy : 75.3%, Test Loss: 0.7911483943462372\n",
      "Epoch Time (Training + Test) = 9.07 seconds\n",
      "epoch: 31 average loss: 0.692\n",
      "Test Accuracy : 74.2%, Test Loss: 0.8147797491401434\n",
      "Epoch Time (Training + Test) = 9.03 seconds\n",
      "epoch: 32 average loss: 0.678\n",
      "Test Accuracy : 74.3%, Test Loss: 0.8138980139046907\n",
      "Epoch Time (Training + Test) = 9.06 seconds\n",
      "epoch: 33 average loss: 0.676\n",
      "Test Accuracy : 74.7%, Test Loss: 0.7898461688309908\n",
      "Epoch Time (Training + Test) = 9.04 seconds\n",
      "epoch: 34 average loss: 0.666\n",
      "Test Accuracy : 74.2%, Test Loss: 0.813548257574439\n",
      "Epoch Time (Training + Test) = 9.07 seconds\n",
      "epoch: 35 average loss: 0.663\n",
      "Test Accuracy : 75.7%, Test Loss: 0.7748268945142627\n",
      "Epoch Time (Training + Test) = 9.00 seconds\n",
      "epoch: 36 average loss: 0.663\n",
      "Test Accuracy : 74.3%, Test Loss: 0.8054556082934141\n",
      "Epoch Time (Training + Test) = 9.09 seconds\n",
      "epoch: 37 average loss: 0.655\n",
      "Test Accuracy : 73.1%, Test Loss: 0.8217540513724089\n",
      "Epoch Time (Training + Test) = 9.08 seconds\n",
      "conv2\n",
      "conv3\n",
      "conv2\n",
      "conv3\n",
      "conv2\n",
      "conv3\n",
      "Pruned [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]% of weights at epoch 37\n",
      "epoch: 38 average loss: 0.647\n",
      "Test Accuracy : 75.8%, Test Loss: 0.7815782744437456\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.11 seconds\n",
      "epoch: 39 average loss: 0.636\n",
      "Test Accuracy : 75.9%, Test Loss: 0.7539247618988156\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.06 seconds\n",
      "epoch: 40 average loss: 0.630\n",
      "Test Accuracy : 74.2%, Test Loss: 0.8036568127572536\n",
      "Epoch Time (Training + Test) = 9.03 seconds\n",
      "epoch: 41 average loss: 0.618\n",
      "Test Accuracy : 76.3%, Test Loss: 0.7801319733262062\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.08 seconds\n",
      "epoch: 42 average loss: 0.607\n",
      "Test Accuracy : 74.5%, Test Loss: 0.7848124541342258\n",
      "Epoch Time (Training + Test) = 9.03 seconds\n",
      "epoch: 43 average loss: 0.605\n",
      "Test Accuracy : 74.1%, Test Loss: 0.7935921642929316\n",
      "Epoch Time (Training + Test) = 9.07 seconds\n",
      "epoch: 44 average loss: 0.616\n",
      "Test Accuracy : 75.0%, Test Loss: 0.7807164341211319\n",
      "Epoch Time (Training + Test) = 8.97 seconds\n",
      "epoch: 45 average loss: 0.604\n",
      "Test Accuracy : 76.2%, Test Loss: 0.7750886250287294\n",
      "Epoch Time (Training + Test) = 9.06 seconds\n",
      "epoch: 46 average loss: 0.590\n",
      "Test Accuracy : 74.5%, Test Loss: 0.7971090823411942\n",
      "Epoch Time (Training + Test) = 9.09 seconds\n",
      "epoch: 47 average loss: 0.603\n",
      "Test Accuracy : 75.3%, Test Loss: 0.8134694807231426\n",
      "Epoch Time (Training + Test) = 9.01 seconds\n",
      "epoch: 48 average loss: 0.590\n",
      "Test Accuracy : 74.8%, Test Loss: 0.7995290383696556\n",
      "Epoch Time (Training + Test) = 9.05 seconds\n",
      "epoch: 49 average loss: 0.582\n",
      "Test Accuracy : 75.2%, Test Loss: 0.766127354465425\n",
      "Epoch Time (Training + Test) = 9.15 seconds\n",
      "epoch: 50 average loss: 0.572\n",
      "Test Accuracy : 74.6%, Test Loss: 0.8160260021686554\n",
      "Epoch Time (Training + Test) = 9.18 seconds\n",
      "epoch: 51 average loss: 0.544\n",
      "Test Accuracy : 75.4%, Test Loss: 0.8101447420194745\n",
      "Epoch Time (Training + Test) = 9.12 seconds\n",
      "epoch: 52 average loss: 0.569\n",
      "Test Accuracy : 75.2%, Test Loss: 0.7762504676356912\n",
      "Epoch Time (Training + Test) = 9.00 seconds\n",
      "epoch: 53 average loss: 0.567\n",
      "Test Accuracy : 75.6%, Test Loss: 0.770502146333456\n",
      "Epoch Time (Training + Test) = 9.04 seconds\n",
      "epoch: 54 average loss: 0.559\n",
      "Test Accuracy : 75.8%, Test Loss: 0.7798085231333971\n",
      "Epoch Time (Training + Test) = 9.07 seconds\n",
      "epoch: 55 average loss: 0.561\n",
      "Test Accuracy : 75.3%, Test Loss: 0.7991105951368809\n",
      "Epoch Time (Training + Test) = 9.13 seconds\n",
      "epoch: 56 average loss: 0.535\n",
      "Test Accuracy : 75.4%, Test Loss: 0.7961680088192225\n",
      "Epoch Time (Training + Test) = 9.03 seconds\n",
      "epoch: 57 average loss: 0.547\n",
      "Test Accuracy : 75.6%, Test Loss: 0.7850279128178954\n",
      "Epoch Time (Training + Test) = 8.99 seconds\n",
      "epoch: 58 average loss: 0.533\n",
      "Test Accuracy : 75.9%, Test Loss: 0.7856079265475273\n",
      "Epoch Time (Training + Test) = 9.05 seconds\n",
      "epoch: 59 average loss: 0.520\n",
      "Test Accuracy : 76.1%, Test Loss: 0.7981091216206551\n",
      "Epoch Time (Training + Test) = 9.06 seconds\n",
      "epoch: 60 average loss: 0.528\n",
      "Test Accuracy : 75.1%, Test Loss: 0.7880149818956852\n",
      "Epoch Time (Training + Test) = 8.95 seconds\n",
      "epoch: 61 average loss: 0.535\n",
      "Test Accuracy : 74.7%, Test Loss: 0.7977537121623755\n",
      "Epoch Time (Training + Test) = 8.96 seconds\n",
      "epoch: 62 average loss: 0.525\n",
      "Test Accuracy : 74.5%, Test Loss: 0.8096903022378683\n",
      "Epoch Time (Training + Test) = 9.06 seconds\n",
      "epoch: 63 average loss: 0.533\n",
      "Test Accuracy : 74.6%, Test Loss: 0.8042122516781092\n",
      "Epoch Time (Training + Test) = 9.03 seconds\n",
      "epoch: 64 average loss: 0.518\n",
      "Test Accuracy : 75.0%, Test Loss: 0.808413190767169\n",
      "Epoch Time (Training + Test) = 9.07 seconds\n",
      "epoch: 65 average loss: 0.497\n",
      "Test Accuracy : 75.2%, Test Loss: 0.8157811034470797\n",
      "Epoch Time (Training + Test) = 9.05 seconds\n",
      "epoch: 66 average loss: 0.516\n",
      "Test Accuracy : 75.1%, Test Loss: 0.8200834263116121\n",
      "Epoch Time (Training + Test) = 9.05 seconds\n",
      "epoch: 67 average loss: 0.499\n",
      "Test Accuracy : 74.7%, Test Loss: 0.8108195047825575\n",
      "Epoch Time (Training + Test) = 9.03 seconds\n",
      "epoch: 68 average loss: 0.498\n",
      "Test Accuracy : 74.5%, Test Loss: 0.8367119804024696\n",
      "Epoch Time (Training + Test) = 8.96 seconds\n",
      "epoch: 69 average loss: 0.484\n",
      "Test Accuracy : 74.7%, Test Loss: 0.8165171593427658\n",
      "Epoch Time (Training + Test) = 9.03 seconds\n",
      "epoch: 70 average loss: 0.498\n",
      "Test Accuracy : 75.8%, Test Loss: 0.7782576307654381\n",
      "Epoch Time (Training + Test) = 9.00 seconds\n",
      "epoch: 71 average loss: 0.488\n",
      "Test Accuracy : 75.3%, Test Loss: 0.7899026786908507\n",
      "Epoch Time (Training + Test) = 9.04 seconds\n",
      "epoch: 72 average loss: 0.482\n",
      "Test Accuracy : 75.0%, Test Loss: 0.8324218560010195\n",
      "Epoch Time (Training + Test) = 9.11 seconds\n",
      "epoch: 73 average loss: 0.479\n",
      "Test Accuracy : 74.9%, Test Loss: 0.7900710897520185\n",
      "Epoch Time (Training + Test) = 9.04 seconds\n",
      "epoch: 74 average loss: 0.479\n",
      "Test Accuracy : 75.4%, Test Loss: 0.8121845219284296\n",
      "Epoch Time (Training + Test) = 9.00 seconds\n",
      "epoch: 75 average loss: 0.486\n",
      "Test Accuracy : 75.0%, Test Loss: 0.7817527987062931\n",
      "Epoch Time (Training + Test) = 8.98 seconds\n",
      "Data Saved to Cifar80_Prune_0.0%.csv\n",
      "Finished Training: \n",
      "Total Time 0.378937 hours\n",
      " Average Time Per Epoch 18.19 seconds\n",
      "BNN_Resnet_UniAdapt(\n",
      "  (conv1): BinarizeConv2d(3, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (bn1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "  (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(80, 160, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): BinarizeConv2d(80, 160, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(160, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): BinarizeConv2d(160, 320, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)\n",
      "  (bn2): BatchNorm1d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (bn3): BatchNorm1d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (logsoftmax): LogSoftmax(dim=-1)\n",
      "  (fc): BinarizeLinear(in_features=320, out_features=80, bias=True)\n",
      "  (adapter_dict): ModuleDict()\n",
      "  (head_bn): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (head): BinarizeLinear(in_features=640, out_features=20, bias=True)\n",
      "  (encoder_net): placeholder()\n",
      "  (uniAdaptNet): Sequential(\n",
      "    (0): uniAdapt_Net(\n",
      "      (layer1): thinBlock3(\n",
      "        (conv2): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=80, bias=False)\n",
      "        (bn2): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(80, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (layer2): thinBlock3(\n",
      "        (conv2): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=160, bias=False)\n",
      "        (bn2): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(160, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (layer3): thinBlock3(\n",
      "        (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=320, bias=False)\n",
      "        (bn2): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (pool1): AdaptiveAvgPool2d(output_size=16)\n",
      "      (pool2): AdaptiveAvgPool2d(output_size=8)\n",
      "      (pool3): AdaptiveAvgPool2d(output_size=1)\n",
      "    )\n",
      "    (1): Flatten(start_dim=1, end_dim=-1)\n",
      "  )\n",
      ")\n",
      "No lr scheduler\n",
      "Optimizer Set to New: {'state': {}, 'param_groups': [{'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': False, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]}]}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:390fe0c9) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Current Best Acc</td><td>▁▇▇█████████████████████████████████████</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>epoch time (s)</td><td>▁▇██▇▇▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▅▆▆▆▆▆▆▆▆▅▆▆▆▆▆▆▅</td></tr><tr><td>lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_accuracy</td><td>▁▇▇█████████████████████████████████████</td></tr><tr><td>test_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>training_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Best Accuracy</td><td>0.7635</td></tr><tr><td>Current Best Acc</td><td>0.7635</td></tr><tr><td>Total Time (hours)</td><td>0.37894</td></tr><tr><td>epoch</td><td>75</td></tr><tr><td>epoch time (s)</td><td>8.98121</td></tr><tr><td>lr</td><td>0.001</td></tr><tr><td>test_accuracy</td><td>0.7505</td></tr><tr><td>test_loss</td><td>0.78175</td></tr><tr><td>training_loss</td><td>0.48554</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">tough-salad-3</strong>: <a href=\"https://wandb.ai/johnny_suu/FINAL_PRUNING_CIFAR100/runs/390fe0c9\" target=\"_blank\">https://wandb.ai/johnny_suu/FINAL_PRUNING_CIFAR100/runs/390fe0c9</a><br/>Synced 5 W&B file(s), 1 media file(s), 1 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230510_104010-390fe0c9\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:390fe0c9). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06a7820774764fbdba2700e2781d26dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01691666666883975, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\John Su\\Downloads\\SydneyUni\\2022\\thesis\\Thesis\\wandb\\run-20230510_105147-2wyr848i</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/johnny_suu/FINAL_PRUNING_CIFAR100/runs/2wyr848i\" target=\"_blank\">sage-pond-4</a></strong> to <a href=\"https://wandb.ai/johnny_suu/FINAL_PRUNING_CIFAR100\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "Project Name: FINAL_PRUNING_CIFAR100, Run Name Cifar80_Prune_80.0% \n",
      "\n",
      "\n",
      "Run Start : 2023-05-10 10-51-47\n",
      "start_epoch : 0\n",
      "initial_lr : 0.001\n",
      "batch_size : 64\n",
      "epochs : 75\n",
      "epoch_chkpts : []\n",
      "device : cuda:0\n",
      "criterion : CrossEntropyLoss()\n",
      "optimizer : Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: False\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "seed : None\n",
      "model_architecture : <class 'NN_Thesis.nn_classes.BNN_Resnet_UniAdapt'>\n",
      "binerised_training : True\n",
      "Number of Elements : 4547660\n",
      "Initial accuracy:\n",
      "Test Accuracy : 6.1%, Test Loss: 50.03438497652673\n",
      "best_acc.pth saved!\n",
      "Test Accuracy : 5.9%, Test Loss: 52.0525928735733\n",
      "epoch: 1 average loss: 1.647\n",
      "Test Accuracy : 66.9%, Test Loss: 1.3134911134839058\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.06 seconds\n",
      "epoch: 2 average loss: 1.359\n",
      "Test Accuracy : 69.1%, Test Loss: 1.1990348659455776\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.08 seconds\n",
      "epoch: 3 average loss: 1.261\n",
      "Test Accuracy : 69.3%, Test Loss: 1.1486778669059277\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.07 seconds\n",
      "epoch: 4 average loss: 1.196\n",
      "Test Accuracy : 70.0%, Test Loss: 1.07905369438231\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.04 seconds\n",
      "epoch: 5 average loss: 1.125\n",
      "Test Accuracy : 71.2%, Test Loss: 1.026860449463129\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.02 seconds\n",
      "epoch: 6 average loss: 1.080\n",
      "Test Accuracy : 71.2%, Test Loss: 1.0004010666161776\n",
      "Epoch Time (Training + Test) = 9.07 seconds\n",
      "epoch: 7 average loss: 1.039\n",
      "Test Accuracy : 72.6%, Test Loss: 0.9735543038696051\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.08 seconds\n",
      "epoch: 8 average loss: 0.999\n",
      "Test Accuracy : 72.0%, Test Loss: 0.937943659722805\n",
      "Epoch Time (Training + Test) = 9.03 seconds\n",
      "epoch: 9 average loss: 0.986\n",
      "Test Accuracy : 72.1%, Test Loss: 0.9307738244533539\n",
      "Epoch Time (Training + Test) = 9.01 seconds\n",
      "epoch: 10 average loss: 0.942\n",
      "Test Accuracy : 72.4%, Test Loss: 0.8996560592204332\n",
      "Epoch Time (Training + Test) = 9.06 seconds\n",
      "epoch: 11 average loss: 0.926\n",
      "Test Accuracy : 72.8%, Test Loss: 0.8977632708847523\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.18 seconds\n",
      "epoch: 12 average loss: 0.916\n",
      "Test Accuracy : 72.8%, Test Loss: 0.8831059765070677\n",
      "Epoch Time (Training + Test) = 9.14 seconds\n",
      "epoch: 13 average loss: 0.893\n",
      "Test Accuracy : 73.5%, Test Loss: 0.8805612940341234\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.10 seconds\n",
      "epoch: 14 average loss: 0.882\n",
      "Test Accuracy : 74.4%, Test Loss: 0.8455114774405956\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.07 seconds\n",
      "epoch: 15 average loss: 0.862\n",
      "Test Accuracy : 74.4%, Test Loss: 0.8481769878417253\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.09 seconds\n",
      "epoch: 16 average loss: 0.843\n",
      "Test Accuracy : 73.2%, Test Loss: 0.8512496650218964\n",
      "Epoch Time (Training + Test) = 9.02 seconds\n",
      "epoch: 17 average loss: 0.833\n",
      "Test Accuracy : 72.9%, Test Loss: 0.8691344391554594\n",
      "Epoch Time (Training + Test) = 9.01 seconds\n",
      "epoch: 18 average loss: 0.811\n",
      "Test Accuracy : 75.0%, Test Loss: 0.8347697909921408\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.10 seconds\n",
      "epoch: 19 average loss: 0.812\n",
      "Test Accuracy : 74.7%, Test Loss: 0.8104919046163559\n",
      "Epoch Time (Training + Test) = 9.02 seconds\n",
      "epoch: 20 average loss: 0.777\n",
      "Test Accuracy : 74.7%, Test Loss: 0.8409406915307045\n",
      "Epoch Time (Training + Test) = 9.06 seconds\n",
      "epoch: 21 average loss: 0.762\n",
      "Test Accuracy : 73.7%, Test Loss: 0.8313503041863441\n",
      "Epoch Time (Training + Test) = 9.05 seconds\n",
      "epoch: 22 average loss: 0.760\n",
      "Test Accuracy : 75.2%, Test Loss: 0.7986831460148096\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.10 seconds\n",
      "epoch: 23 average loss: 0.751\n",
      "Test Accuracy : 74.8%, Test Loss: 0.8110533654689789\n",
      "Epoch Time (Training + Test) = 9.09 seconds\n",
      "epoch: 24 average loss: 0.732\n",
      "Test Accuracy : 75.6%, Test Loss: 0.7803839202970266\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.04 seconds\n",
      "epoch: 25 average loss: 0.726\n",
      "Test Accuracy : 75.2%, Test Loss: 0.7899916972965002\n",
      "Epoch Time (Training + Test) = 9.02 seconds\n",
      "epoch: 26 average loss: 0.722\n",
      "Test Accuracy : 73.6%, Test Loss: 0.8231964614242315\n",
      "Epoch Time (Training + Test) = 9.05 seconds\n",
      "epoch: 27 average loss: 0.709\n",
      "Test Accuracy : 75.1%, Test Loss: 0.7923254072666168\n",
      "Epoch Time (Training + Test) = 9.08 seconds\n",
      "epoch: 28 average loss: 0.701\n",
      "Test Accuracy : 75.1%, Test Loss: 0.7896410264074802\n",
      "Epoch Time (Training + Test) = 9.06 seconds\n",
      "epoch: 29 average loss: 0.695\n",
      "Test Accuracy : 74.0%, Test Loss: 0.8069303873926401\n",
      "Epoch Time (Training + Test) = 9.08 seconds\n",
      "epoch: 30 average loss: 0.683\n",
      "Test Accuracy : 74.9%, Test Loss: 0.7966353669762611\n",
      "Epoch Time (Training + Test) = 9.02 seconds\n",
      "epoch: 31 average loss: 0.676\n",
      "Test Accuracy : 75.3%, Test Loss: 0.7804105523973703\n",
      "Epoch Time (Training + Test) = 9.07 seconds\n",
      "epoch: 32 average loss: 0.664\n",
      "Test Accuracy : 74.5%, Test Loss: 0.8005280811339617\n",
      "Epoch Time (Training + Test) = 9.07 seconds\n",
      "epoch: 33 average loss: 0.659\n",
      "Test Accuracy : 74.5%, Test Loss: 0.8341162037104368\n",
      "Epoch Time (Training + Test) = 9.04 seconds\n",
      "epoch: 34 average loss: 0.654\n",
      "Test Accuracy : 74.8%, Test Loss: 0.799628458917141\n",
      "Epoch Time (Training + Test) = 9.08 seconds\n",
      "epoch: 35 average loss: 0.639\n",
      "Test Accuracy : 75.4%, Test Loss: 0.7960180230438709\n",
      "Epoch Time (Training + Test) = 9.02 seconds\n",
      "epoch: 36 average loss: 0.626\n",
      "Test Accuracy : 75.3%, Test Loss: 0.793052714318037\n",
      "Epoch Time (Training + Test) = 9.09 seconds\n",
      "epoch: 37 average loss: 0.622\n",
      "Test Accuracy : 73.9%, Test Loss: 0.8024981580674648\n",
      "Epoch Time (Training + Test) = 9.09 seconds\n",
      "conv2\n",
      "conv3\n",
      "conv2\n",
      "conv3\n",
      "conv2\n",
      "conv3\n",
      "Pruned [0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8]% of weights at epoch 37\n",
      "epoch: 38 average loss: 1.162\n",
      "Test Accuracy : 69.0%, Test Loss: 0.9681526944041252\n",
      "Epoch Time (Training + Test) = 9.05 seconds\n",
      "epoch: 39 average loss: 0.971\n",
      "Test Accuracy : 70.8%, Test Loss: 0.9238944873213768\n",
      "Epoch Time (Training + Test) = 9.01 seconds\n",
      "epoch: 40 average loss: 0.923\n",
      "Test Accuracy : 71.2%, Test Loss: 0.8854571375995874\n",
      "Epoch Time (Training + Test) = 9.06 seconds\n",
      "epoch: 41 average loss: 0.918\n",
      "Test Accuracy : 71.3%, Test Loss: 0.9100971966981888\n",
      "Epoch Time (Training + Test) = 9.05 seconds\n",
      "epoch: 42 average loss: 0.899\n",
      "Test Accuracy : 73.0%, Test Loss: 0.8889724537730217\n",
      "Epoch Time (Training + Test) = 9.01 seconds\n",
      "epoch: 43 average loss: 0.872\n",
      "Test Accuracy : 72.4%, Test Loss: 0.8567459136247635\n",
      "Epoch Time (Training + Test) = 9.00 seconds\n",
      "epoch: 44 average loss: 0.865\n",
      "Test Accuracy : 73.0%, Test Loss: 0.8577482216060162\n",
      "Epoch Time (Training + Test) = 8.99 seconds\n",
      "epoch: 45 average loss: 0.875\n",
      "Test Accuracy : 72.1%, Test Loss: 0.8761745877563953\n",
      "Epoch Time (Training + Test) = 9.04 seconds\n",
      "epoch: 46 average loss: 0.853\n",
      "Test Accuracy : 72.7%, Test Loss: 0.8518865685909986\n",
      "Epoch Time (Training + Test) = 9.00 seconds\n",
      "epoch: 47 average loss: 0.850\n",
      "Test Accuracy : 73.0%, Test Loss: 0.8489396031945944\n",
      "Epoch Time (Training + Test) = 9.03 seconds\n",
      "epoch: 48 average loss: 0.837\n",
      "Test Accuracy : 72.2%, Test Loss: 0.85866903886199\n",
      "Epoch Time (Training + Test) = 9.06 seconds\n",
      "epoch: 49 average loss: 0.827\n",
      "Test Accuracy : 73.7%, Test Loss: 0.8413185384124517\n",
      "Epoch Time (Training + Test) = 8.98 seconds\n",
      "epoch: 50 average loss: 0.820\n",
      "Test Accuracy : 71.9%, Test Loss: 0.8711982034146786\n",
      "Epoch Time (Training + Test) = 9.00 seconds\n",
      "epoch: 51 average loss: 0.812\n",
      "Test Accuracy : 74.1%, Test Loss: 0.8430090993642807\n",
      "Epoch Time (Training + Test) = 9.08 seconds\n",
      "epoch: 52 average loss: 0.831\n",
      "Test Accuracy : 73.7%, Test Loss: 0.8519502468407154\n",
      "Epoch Time (Training + Test) = 8.99 seconds\n",
      "epoch: 53 average loss: 0.826\n",
      "Test Accuracy : 71.6%, Test Loss: 0.8657249454408884\n",
      "Epoch Time (Training + Test) = 9.04 seconds\n",
      "epoch: 54 average loss: 0.808\n",
      "Test Accuracy : 72.8%, Test Loss: 0.8705917242914438\n",
      "Epoch Time (Training + Test) = 8.95 seconds\n",
      "epoch: 55 average loss: 0.816\n",
      "Test Accuracy : 73.0%, Test Loss: 0.8403946459293365\n",
      "Epoch Time (Training + Test) = 9.06 seconds\n",
      "epoch: 56 average loss: 0.807\n",
      "Test Accuracy : 73.1%, Test Loss: 0.8515809997916222\n",
      "Epoch Time (Training + Test) = 9.05 seconds\n",
      "epoch: 57 average loss: 0.814\n",
      "Test Accuracy : 71.9%, Test Loss: 0.8587810304015875\n",
      "Epoch Time (Training + Test) = 8.99 seconds\n",
      "epoch: 58 average loss: 0.815\n",
      "Test Accuracy : 71.8%, Test Loss: 0.8592242281883955\n",
      "Epoch Time (Training + Test) = 9.07 seconds\n",
      "epoch: 59 average loss: 0.806\n",
      "Test Accuracy : 71.8%, Test Loss: 0.8736698627471924\n",
      "Epoch Time (Training + Test) = 9.08 seconds\n",
      "epoch: 60 average loss: 0.804\n",
      "Test Accuracy : 73.0%, Test Loss: 0.8658454921096563\n",
      "Epoch Time (Training + Test) = 9.01 seconds\n",
      "epoch: 61 average loss: 0.801\n",
      "Test Accuracy : 73.6%, Test Loss: 0.8423011060804129\n",
      "Epoch Time (Training + Test) = 8.96 seconds\n",
      "epoch: 62 average loss: 0.788\n",
      "Test Accuracy : 74.0%, Test Loss: 0.8533434476703405\n",
      "Epoch Time (Training + Test) = 9.02 seconds\n",
      "epoch: 63 average loss: 0.789\n",
      "Test Accuracy : 73.2%, Test Loss: 0.8465855829417706\n",
      "Epoch Time (Training + Test) = 9.03 seconds\n",
      "epoch: 64 average loss: 0.785\n",
      "Test Accuracy : 72.5%, Test Loss: 0.851379519328475\n",
      "Epoch Time (Training + Test) = 9.06 seconds\n",
      "epoch: 65 average loss: 0.790\n",
      "Test Accuracy : 73.5%, Test Loss: 0.835612814873457\n",
      "Epoch Time (Training + Test) = 9.05 seconds\n",
      "epoch: 66 average loss: 0.790\n",
      "Test Accuracy : 72.1%, Test Loss: 0.8721528146415949\n",
      "Epoch Time (Training + Test) = 9.03 seconds\n",
      "epoch: 67 average loss: 0.788\n",
      "Test Accuracy : 72.1%, Test Loss: 0.8687449172139168\n",
      "Epoch Time (Training + Test) = 9.01 seconds\n",
      "epoch: 68 average loss: 0.796\n",
      "Test Accuracy : 73.1%, Test Loss: 0.8455868978053331\n",
      "Epoch Time (Training + Test) = 9.01 seconds\n",
      "epoch: 69 average loss: 0.780\n",
      "Test Accuracy : 72.5%, Test Loss: 0.8468366302549839\n",
      "Epoch Time (Training + Test) = 8.99 seconds\n",
      "epoch: 70 average loss: 0.780\n",
      "Test Accuracy : 72.5%, Test Loss: 0.8363156383857131\n",
      "Epoch Time (Training + Test) = 9.02 seconds\n",
      "epoch: 71 average loss: 0.771\n",
      "Test Accuracy : 73.5%, Test Loss: 0.8387243915349245\n",
      "Epoch Time (Training + Test) = 9.06 seconds\n",
      "epoch: 72 average loss: 0.764\n",
      "Test Accuracy : 71.6%, Test Loss: 0.8592839278280735\n",
      "Epoch Time (Training + Test) = 9.13 seconds\n",
      "epoch: 73 average loss: 0.790\n",
      "Test Accuracy : 73.6%, Test Loss: 0.8494934998452663\n",
      "Epoch Time (Training + Test) = 8.75 seconds\n",
      "epoch: 74 average loss: 0.771\n",
      "Test Accuracy : 73.9%, Test Loss: 0.828794289380312\n",
      "Epoch Time (Training + Test) = 8.72 seconds\n",
      "epoch: 75 average loss: 0.780\n",
      "Test Accuracy : 73.0%, Test Loss: 0.8554720869287848\n",
      "Epoch Time (Training + Test) = 8.77 seconds\n",
      "Data Saved to Cifar80_Prune_80.0%.csv\n",
      "Finished Training: \n",
      "Total Time 0.376424 hours\n",
      " Average Time Per Epoch 18.07 seconds\n",
      "BNN_Resnet_UniAdapt(\n",
      "  (conv1): BinarizeConv2d(3, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (bn1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "  (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(80, 160, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): BinarizeConv2d(80, 160, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(160, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): BinarizeConv2d(160, 320, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)\n",
      "  (bn2): BatchNorm1d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (bn3): BatchNorm1d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (logsoftmax): LogSoftmax(dim=-1)\n",
      "  (fc): BinarizeLinear(in_features=320, out_features=80, bias=True)\n",
      "  (adapter_dict): ModuleDict()\n",
      "  (head_bn): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (head): BinarizeLinear(in_features=640, out_features=20, bias=True)\n",
      "  (encoder_net): placeholder()\n",
      "  (uniAdaptNet): Sequential(\n",
      "    (0): uniAdapt_Net(\n",
      "      (layer1): thinBlock3(\n",
      "        (conv2): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=80, bias=False)\n",
      "        (bn2): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(80, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (layer2): thinBlock3(\n",
      "        (conv2): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=160, bias=False)\n",
      "        (bn2): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(160, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (layer3): thinBlock3(\n",
      "        (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=320, bias=False)\n",
      "        (bn2): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (pool1): AdaptiveAvgPool2d(output_size=16)\n",
      "      (pool2): AdaptiveAvgPool2d(output_size=8)\n",
      "      (pool3): AdaptiveAvgPool2d(output_size=1)\n",
      "    )\n",
      "    (1): Flatten(start_dim=1, end_dim=-1)\n",
      "  )\n",
      ")\n",
      "No lr scheduler\n",
      "Optimizer Set to New: {'state': {}, 'param_groups': [{'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': False, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]}]}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2wyr848i) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Current Best Acc</td><td>▁▇▇█████████████████████████████████████</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>epoch time (s)</td><td>▁▇▇▇▇▇███▇▇▇█▇▇▇▇▇▇█▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▆▆</td></tr><tr><td>lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_accuracy</td><td>▁▇▇█████████████████▇███████████████████</td></tr><tr><td>test_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>training_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Best Accuracy</td><td>0.7565</td></tr><tr><td>Current Best Acc</td><td>0.7565</td></tr><tr><td>Total Time (hours)</td><td>0.37642</td></tr><tr><td>epoch</td><td>75</td></tr><tr><td>epoch time (s)</td><td>8.76938</td></tr><tr><td>lr</td><td>0.001</td></tr><tr><td>test_accuracy</td><td>0.73</td></tr><tr><td>test_loss</td><td>0.85547</td></tr><tr><td>training_loss</td><td>0.78001</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">sage-pond-4</strong>: <a href=\"https://wandb.ai/johnny_suu/FINAL_PRUNING_CIFAR100/runs/2wyr848i\" target=\"_blank\">https://wandb.ai/johnny_suu/FINAL_PRUNING_CIFAR100/runs/2wyr848i</a><br/>Synced 5 W&B file(s), 1 media file(s), 1 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230510_105147-2wyr848i\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2wyr848i). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0a79a27c39b4809b34b8b9fea66546a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\John Su\\Downloads\\SydneyUni\\2022\\thesis\\Thesis\\wandb\\run-20230510_110325-2uz43ipd</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/johnny_suu/FINAL_PRUNING_CIFAR100/runs/2uz43ipd\" target=\"_blank\">lilac-universe-5</a></strong> to <a href=\"https://wandb.ai/johnny_suu/FINAL_PRUNING_CIFAR100\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "Project Name: FINAL_PRUNING_CIFAR100, Run Name Cifar80_Prune_90.0% \n",
      "\n",
      "\n",
      "Run Start : 2023-05-10 11-03-25\n",
      "start_epoch : 0\n",
      "initial_lr : 0.001\n",
      "batch_size : 64\n",
      "epochs : 75\n",
      "epoch_chkpts : []\n",
      "device : cuda:0\n",
      "criterion : CrossEntropyLoss()\n",
      "optimizer : Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: False\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "seed : None\n",
      "model_architecture : <class 'NN_Thesis.nn_classes.BNN_Resnet_UniAdapt'>\n",
      "binerised_training : True\n",
      "Number of Elements : 4547660\n",
      "Initial accuracy:\n",
      "Test Accuracy : 4.9%, Test Loss: 46.913473578775005\n",
      "best_acc.pth saved!\n",
      "Test Accuracy : 5.5%, Test Loss: 46.263484835624695\n",
      "best_acc.pth saved!\n",
      "epoch: 1 average loss: 1.645\n",
      "Test Accuracy : 66.6%, Test Loss: 1.3236247189342976\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 8.72 seconds\n",
      "epoch: 2 average loss: 1.362\n",
      "Test Accuracy : 69.0%, Test Loss: 1.2240473236888647\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 8.83 seconds\n",
      "epoch: 3 average loss: 1.268\n",
      "Test Accuracy : 69.2%, Test Loss: 1.1315806899219751\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 8.74 seconds\n",
      "epoch: 4 average loss: 1.190\n",
      "Test Accuracy : 70.5%, Test Loss: 1.089297452941537\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 8.95 seconds\n",
      "epoch: 5 average loss: 1.135\n",
      "Test Accuracy : 70.8%, Test Loss: 1.0501213911920786\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 8.78 seconds\n",
      "epoch: 6 average loss: 1.081\n",
      "Test Accuracy : 70.6%, Test Loss: 1.0304740946739912\n",
      "Epoch Time (Training + Test) = 8.73 seconds\n",
      "epoch: 7 average loss: 1.051\n",
      "Test Accuracy : 72.3%, Test Loss: 0.9787686821073294\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 8.71 seconds\n",
      "epoch: 8 average loss: 1.012\n",
      "Test Accuracy : 70.7%, Test Loss: 0.9609019346535206\n",
      "Epoch Time (Training + Test) = 8.82 seconds\n",
      "epoch: 9 average loss: 0.982\n",
      "Test Accuracy : 71.5%, Test Loss: 0.928375668823719\n",
      "Epoch Time (Training + Test) = 8.69 seconds\n",
      "epoch: 10 average loss: 0.961\n",
      "Test Accuracy : 72.0%, Test Loss: 0.9222586695104837\n",
      "Epoch Time (Training + Test) = 8.79 seconds\n",
      "epoch: 11 average loss: 0.939\n",
      "Test Accuracy : 71.8%, Test Loss: 0.9097331427037716\n",
      "Epoch Time (Training + Test) = 8.70 seconds\n",
      "epoch: 12 average loss: 0.932\n",
      "Test Accuracy : 72.8%, Test Loss: 0.8928880244493484\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 8.87 seconds\n",
      "epoch: 13 average loss: 0.911\n",
      "Test Accuracy : 72.7%, Test Loss: 0.8881567511707544\n",
      "Epoch Time (Training + Test) = 8.84 seconds\n",
      "epoch: 14 average loss: 0.875\n",
      "Test Accuracy : 73.6%, Test Loss: 0.8626323658972979\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 8.71 seconds\n",
      "epoch: 15 average loss: 0.857\n",
      "Test Accuracy : 73.0%, Test Loss: 0.8617104310542345\n",
      "Epoch Time (Training + Test) = 8.77 seconds\n",
      "epoch: 16 average loss: 0.850\n",
      "Test Accuracy : 72.4%, Test Loss: 0.8772466536611319\n",
      "Epoch Time (Training + Test) = 8.73 seconds\n",
      "epoch: 17 average loss: 0.834\n",
      "Test Accuracy : 73.0%, Test Loss: 0.8524982836097479\n",
      "Epoch Time (Training + Test) = 8.80 seconds\n",
      "epoch: 18 average loss: 0.821\n",
      "Test Accuracy : 75.2%, Test Loss: 0.8273237366229296\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 8.75 seconds\n",
      "epoch: 19 average loss: 0.813\n",
      "Test Accuracy : 74.7%, Test Loss: 0.8224615883082151\n",
      "Epoch Time (Training + Test) = 8.79 seconds\n",
      "epoch: 20 average loss: 0.792\n",
      "Test Accuracy : 73.3%, Test Loss: 0.8418197985738516\n",
      "Epoch Time (Training + Test) = 8.81 seconds\n",
      "epoch: 21 average loss: 0.796\n",
      "Test Accuracy : 74.8%, Test Loss: 0.8136255312711\n",
      "Epoch Time (Training + Test) = 8.88 seconds\n",
      "epoch: 22 average loss: 0.771\n",
      "Test Accuracy : 73.9%, Test Loss: 0.8224761262536049\n",
      "Epoch Time (Training + Test) = 8.67 seconds\n",
      "epoch: 23 average loss: 0.755\n",
      "Test Accuracy : 74.7%, Test Loss: 0.7995015885680914\n",
      "Epoch Time (Training + Test) = 8.68 seconds\n",
      "epoch: 24 average loss: 0.739\n",
      "Test Accuracy : 74.5%, Test Loss: 0.818698350340128\n",
      "Epoch Time (Training + Test) = 8.83 seconds\n",
      "epoch: 25 average loss: 0.756\n",
      "Test Accuracy : 74.2%, Test Loss: 0.8116299156099558\n",
      "Epoch Time (Training + Test) = 8.65 seconds\n",
      "epoch: 26 average loss: 0.727\n",
      "Test Accuracy : 74.1%, Test Loss: 0.8151139300316572\n",
      "Epoch Time (Training + Test) = 8.72 seconds\n",
      "epoch: 27 average loss: 0.713\n",
      "Test Accuracy : 74.6%, Test Loss: 0.8026851490139961\n",
      "Epoch Time (Training + Test) = 8.74 seconds\n",
      "epoch: 28 average loss: 0.716\n",
      "Test Accuracy : 74.6%, Test Loss: 0.804583165794611\n",
      "Epoch Time (Training + Test) = 8.66 seconds\n",
      "epoch: 29 average loss: 0.703\n",
      "Test Accuracy : 75.4%, Test Loss: 0.7945721466094255\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 8.76 seconds\n",
      "epoch: 30 average loss: 0.703\n",
      "Test Accuracy : 74.3%, Test Loss: 0.8035034779459238\n",
      "Epoch Time (Training + Test) = 8.79 seconds\n",
      "epoch: 31 average loss: 0.680\n",
      "Test Accuracy : 74.8%, Test Loss: 0.7787160146981478\n",
      "Epoch Time (Training + Test) = 8.69 seconds\n",
      "epoch: 32 average loss: 0.688\n",
      "Test Accuracy : 74.8%, Test Loss: 0.8004276920109987\n",
      "Epoch Time (Training + Test) = 8.73 seconds\n",
      "epoch: 33 average loss: 0.670\n",
      "Test Accuracy : 74.5%, Test Loss: 0.7895749472081661\n",
      "Epoch Time (Training + Test) = 8.76 seconds\n",
      "epoch: 34 average loss: 0.663\n",
      "Test Accuracy : 75.1%, Test Loss: 0.7901599295437336\n",
      "Epoch Time (Training + Test) = 8.87 seconds\n",
      "epoch: 35 average loss: 0.655\n",
      "Test Accuracy : 75.0%, Test Loss: 0.8011702131479979\n",
      "Epoch Time (Training + Test) = 8.72 seconds\n",
      "epoch: 36 average loss: 0.653\n",
      "Test Accuracy : 75.6%, Test Loss: 0.7990819029510021\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 8.84 seconds\n",
      "epoch: 37 average loss: 0.651\n",
      "Test Accuracy : 75.6%, Test Loss: 0.7863028636202216\n",
      "Epoch Time (Training + Test) = 8.81 seconds\n",
      "conv2\n",
      "conv3\n",
      "conv2\n",
      "conv3\n",
      "conv2\n",
      "conv3\n",
      "Pruned [0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9]% of weights at epoch 37\n",
      "epoch: 38 average loss: 1.316\n",
      "Test Accuracy : 66.3%, Test Loss: 1.0684241130948067\n",
      "Epoch Time (Training + Test) = 8.74 seconds\n",
      "epoch: 39 average loss: 1.091\n",
      "Test Accuracy : 68.8%, Test Loss: 0.9994958713650703\n",
      "Epoch Time (Training + Test) = 8.77 seconds\n",
      "epoch: 40 average loss: 1.044\n",
      "Test Accuracy : 69.3%, Test Loss: 0.980167830362916\n",
      "Epoch Time (Training + Test) = 8.75 seconds\n",
      "epoch: 41 average loss: 1.026\n",
      "Test Accuracy : 69.7%, Test Loss: 0.975088344886899\n",
      "Epoch Time (Training + Test) = 8.75 seconds\n",
      "epoch: 42 average loss: 0.995\n",
      "Test Accuracy : 68.9%, Test Loss: 0.9731979444622993\n",
      "Epoch Time (Training + Test) = 8.71 seconds\n",
      "epoch: 43 average loss: 0.972\n",
      "Test Accuracy : 70.2%, Test Loss: 0.9411255642771721\n",
      "Epoch Time (Training + Test) = 8.68 seconds\n",
      "epoch: 44 average loss: 0.967\n",
      "Test Accuracy : 70.9%, Test Loss: 0.9163884837180376\n",
      "Epoch Time (Training + Test) = 8.79 seconds\n",
      "epoch: 45 average loss: 0.964\n",
      "Test Accuracy : 69.9%, Test Loss: 0.9346358217298985\n",
      "Epoch Time (Training + Test) = 8.71 seconds\n",
      "epoch: 46 average loss: 0.962\n",
      "Test Accuracy : 70.9%, Test Loss: 0.9198920596390963\n",
      "Epoch Time (Training + Test) = 8.72 seconds\n",
      "epoch: 47 average loss: 0.938\n",
      "Test Accuracy : 70.4%, Test Loss: 0.9242404755204916\n",
      "Epoch Time (Training + Test) = 8.77 seconds\n",
      "epoch: 48 average loss: 0.944\n",
      "Test Accuracy : 71.8%, Test Loss: 0.9279070384800434\n",
      "Epoch Time (Training + Test) = 8.73 seconds\n",
      "epoch: 49 average loss: 0.942\n",
      "Test Accuracy : 71.1%, Test Loss: 0.9054223652929068\n",
      "Epoch Time (Training + Test) = 8.79 seconds\n",
      "epoch: 50 average loss: 0.922\n",
      "Test Accuracy : 70.5%, Test Loss: 0.9156472496688366\n",
      "Epoch Time (Training + Test) = 8.82 seconds\n",
      "epoch: 51 average loss: 0.934\n",
      "Test Accuracy : 70.4%, Test Loss: 0.9271403662860394\n",
      "Epoch Time (Training + Test) = 8.74 seconds\n",
      "epoch: 52 average loss: 0.927\n",
      "Test Accuracy : 70.7%, Test Loss: 0.9312487598508596\n",
      "Epoch Time (Training + Test) = 8.79 seconds\n",
      "epoch: 53 average loss: 0.920\n",
      "Test Accuracy : 70.2%, Test Loss: 0.9144216850399971\n",
      "Epoch Time (Training + Test) = 8.72 seconds\n",
      "epoch: 54 average loss: 0.926\n",
      "Test Accuracy : 71.5%, Test Loss: 0.9056939277797937\n",
      "Epoch Time (Training + Test) = 8.70 seconds\n",
      "epoch: 55 average loss: 0.912\n",
      "Test Accuracy : 71.4%, Test Loss: 0.9074961133301258\n",
      "Epoch Time (Training + Test) = 8.75 seconds\n",
      "epoch: 56 average loss: 0.914\n",
      "Test Accuracy : 71.5%, Test Loss: 0.9146249536424875\n",
      "Epoch Time (Training + Test) = 8.72 seconds\n",
      "epoch: 57 average loss: 0.912\n",
      "Test Accuracy : 71.2%, Test Loss: 0.9042103718966246\n",
      "Epoch Time (Training + Test) = 8.71 seconds\n",
      "epoch: 58 average loss: 0.900\n",
      "Test Accuracy : 72.6%, Test Loss: 0.8852144796401262\n",
      "Epoch Time (Training + Test) = 8.76 seconds\n",
      "epoch: 59 average loss: 0.910\n",
      "Test Accuracy : 70.6%, Test Loss: 0.89329769089818\n",
      "Epoch Time (Training + Test) = 8.72 seconds\n",
      "epoch: 60 average loss: 0.891\n",
      "Test Accuracy : 71.0%, Test Loss: 0.8976263087242842\n",
      "Epoch Time (Training + Test) = 8.71 seconds\n",
      "epoch: 61 average loss: 0.911\n",
      "Test Accuracy : 70.9%, Test Loss: 0.8873702958226204\n",
      "Epoch Time (Training + Test) = 8.74 seconds\n",
      "epoch: 62 average loss: 0.902\n",
      "Test Accuracy : 72.2%, Test Loss: 0.8785678837448359\n",
      "Epoch Time (Training + Test) = 8.67 seconds\n",
      "epoch: 63 average loss: 0.888\n",
      "Test Accuracy : 70.8%, Test Loss: 0.9135034885257483\n",
      "Epoch Time (Training + Test) = 8.73 seconds\n",
      "epoch: 64 average loss: 0.908\n",
      "Test Accuracy : 69.7%, Test Loss: 0.9009222649037838\n",
      "Epoch Time (Training + Test) = 8.83 seconds\n",
      "epoch: 65 average loss: 0.892\n",
      "Test Accuracy : 71.1%, Test Loss: 0.894105052575469\n",
      "Epoch Time (Training + Test) = 8.80 seconds\n",
      "epoch: 66 average loss: 0.880\n",
      "Test Accuracy : 71.8%, Test Loss: 0.9011462572962046\n",
      "Epoch Time (Training + Test) = 8.71 seconds\n",
      "epoch: 67 average loss: 0.888\n",
      "Test Accuracy : 73.4%, Test Loss: 0.8633159715682268\n",
      "Epoch Time (Training + Test) = 8.77 seconds\n",
      "epoch: 68 average loss: 0.871\n",
      "Test Accuracy : 72.0%, Test Loss: 0.8807600885629654\n",
      "Epoch Time (Training + Test) = 8.70 seconds\n",
      "epoch: 69 average loss: 0.886\n",
      "Test Accuracy : 72.4%, Test Loss: 0.8727516401559114\n",
      "Epoch Time (Training + Test) = 8.70 seconds\n",
      "epoch: 70 average loss: 0.880\n",
      "Test Accuracy : 72.0%, Test Loss: 0.8957835342735052\n",
      "Epoch Time (Training + Test) = 8.70 seconds\n",
      "epoch: 71 average loss: 0.880\n",
      "Test Accuracy : 72.8%, Test Loss: 0.8694942556321621\n",
      "Epoch Time (Training + Test) = 8.81 seconds\n",
      "epoch: 72 average loss: 0.878\n",
      "Test Accuracy : 72.5%, Test Loss: 0.8620742037892342\n",
      "Epoch Time (Training + Test) = 8.82 seconds\n",
      "epoch: 73 average loss: 0.885\n",
      "Test Accuracy : 71.7%, Test Loss: 0.8856008760631084\n",
      "Epoch Time (Training + Test) = 8.76 seconds\n",
      "epoch: 74 average loss: 0.876\n",
      "Test Accuracy : 72.3%, Test Loss: 0.8792344368994236\n",
      "Epoch Time (Training + Test) = 8.74 seconds\n",
      "epoch: 75 average loss: 0.876\n",
      "Test Accuracy : 71.1%, Test Loss: 0.8989768736064434\n",
      "Epoch Time (Training + Test) = 8.72 seconds\n",
      "Data Saved to Cifar80_Prune_90.0%.csv\n",
      "Finished Training: \n",
      "Total Time 0.364775 hours\n",
      " Average Time Per Epoch 17.51 seconds\n",
      "BNN_Resnet_UniAdapt(\n",
      "  (conv1): BinarizeConv2d(3, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (bn1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "  (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(80, 160, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): BinarizeConv2d(80, 160, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(160, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): BinarizeConv2d(160, 320, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)\n",
      "  (bn2): BatchNorm1d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (bn3): BatchNorm1d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (logsoftmax): LogSoftmax(dim=-1)\n",
      "  (fc): BinarizeLinear(in_features=320, out_features=80, bias=True)\n",
      "  (adapter_dict): ModuleDict()\n",
      "  (head_bn): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (head): BinarizeLinear(in_features=640, out_features=20, bias=True)\n",
      "  (encoder_net): placeholder()\n",
      "  (uniAdaptNet): Sequential(\n",
      "    (0): uniAdapt_Net(\n",
      "      (layer1): thinBlock3(\n",
      "        (conv2): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=80, bias=False)\n",
      "        (bn2): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(80, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (layer2): thinBlock3(\n",
      "        (conv2): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=160, bias=False)\n",
      "        (bn2): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(160, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (layer3): thinBlock3(\n",
      "        (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=320, bias=False)\n",
      "        (bn2): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (pool1): AdaptiveAvgPool2d(output_size=16)\n",
      "      (pool2): AdaptiveAvgPool2d(output_size=8)\n",
      "      (pool3): AdaptiveAvgPool2d(output_size=1)\n",
      "    )\n",
      "    (1): Flatten(start_dim=1, end_dim=-1)\n",
      "  )\n",
      ")\n",
      "No lr scheduler\n",
      "Optimizer Set to New: {'state': {}, 'param_groups': [{'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': False, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]}]}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2uz43ipd) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Current Best Acc</td><td>▁▇▇█████████████████████████████████████</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>epoch time (s)</td><td>▁▇▇▇▇▇▇█▇▇▇█▇▆▇▆▇▇██▇▇▇▇▇▇█▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_accuracy</td><td>▁▇▇█████████████████▇▇▇███▇▇▇██▇████████</td></tr><tr><td>test_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>training_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Best Accuracy</td><td>0.7565</td></tr><tr><td>Current Best Acc</td><td>0.7565</td></tr><tr><td>Total Time (hours)</td><td>0.36478</td></tr><tr><td>epoch</td><td>75</td></tr><tr><td>epoch time (s)</td><td>8.71941</td></tr><tr><td>lr</td><td>0.001</td></tr><tr><td>test_accuracy</td><td>0.711</td></tr><tr><td>test_loss</td><td>0.89898</td></tr><tr><td>training_loss</td><td>0.87583</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">lilac-universe-5</strong>: <a href=\"https://wandb.ai/johnny_suu/FINAL_PRUNING_CIFAR100/runs/2uz43ipd\" target=\"_blank\">https://wandb.ai/johnny_suu/FINAL_PRUNING_CIFAR100/runs/2uz43ipd</a><br/>Synced 5 W&B file(s), 1 media file(s), 1 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230510_110325-2uz43ipd\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2uz43ipd). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5b228c4277441d7a9723101aad11914",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\John Su\\Downloads\\SydneyUni\\2022\\thesis\\Thesis\\wandb\\run-20230510_111443-5eqsl9ng</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/johnny_suu/FINAL_PRUNING_CIFAR100/runs/5eqsl9ng\" target=\"_blank\">zesty-cherry-6</a></strong> to <a href=\"https://wandb.ai/johnny_suu/FINAL_PRUNING_CIFAR100\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "Project Name: FINAL_PRUNING_CIFAR100, Run Name Cifar80_Prune_95.0% \n",
      "\n",
      "\n",
      "Run Start : 2023-05-10 11-14-43\n",
      "start_epoch : 0\n",
      "initial_lr : 0.001\n",
      "batch_size : 64\n",
      "epochs : 75\n",
      "epoch_chkpts : []\n",
      "device : cuda:0\n",
      "criterion : CrossEntropyLoss()\n",
      "optimizer : Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: False\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "seed : None\n",
      "model_architecture : <class 'NN_Thesis.nn_classes.BNN_Resnet_UniAdapt'>\n",
      "binerised_training : True\n",
      "Number of Elements : 4547660\n",
      "Initial accuracy:\n",
      "Test Accuracy : 3.5%, Test Loss: 52.68627118153177\n",
      "best_acc.pth saved!\n",
      "Test Accuracy : 3.2%, Test Loss: 52.512542366981506\n",
      "epoch: 1 average loss: 1.653\n",
      "Test Accuracy : 67.2%, Test Loss: 1.3136250413954258\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 8.82 seconds\n",
      "epoch: 2 average loss: 1.350\n",
      "Test Accuracy : 69.2%, Test Loss: 1.2027185447514057\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 8.87 seconds\n",
      "epoch: 3 average loss: 1.254\n",
      "Test Accuracy : 70.0%, Test Loss: 1.1286515407264233\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 8.75 seconds\n",
      "epoch: 4 average loss: 1.192\n",
      "Test Accuracy : 70.9%, Test Loss: 1.0785705223679543\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 8.76 seconds\n",
      "epoch: 5 average loss: 1.134\n",
      "Test Accuracy : 72.2%, Test Loss: 1.0452470742166042\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 8.75 seconds\n",
      "epoch: 6 average loss: 1.086\n",
      "Test Accuracy : 71.1%, Test Loss: 1.0127207543700933\n",
      "Epoch Time (Training + Test) = 8.75 seconds\n",
      "epoch: 7 average loss: 1.046\n",
      "Test Accuracy : 72.0%, Test Loss: 0.99086032807827\n",
      "Epoch Time (Training + Test) = 8.70 seconds\n",
      "epoch: 8 average loss: 1.008\n",
      "Test Accuracy : 73.9%, Test Loss: 0.9448584318161011\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 8.83 seconds\n",
      "epoch: 9 average loss: 0.984\n",
      "Test Accuracy : 72.9%, Test Loss: 0.9334404151886702\n",
      "Epoch Time (Training + Test) = 8.82 seconds\n",
      "epoch: 10 average loss: 0.957\n",
      "Test Accuracy : 72.7%, Test Loss: 0.8973538428544998\n",
      "Epoch Time (Training + Test) = 8.74 seconds\n",
      "epoch: 11 average loss: 0.943\n",
      "Test Accuracy : 73.8%, Test Loss: 0.8845482468605042\n",
      "Epoch Time (Training + Test) = 8.79 seconds\n",
      "epoch: 12 average loss: 0.916\n",
      "Test Accuracy : 74.2%, Test Loss: 0.8940127231180668\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 8.86 seconds\n",
      "epoch: 13 average loss: 0.885\n",
      "Test Accuracy : 72.7%, Test Loss: 0.8806086052209139\n",
      "Epoch Time (Training + Test) = 8.74 seconds\n",
      "epoch: 14 average loss: 0.889\n",
      "Test Accuracy : 73.3%, Test Loss: 0.8651237618178129\n",
      "Epoch Time (Training + Test) = 8.72 seconds\n",
      "epoch: 15 average loss: 0.862\n",
      "Test Accuracy : 73.2%, Test Loss: 0.8711917903274298\n",
      "Epoch Time (Training + Test) = 8.73 seconds\n",
      "epoch: 16 average loss: 0.843\n",
      "Test Accuracy : 75.4%, Test Loss: 0.8363323286175728\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 8.72 seconds\n",
      "epoch: 17 average loss: 0.832\n",
      "Test Accuracy : 73.4%, Test Loss: 0.8478710241615772\n",
      "Epoch Time (Training + Test) = 8.74 seconds\n",
      "epoch: 18 average loss: 0.817\n",
      "Test Accuracy : 73.2%, Test Loss: 0.8415573872625828\n",
      "Epoch Time (Training + Test) = 8.70 seconds\n",
      "epoch: 19 average loss: 0.803\n",
      "Test Accuracy : 73.3%, Test Loss: 0.831755293533206\n",
      "Epoch Time (Training + Test) = 8.83 seconds\n",
      "epoch: 20 average loss: 0.799\n",
      "Test Accuracy : 74.7%, Test Loss: 0.8269955683499575\n",
      "Epoch Time (Training + Test) = 8.69 seconds\n",
      "epoch: 21 average loss: 0.777\n",
      "Test Accuracy : 74.8%, Test Loss: 0.8173311445862055\n",
      "Epoch Time (Training + Test) = 8.79 seconds\n",
      "epoch: 22 average loss: 0.772\n",
      "Test Accuracy : 75.9%, Test Loss: 0.8203211724758148\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 8.75 seconds\n",
      "epoch: 23 average loss: 0.766\n",
      "Test Accuracy : 73.5%, Test Loss: 0.8285377714782953\n",
      "Epoch Time (Training + Test) = 8.79 seconds\n",
      "epoch: 24 average loss: 0.769\n",
      "Test Accuracy : 74.0%, Test Loss: 0.830196475610137\n",
      "Epoch Time (Training + Test) = 8.68 seconds\n",
      "epoch: 25 average loss: 0.746\n",
      "Test Accuracy : 74.6%, Test Loss: 0.811636071652174\n",
      "Epoch Time (Training + Test) = 8.84 seconds\n",
      "epoch: 26 average loss: 0.744\n",
      "Test Accuracy : 75.1%, Test Loss: 0.8156393393874168\n",
      "Epoch Time (Training + Test) = 8.74 seconds\n",
      "epoch: 27 average loss: 0.726\n",
      "Test Accuracy : 74.9%, Test Loss: 0.8332759011536837\n",
      "Epoch Time (Training + Test) = 8.77 seconds\n",
      "epoch: 28 average loss: 0.727\n",
      "Test Accuracy : 74.5%, Test Loss: 0.8066263385117054\n",
      "Epoch Time (Training + Test) = 8.69 seconds\n",
      "epoch: 29 average loss: 0.715\n",
      "Test Accuracy : 74.7%, Test Loss: 0.8327660094946623\n",
      "Epoch Time (Training + Test) = 8.83 seconds\n",
      "epoch: 30 average loss: 0.704\n",
      "Test Accuracy : 75.0%, Test Loss: 0.80466103926301\n",
      "Epoch Time (Training + Test) = 8.73 seconds\n",
      "epoch: 31 average loss: 0.694\n",
      "Test Accuracy : 75.2%, Test Loss: 0.8050192091614008\n",
      "Epoch Time (Training + Test) = 8.76 seconds\n",
      "epoch: 32 average loss: 0.680\n",
      "Test Accuracy : 75.2%, Test Loss: 0.7806036416441202\n",
      "Epoch Time (Training + Test) = 8.72 seconds\n",
      "epoch: 33 average loss: 0.673\n",
      "Test Accuracy : 76.0%, Test Loss: 0.7794178510084748\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 8.84 seconds\n",
      "epoch: 34 average loss: 0.659\n",
      "Test Accuracy : 75.5%, Test Loss: 0.7714155670255423\n",
      "Epoch Time (Training + Test) = 8.72 seconds\n",
      "epoch: 35 average loss: 0.654\n",
      "Test Accuracy : 75.4%, Test Loss: 0.7944580186158419\n",
      "Epoch Time (Training + Test) = 8.76 seconds\n",
      "epoch: 36 average loss: 0.655\n",
      "Test Accuracy : 75.2%, Test Loss: 0.7997961584478617\n",
      "Epoch Time (Training + Test) = 8.67 seconds\n",
      "epoch: 37 average loss: 0.656\n",
      "Test Accuracy : 74.5%, Test Loss: 0.8015489690005779\n",
      "Epoch Time (Training + Test) = 8.67 seconds\n",
      "conv2\n",
      "conv3\n",
      "conv2\n",
      "conv3\n",
      "conv2\n",
      "conv3\n",
      "Pruned [0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]% of weights at epoch 37\n",
      "epoch: 38 average loss: 1.471\n",
      "Test Accuracy : 63.2%, Test Loss: 1.1736254543066025\n",
      "Epoch Time (Training + Test) = 8.80 seconds\n",
      "epoch: 39 average loss: 1.184\n",
      "Test Accuracy : 65.5%, Test Loss: 1.07777813449502\n",
      "Epoch Time (Training + Test) = 8.73 seconds\n",
      "epoch: 40 average loss: 1.129\n",
      "Test Accuracy : 67.8%, Test Loss: 1.048267787322402\n",
      "Epoch Time (Training + Test) = 8.70 seconds\n",
      "epoch: 41 average loss: 1.113\n",
      "Test Accuracy : 66.4%, Test Loss: 1.0450604315847158\n",
      "Epoch Time (Training + Test) = 8.75 seconds\n",
      "epoch: 42 average loss: 1.075\n",
      "Test Accuracy : 67.5%, Test Loss: 1.020392945036292\n",
      "Epoch Time (Training + Test) = 8.71 seconds\n",
      "epoch: 43 average loss: 1.083\n",
      "Test Accuracy : 67.8%, Test Loss: 1.0131624396890402\n",
      "Epoch Time (Training + Test) = 8.71 seconds\n",
      "epoch: 44 average loss: 1.068\n",
      "Test Accuracy : 67.8%, Test Loss: 1.0298450253903866\n",
      "Epoch Time (Training + Test) = 8.74 seconds\n",
      "epoch: 45 average loss: 1.056\n",
      "Test Accuracy : 67.3%, Test Loss: 0.994636595249176\n",
      "Epoch Time (Training + Test) = 8.73 seconds\n",
      "epoch: 46 average loss: 1.051\n",
      "Test Accuracy : 69.2%, Test Loss: 1.0020876172930002\n",
      "Epoch Time (Training + Test) = 8.78 seconds\n",
      "epoch: 47 average loss: 1.045\n",
      "Test Accuracy : 69.2%, Test Loss: 0.9663172829896212\n",
      "Epoch Time (Training + Test) = 8.69 seconds\n",
      "epoch: 48 average loss: 1.050\n",
      "Test Accuracy : 69.4%, Test Loss: 0.9688564725220203\n",
      "Epoch Time (Training + Test) = 8.69 seconds\n",
      "epoch: 49 average loss: 1.029\n",
      "Test Accuracy : 68.0%, Test Loss: 0.9998797215521336\n",
      "Epoch Time (Training + Test) = 8.70 seconds\n",
      "epoch: 50 average loss: 1.032\n",
      "Test Accuracy : 66.8%, Test Loss: 1.0149216651916504\n",
      "Epoch Time (Training + Test) = 8.75 seconds\n",
      "epoch: 51 average loss: 1.016\n",
      "Test Accuracy : 68.2%, Test Loss: 1.0039310194551945\n",
      "Epoch Time (Training + Test) = 8.72 seconds\n",
      "epoch: 52 average loss: 1.019\n",
      "Test Accuracy : 67.7%, Test Loss: 0.990359041839838\n",
      "Epoch Time (Training + Test) = 8.75 seconds\n",
      "epoch: 53 average loss: 1.015\n",
      "Test Accuracy : 69.0%, Test Loss: 0.955223286524415\n",
      "Epoch Time (Training + Test) = 8.78 seconds\n",
      "epoch: 54 average loss: 1.015\n",
      "Test Accuracy : 70.0%, Test Loss: 0.9674214646220207\n",
      "Epoch Time (Training + Test) = 8.69 seconds\n",
      "epoch: 55 average loss: 1.022\n",
      "Test Accuracy : 69.5%, Test Loss: 0.944269098341465\n",
      "Epoch Time (Training + Test) = 8.78 seconds\n",
      "epoch: 56 average loss: 1.001\n",
      "Test Accuracy : 70.5%, Test Loss: 0.942838666960597\n",
      "Epoch Time (Training + Test) = 8.71 seconds\n",
      "epoch: 57 average loss: 1.014\n",
      "Test Accuracy : 68.8%, Test Loss: 0.9721759501844645\n",
      "Epoch Time (Training + Test) = 8.77 seconds\n",
      "epoch: 58 average loss: 0.997\n",
      "Test Accuracy : 68.5%, Test Loss: 0.9865629468113184\n",
      "Epoch Time (Training + Test) = 8.72 seconds\n",
      "epoch: 59 average loss: 1.001\n",
      "Test Accuracy : 69.2%, Test Loss: 0.959255063906312\n",
      "Epoch Time (Training + Test) = 8.81 seconds\n",
      "epoch: 60 average loss: 0.986\n",
      "Test Accuracy : 69.2%, Test Loss: 0.9682410545647144\n",
      "Epoch Time (Training + Test) = 8.71 seconds\n",
      "epoch: 61 average loss: 0.999\n",
      "Test Accuracy : 68.2%, Test Loss: 0.9699711743742228\n",
      "Epoch Time (Training + Test) = 8.79 seconds\n",
      "epoch: 62 average loss: 0.998\n",
      "Test Accuracy : 69.8%, Test Loss: 0.9559550024569035\n",
      "Epoch Time (Training + Test) = 8.71 seconds\n",
      "epoch: 63 average loss: 1.003\n",
      "Test Accuracy : 71.2%, Test Loss: 0.9544021394103765\n",
      "Epoch Time (Training + Test) = 8.81 seconds\n",
      "epoch: 64 average loss: 0.989\n",
      "Test Accuracy : 70.7%, Test Loss: 0.9551444817334414\n",
      "Epoch Time (Training + Test) = 8.87 seconds\n",
      "epoch: 65 average loss: 0.993\n",
      "Test Accuracy : 69.8%, Test Loss: 0.9550488367676735\n",
      "Epoch Time (Training + Test) = 8.70 seconds\n",
      "epoch: 66 average loss: 0.987\n",
      "Test Accuracy : 70.7%, Test Loss: 0.9286558628082275\n",
      "Epoch Time (Training + Test) = 8.77 seconds\n",
      "epoch: 67 average loss: 0.985\n",
      "Test Accuracy : 69.8%, Test Loss: 0.9411110542714596\n",
      "Epoch Time (Training + Test) = 8.70 seconds\n",
      "epoch: 68 average loss: 0.980\n",
      "Test Accuracy : 69.1%, Test Loss: 0.9510323852300644\n",
      "Epoch Time (Training + Test) = 8.72 seconds\n",
      "epoch: 69 average loss: 0.984\n",
      "Test Accuracy : 69.0%, Test Loss: 0.9480505790561438\n",
      "Epoch Time (Training + Test) = 8.84 seconds\n",
      "epoch: 70 average loss: 0.974\n",
      "Test Accuracy : 69.2%, Test Loss: 0.9413804206997156\n",
      "Epoch Time (Training + Test) = 8.78 seconds\n",
      "epoch: 71 average loss: 0.984\n",
      "Test Accuracy : 70.0%, Test Loss: 0.9420299381017685\n",
      "Epoch Time (Training + Test) = 8.70 seconds\n",
      "epoch: 72 average loss: 0.975\n",
      "Test Accuracy : 71.1%, Test Loss: 0.9291454721242189\n",
      "Epoch Time (Training + Test) = 8.69 seconds\n",
      "epoch: 73 average loss: 0.979\n",
      "Test Accuracy : 69.8%, Test Loss: 0.9586923066526651\n",
      "Epoch Time (Training + Test) = 8.71 seconds\n",
      "epoch: 74 average loss: 0.989\n",
      "Test Accuracy : 69.3%, Test Loss: 0.9562168065458536\n",
      "Epoch Time (Training + Test) = 8.76 seconds\n",
      "epoch: 75 average loss: 0.977\n",
      "Test Accuracy : 70.4%, Test Loss: 0.9280444644391537\n",
      "Epoch Time (Training + Test) = 8.72 seconds\n",
      "Data Saved to Cifar80_Prune_95.0%.csv\n",
      "Finished Training: \n",
      "Total Time 0.364546 hours\n",
      " Average Time Per Epoch 17.50 seconds\n",
      "BNN_Resnet_UniAdapt(\n",
      "  (conv1): BinarizeConv2d(3, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (bn1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "  (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(80, 160, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): BinarizeConv2d(80, 160, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(160, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): BinarizeConv2d(160, 320, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)\n",
      "  (bn2): BatchNorm1d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (bn3): BatchNorm1d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (logsoftmax): LogSoftmax(dim=-1)\n",
      "  (fc): BinarizeLinear(in_features=320, out_features=80, bias=True)\n",
      "  (adapter_dict): ModuleDict()\n",
      "  (head_bn): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (head): BinarizeLinear(in_features=640, out_features=20, bias=True)\n",
      "  (encoder_net): placeholder()\n",
      "  (uniAdaptNet): Sequential(\n",
      "    (0): uniAdapt_Net(\n",
      "      (layer1): thinBlock3(\n",
      "        (conv2): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=80, bias=False)\n",
      "        (bn2): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(80, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (layer2): thinBlock3(\n",
      "        (conv2): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=160, bias=False)\n",
      "        (bn2): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(160, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (layer3): thinBlock3(\n",
      "        (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=320, bias=False)\n",
      "        (bn2): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (pool1): AdaptiveAvgPool2d(output_size=16)\n",
      "      (pool2): AdaptiveAvgPool2d(output_size=8)\n",
      "      (pool3): AdaptiveAvgPool2d(output_size=1)\n",
      "    )\n",
      "    (1): Flatten(start_dim=1, end_dim=-1)\n",
      "  )\n",
      ")\n",
      "No lr scheduler\n",
      "Optimizer Set to New: {'state': {}, 'param_groups': [{'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': False, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]}]}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:5eqsl9ng) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Current Best Acc</td><td>▁▇▇█████████████████████████████████████</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>epoch time (s)</td><td>▁█▇▇▇██▇▇▇████▇▇▇▇▇▇█▇▇▇█▇▇▇██████▇▇█▇▇▇</td></tr><tr><td>lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_accuracy</td><td>▁▇▇█████████████████▇▇▇▇▇▇▇▇▇▇▇▇▇█▇▇▇▇▇█</td></tr><tr><td>test_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>training_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Best Accuracy</td><td>0.76</td></tr><tr><td>Current Best Acc</td><td>0.76</td></tr><tr><td>Total Time (hours)</td><td>0.36455</td></tr><tr><td>epoch</td><td>75</td></tr><tr><td>epoch time (s)</td><td>8.7204</td></tr><tr><td>lr</td><td>0.001</td></tr><tr><td>test_accuracy</td><td>0.704</td></tr><tr><td>test_loss</td><td>0.92804</td></tr><tr><td>training_loss</td><td>0.97748</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">zesty-cherry-6</strong>: <a href=\"https://wandb.ai/johnny_suu/FINAL_PRUNING_CIFAR100/runs/5eqsl9ng\" target=\"_blank\">https://wandb.ai/johnny_suu/FINAL_PRUNING_CIFAR100/runs/5eqsl9ng</a><br/>Synced 5 W&B file(s), 1 media file(s), 1 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230510_111443-5eqsl9ng\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:5eqsl9ng). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a365be8667b419db9076dc262cac58b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\John Su\\Downloads\\SydneyUni\\2022\\thesis\\Thesis\\wandb\\run-20230510_112600-3g2xdfzs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/johnny_suu/FINAL_PRUNING_CIFAR100/runs/3g2xdfzs\" target=\"_blank\">visionary-darkness-7</a></strong> to <a href=\"https://wandb.ai/johnny_suu/FINAL_PRUNING_CIFAR100\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "Project Name: FINAL_PRUNING_CIFAR100, Run Name Cifar80_Prune_99.0% \n",
      "\n",
      "\n",
      "Run Start : 2023-05-10 11-26-00\n",
      "start_epoch : 0\n",
      "initial_lr : 0.001\n",
      "batch_size : 64\n",
      "epochs : 75\n",
      "epoch_chkpts : []\n",
      "device : cuda:0\n",
      "criterion : CrossEntropyLoss()\n",
      "optimizer : Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: False\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "seed : None\n",
      "model_architecture : <class 'NN_Thesis.nn_classes.BNN_Resnet_UniAdapt'>\n",
      "binerised_training : True\n",
      "Number of Elements : 4547660\n",
      "Initial accuracy:\n",
      "Test Accuracy : 3.6%, Test Loss: 47.63883772929003\n",
      "best_acc.pth saved!\n",
      "Test Accuracy : 4.0%, Test Loss: 46.83586823940277\n",
      "best_acc.pth saved!\n",
      "epoch: 1 average loss: 1.635\n",
      "Test Accuracy : 66.5%, Test Loss: 1.3140449598431587\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 8.79 seconds\n",
      "epoch: 2 average loss: 1.351\n",
      "Test Accuracy : 68.4%, Test Loss: 1.2102045435458422\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 8.77 seconds\n",
      "epoch: 3 average loss: 1.257\n",
      "Test Accuracy : 70.8%, Test Loss: 1.122979922220111\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 8.76 seconds\n",
      "epoch: 4 average loss: 1.192\n",
      "Test Accuracy : 70.8%, Test Loss: 1.0720251258462667\n",
      "Epoch Time (Training + Test) = 8.80 seconds\n",
      "epoch: 5 average loss: 1.140\n",
      "Test Accuracy : 70.5%, Test Loss: 1.0448951330035925\n",
      "Epoch Time (Training + Test) = 8.73 seconds\n",
      "epoch: 6 average loss: 1.089\n",
      "Test Accuracy : 71.5%, Test Loss: 1.002985654398799\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 8.82 seconds\n",
      "epoch: 7 average loss: 1.046\n",
      "Test Accuracy : 72.2%, Test Loss: 0.9730233550071716\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 8.72 seconds\n",
      "epoch: 8 average loss: 1.017\n",
      "Test Accuracy : 73.5%, Test Loss: 0.9289574641734362\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 8.82 seconds\n",
      "epoch: 9 average loss: 0.989\n",
      "Test Accuracy : 72.9%, Test Loss: 0.9340165723115206\n",
      "Epoch Time (Training + Test) = 8.83 seconds\n",
      "epoch: 10 average loss: 0.959\n",
      "Test Accuracy : 72.5%, Test Loss: 0.9073255229741335\n",
      "Epoch Time (Training + Test) = 8.69 seconds\n",
      "epoch: 11 average loss: 0.937\n",
      "Test Accuracy : 71.2%, Test Loss: 0.9180571418255568\n",
      "Epoch Time (Training + Test) = 8.70 seconds\n",
      "epoch: 12 average loss: 0.919\n",
      "Test Accuracy : 72.7%, Test Loss: 0.8818211182951927\n",
      "Epoch Time (Training + Test) = 8.78 seconds\n",
      "epoch: 13 average loss: 0.895\n",
      "Test Accuracy : 72.9%, Test Loss: 0.8894844558089972\n",
      "Epoch Time (Training + Test) = 8.86 seconds\n",
      "epoch: 14 average loss: 0.877\n",
      "Test Accuracy : 73.8%, Test Loss: 0.8621346969157457\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 8.72 seconds\n",
      "epoch: 15 average loss: 0.873\n",
      "Test Accuracy : 72.7%, Test Loss: 0.8856671508401632\n",
      "Epoch Time (Training + Test) = 8.81 seconds\n",
      "epoch: 16 average loss: 0.841\n",
      "Test Accuracy : 73.7%, Test Loss: 0.8757074605673552\n",
      "Epoch Time (Training + Test) = 8.75 seconds\n",
      "epoch: 17 average loss: 0.840\n",
      "Test Accuracy : 74.3%, Test Loss: 0.8423805292695761\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 8.82 seconds\n",
      "epoch: 18 average loss: 0.832\n",
      "Test Accuracy : 74.4%, Test Loss: 0.8504022117704153\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 8.84 seconds\n",
      "epoch: 19 average loss: 0.819\n",
      "Test Accuracy : 74.7%, Test Loss: 0.832656241953373\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 8.82 seconds\n",
      "epoch: 20 average loss: 0.810\n",
      "Test Accuracy : 75.1%, Test Loss: 0.815712034702301\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 8.85 seconds\n",
      "epoch: 21 average loss: 0.799\n",
      "Test Accuracy : 75.5%, Test Loss: 0.8103822786360979\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 8.89 seconds\n",
      "epoch: 22 average loss: 0.780\n",
      "Test Accuracy : 74.7%, Test Loss: 0.8101681899279356\n",
      "Epoch Time (Training + Test) = 8.82 seconds\n",
      "epoch: 23 average loss: 0.771\n",
      "Test Accuracy : 73.2%, Test Loss: 0.8296519555151463\n",
      "Epoch Time (Training + Test) = 8.70 seconds\n",
      "epoch: 24 average loss: 0.758\n",
      "Test Accuracy : 74.2%, Test Loss: 0.8399931453168392\n",
      "Epoch Time (Training + Test) = 8.80 seconds\n",
      "epoch: 25 average loss: 0.748\n",
      "Test Accuracy : 74.2%, Test Loss: 0.8371028862893581\n",
      "Epoch Time (Training + Test) = 8.70 seconds\n",
      "epoch: 26 average loss: 0.759\n",
      "Test Accuracy : 75.7%, Test Loss: 0.7791063506156206\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 8.84 seconds\n",
      "epoch: 27 average loss: 0.727\n",
      "Test Accuracy : 75.0%, Test Loss: 0.7984520550817251\n",
      "Epoch Time (Training + Test) = 8.69 seconds\n",
      "epoch: 28 average loss: 0.716\n",
      "Test Accuracy : 74.4%, Test Loss: 0.8032422978430986\n",
      "Epoch Time (Training + Test) = 8.79 seconds\n",
      "epoch: 29 average loss: 0.717\n",
      "Test Accuracy : 74.6%, Test Loss: 0.7947826329618692\n",
      "Epoch Time (Training + Test) = 8.67 seconds\n",
      "epoch: 30 average loss: 0.703\n",
      "Test Accuracy : 75.0%, Test Loss: 0.7945776674896479\n",
      "Epoch Time (Training + Test) = 8.69 seconds\n",
      "epoch: 31 average loss: 0.695\n",
      "Test Accuracy : 73.8%, Test Loss: 0.7983309347182512\n",
      "Epoch Time (Training + Test) = 8.80 seconds\n",
      "epoch: 32 average loss: 0.689\n",
      "Test Accuracy : 75.1%, Test Loss: 0.7847324162721634\n",
      "Epoch Time (Training + Test) = 8.75 seconds\n",
      "epoch: 33 average loss: 0.671\n",
      "Test Accuracy : 76.1%, Test Loss: 0.785551480948925\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 8.82 seconds\n",
      "epoch: 34 average loss: 0.675\n",
      "Test Accuracy : 75.8%, Test Loss: 0.8007306940853596\n",
      "Epoch Time (Training + Test) = 8.70 seconds\n",
      "epoch: 35 average loss: 0.670\n",
      "Test Accuracy : 74.6%, Test Loss: 0.7885923497378826\n",
      "Epoch Time (Training + Test) = 8.80 seconds\n",
      "epoch: 36 average loss: 0.660\n",
      "Test Accuracy : 74.5%, Test Loss: 0.7984278425574303\n",
      "Epoch Time (Training + Test) = 8.82 seconds\n",
      "epoch: 37 average loss: 0.655\n",
      "Test Accuracy : 73.6%, Test Loss: 0.8309242948889732\n",
      "Epoch Time (Training + Test) = 8.88 seconds\n",
      "conv2\n",
      "conv3\n",
      "conv2\n",
      "conv3\n",
      "conv2\n",
      "conv3\n",
      "Pruned [0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99]% of weights at epoch 37\n",
      "epoch: 38 average loss: 1.584\n",
      "Test Accuracy : 60.8%, Test Loss: 1.214294370263815\n",
      "Epoch Time (Training + Test) = 8.79 seconds\n",
      "epoch: 39 average loss: 1.264\n",
      "Test Accuracy : 62.7%, Test Loss: 1.1803515050560236\n",
      "Epoch Time (Training + Test) = 8.74 seconds\n",
      "epoch: 40 average loss: 1.226\n",
      "Test Accuracy : 64.2%, Test Loss: 1.1112050786614418\n",
      "Epoch Time (Training + Test) = 8.80 seconds\n",
      "epoch: 41 average loss: 1.222\n",
      "Test Accuracy : 66.0%, Test Loss: 1.0952113643288612\n",
      "Epoch Time (Training + Test) = 8.67 seconds\n",
      "epoch: 42 average loss: 1.187\n",
      "Test Accuracy : 64.3%, Test Loss: 1.1204090360552073\n",
      "Epoch Time (Training + Test) = 8.82 seconds\n",
      "epoch: 43 average loss: 1.174\n",
      "Test Accuracy : 64.3%, Test Loss: 1.1081314701586962\n",
      "Epoch Time (Training + Test) = 8.72 seconds\n",
      "epoch: 44 average loss: 1.165\n",
      "Test Accuracy : 64.6%, Test Loss: 1.0903250221163034\n",
      "Epoch Time (Training + Test) = 8.78 seconds\n",
      "epoch: 45 average loss: 1.169\n",
      "Test Accuracy : 65.2%, Test Loss: 1.0966749265789986\n",
      "Epoch Time (Training + Test) = 8.70 seconds\n",
      "epoch: 46 average loss: 1.172\n",
      "Test Accuracy : 65.8%, Test Loss: 1.0739475525915623\n",
      "Epoch Time (Training + Test) = 8.73 seconds\n",
      "epoch: 47 average loss: 1.155\n",
      "Test Accuracy : 65.3%, Test Loss: 1.0842635706067085\n",
      "Epoch Time (Training + Test) = 8.75 seconds\n",
      "epoch: 48 average loss: 1.158\n",
      "Test Accuracy : 65.5%, Test Loss: 1.0761618465185165\n",
      "Epoch Time (Training + Test) = 8.72 seconds\n",
      "epoch: 49 average loss: 1.146\n",
      "Test Accuracy : 64.5%, Test Loss: 1.0896032489836216\n",
      "Epoch Time (Training + Test) = 8.80 seconds\n",
      "epoch: 50 average loss: 1.157\n",
      "Test Accuracy : 65.4%, Test Loss: 1.0834775008261204\n",
      "Epoch Time (Training + Test) = 8.73 seconds\n",
      "epoch: 51 average loss: 1.148\n",
      "Test Accuracy : 66.6%, Test Loss: 1.0575036332011223\n",
      "Epoch Time (Training + Test) = 8.77 seconds\n",
      "epoch: 52 average loss: 1.131\n",
      "Test Accuracy : 66.0%, Test Loss: 1.0816461071372032\n",
      "Epoch Time (Training + Test) = 8.78 seconds\n",
      "epoch: 53 average loss: 1.145\n",
      "Test Accuracy : 65.2%, Test Loss: 1.061054227873683\n",
      "Epoch Time (Training + Test) = 8.78 seconds\n",
      "epoch: 54 average loss: 1.150\n",
      "Test Accuracy : 67.1%, Test Loss: 1.0484220795333385\n",
      "Epoch Time (Training + Test) = 8.67 seconds\n",
      "epoch: 55 average loss: 1.141\n",
      "Test Accuracy : 65.0%, Test Loss: 1.0662185437977314\n",
      "Epoch Time (Training + Test) = 9.14 seconds\n",
      "epoch: 56 average loss: 1.130\n",
      "Test Accuracy : 67.0%, Test Loss: 1.0519151855260134\n",
      "Epoch Time (Training + Test) = 8.76 seconds\n",
      "epoch: 57 average loss: 1.146\n",
      "Test Accuracy : 65.5%, Test Loss: 1.09475620649755\n",
      "Epoch Time (Training + Test) = 8.67 seconds\n",
      "epoch: 58 average loss: 1.129\n",
      "Test Accuracy : 63.7%, Test Loss: 1.086791591718793\n",
      "Epoch Time (Training + Test) = 8.84 seconds\n",
      "epoch: 59 average loss: 1.127\n",
      "Test Accuracy : 65.6%, Test Loss: 1.0569765605032444\n",
      "Epoch Time (Training + Test) = 8.72 seconds\n",
      "epoch: 60 average loss: 1.121\n",
      "Test Accuracy : 65.4%, Test Loss: 1.0610906146466732\n",
      "Epoch Time (Training + Test) = 8.69 seconds\n",
      "epoch: 61 average loss: 1.125\n",
      "Test Accuracy : 65.8%, Test Loss: 1.0635184291750193\n",
      "Epoch Time (Training + Test) = 8.77 seconds\n",
      "epoch: 62 average loss: 1.117\n",
      "Test Accuracy : 64.7%, Test Loss: 1.082327613607049\n",
      "Epoch Time (Training + Test) = 8.85 seconds\n",
      "epoch: 63 average loss: 1.131\n",
      "Test Accuracy : 66.3%, Test Loss: 1.0636845510452986\n",
      "Epoch Time (Training + Test) = 8.69 seconds\n",
      "epoch: 64 average loss: 1.115\n",
      "Test Accuracy : 65.4%, Test Loss: 1.0618723407387733\n",
      "Epoch Time (Training + Test) = 8.65 seconds\n",
      "epoch: 65 average loss: 1.129\n",
      "Test Accuracy : 65.5%, Test Loss: 1.0573531705886126\n",
      "Epoch Time (Training + Test) = 8.73 seconds\n",
      "epoch: 66 average loss: 1.132\n",
      "Test Accuracy : 67.9%, Test Loss: 1.0559612400829792\n",
      "Epoch Time (Training + Test) = 8.78 seconds\n",
      "epoch: 67 average loss: 1.123\n",
      "Test Accuracy : 65.9%, Test Loss: 1.0453648939728737\n",
      "Epoch Time (Training + Test) = 8.71 seconds\n",
      "epoch: 68 average loss: 1.122\n",
      "Test Accuracy : 66.0%, Test Loss: 1.0834952965378761\n",
      "Epoch Time (Training + Test) = 8.70 seconds\n",
      "epoch: 69 average loss: 1.131\n",
      "Test Accuracy : 66.5%, Test Loss: 1.0592568665742874\n",
      "Epoch Time (Training + Test) = 8.82 seconds\n",
      "epoch: 70 average loss: 1.122\n",
      "Test Accuracy : 66.2%, Test Loss: 1.0496239718049765\n",
      "Epoch Time (Training + Test) = 8.79 seconds\n",
      "epoch: 71 average loss: 1.123\n",
      "Test Accuracy : 66.0%, Test Loss: 1.0554567016661167\n",
      "Epoch Time (Training + Test) = 8.69 seconds\n",
      "epoch: 72 average loss: 1.130\n",
      "Test Accuracy : 66.2%, Test Loss: 1.0554033145308495\n",
      "Epoch Time (Training + Test) = 8.74 seconds\n",
      "epoch: 73 average loss: 1.126\n",
      "Test Accuracy : 66.1%, Test Loss: 1.0332015454769135\n",
      "Epoch Time (Training + Test) = 8.78 seconds\n",
      "epoch: 74 average loss: 1.121\n",
      "Test Accuracy : 65.1%, Test Loss: 1.0751904621720314\n",
      "Epoch Time (Training + Test) = 8.83 seconds\n",
      "epoch: 75 average loss: 1.133\n",
      "Test Accuracy : 67.2%, Test Loss: 1.0355567149817944\n",
      "Epoch Time (Training + Test) = 8.70 seconds\n",
      "Data Saved to Cifar80_Prune_99.0%.csv\n",
      "Finished Training: \n",
      "Total Time 0.365324 hours\n",
      " Average Time Per Epoch 17.54 seconds\n"
     ]
    }
   ],
   "source": [
    "for prune_amount in [0,0.8,0.9,0.95,0.99]:\n",
    "    \n",
    "    BNN = BNN_Resnet_UniAdapt(80,freeze_pre_weights=False)\n",
    "    BNN.layer3[-1].do_bntan = True\n",
    "    BNN.load_state_dict(state,strict=False)\n",
    "    BNN.freeze()\n",
    "\n",
    "    BNN = BNN.to('cuda:0')\n",
    "    BNN.encoder_net = placeholder()\n",
    "\n",
    "    n1 = uniAdapt_Net([80,160,320],[80,160,320],block = thinBlock3)\n",
    "    n1 = n1.cuda()\n",
    "    n1.train()\n",
    "\n",
    "    adapt_net = nn.Sequential(n1,nn.Flatten())\n",
    "    \n",
    "    adapt_net.to('cuda:0')\n",
    "\n",
    "    BNN.head_bn =  nn.BatchNorm1d(20)\n",
    "    BNN.head =  BinarizeLinear(320 + 320,20)\n",
    "    BNN.add_UniAdapter(adapt_net)\n",
    "    BNN = BNN.to('cuda:0')\n",
    "    BNN.uniAdapt = True\n",
    "    trainer = Trainer(BNN,model_name=f'Cifar80_Prune_{prune_amount*100:.1f}%',project_name='FINAL_PRUNING_CIFAR100',binarise=True,classes=train_20.classes)\n",
    "    trainer.lr = 1e-3\n",
    "    trainer.batch_size = 64\n",
    "    trainer.epochs =75\n",
    "    trainer.epoch_chkpts = []\n",
    "    trainer.start_epoch = 0\n",
    "    trainer.amount = [prune_amount,prune_amount]\n",
    "    # trainer.pruning_rounds = 1 # Only need this for iterative pruning\n",
    "    trainer.model_to_prune = trainer.model.uniAdaptNet[0]\n",
    "    params = nn.ModuleList([BNN.uniAdaptNet,BNN.head_bn,BNN.head]).parameters()\n",
    "    # params = nn.ModuleList([BNN.head_bn,BNN.head]).parameters()\n",
    "    print(trainer.model)\n",
    "    trainer.set_scheduler(None)\n",
    "    trainer.set_optimizer(torch.optim.Adam,params,lr = trainer.lr)\n",
    "    trainer.train(train20_DL,test20_DL,prune_strat='oneshot')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouped Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None 1\n",
      "BNN_Resnet_UniAdapt(\n",
      "  (conv1): BinarizeConv2d(3, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (bn1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "  (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(80, 160, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): BinarizeConv2d(80, 160, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(160, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): BinarizeConv2d(160, 320, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)\n",
      "  (bn2): BatchNorm1d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (bn3): BatchNorm1d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (logsoftmax): LogSoftmax(dim=-1)\n",
      "  (fc): BinarizeLinear(in_features=320, out_features=80, bias=True)\n",
      "  (adapter_dict): ModuleDict()\n",
      "  (head_bn): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (head): BinarizeLinear(in_features=640, out_features=20, bias=True)\n",
      "  (encoder_net): placeholder()\n",
      "  (uniAdaptNet): Sequential(\n",
      "    (0): uniAdapt_Net(\n",
      "      (layer1): thinBlock2(\n",
      "        (conv2): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=80, bias=False)\n",
      "        (bn2): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(80, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (layer2): thinBlock2(\n",
      "        (conv2): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=160, bias=False)\n",
      "        (bn2): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(160, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (layer3): thinBlock2(\n",
      "        (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=320, bias=False)\n",
      "        (bn2): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (pool1): AdaptiveAvgPool2d(output_size=16)\n",
      "      (pool2): AdaptiveAvgPool2d(output_size=8)\n",
      "      (pool3): AdaptiveAvgPool2d(output_size=1)\n",
      "    )\n",
      "    (1): Flatten(start_dim=1, end_dim=-1)\n",
      "  )\n",
      ")\n",
      "No lr scheduler\n",
      "Optimizer Set to New: {'state': {}, 'param_groups': [{'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': False, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]}]}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3g2xdfzs) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1701e65a2ef4af1a41e549e2d32f6cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.064 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.018986…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Current Best Acc</td><td>▁▇▇▇████████████████████████████████████</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>epoch time (s)</td><td>▁▆▆▆▆▆▆▇▆▆▆▇▆▆▇▆▆▆▆▆▆▆▆▆▆▆▆▆▆█▆▆▆▆▆▆▆▆▆▆</td></tr><tr><td>lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_accuracy</td><td>▁▇█▇████████████████▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>test_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>training_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Best Accuracy</td><td>0.7615</td></tr><tr><td>Current Best Acc</td><td>0.7615</td></tr><tr><td>Total Time (hours)</td><td>0.36532</td></tr><tr><td>epoch</td><td>75</td></tr><tr><td>epoch time (s)</td><td>8.6968</td></tr><tr><td>lr</td><td>0.001</td></tr><tr><td>test_accuracy</td><td>0.6715</td></tr><tr><td>test_loss</td><td>1.03556</td></tr><tr><td>training_loss</td><td>1.13304</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">visionary-darkness-7</strong>: <a href=\"https://wandb.ai/johnny_suu/FINAL_PRUNING_CIFAR100/runs/3g2xdfzs\" target=\"_blank\">https://wandb.ai/johnny_suu/FINAL_PRUNING_CIFAR100/runs/3g2xdfzs</a><br/>Synced 5 W&B file(s), 1 media file(s), 1 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230510_112600-3g2xdfzs\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3g2xdfzs). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af6d93dc11f34108a0f396e38716692b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\John Su\\Downloads\\SydneyUni\\2022\\thesis\\Thesis\\wandb\\run-20230510_114032-2or9hrr2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/johnny_suu/FINAL_CIFAR100_GROUPED_CONV/runs/2or9hrr2\" target=\"_blank\">restful-cherry-1</a></strong> to <a href=\"https://wandb.ai/johnny_suu/FINAL_CIFAR100_GROUPED_CONV\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "Project Name: FINAL_CIFAR100_GROUPED_CONV, Run Name Grouped_Convolution_PW_Group_1 \n",
      "\n",
      "\n",
      "Run Start : 2023-05-10 11-40-32\n",
      "start_epoch : 0\n",
      "initial_lr : 0.001\n",
      "batch_size : 64\n",
      "epochs : 75\n",
      "epoch_chkpts : []\n",
      "device : cuda:0\n",
      "criterion : CrossEntropyLoss()\n",
      "optimizer : Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: False\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "seed : None\n",
      "model_architecture : <class 'NN_Thesis.nn_classes.BNN_Resnet_UniAdapt'>\n",
      "binerised_training : True\n",
      "Number of Elements : 4547660\n",
      "Initial accuracy:\n",
      "Test Accuracy : 3.4%, Test Loss: 46.63080985682785\n",
      "best_acc.pth saved!\n",
      "Test Accuracy : 3.5%, Test Loss: 46.1002482175827\n",
      "best_acc.pth saved!\n",
      "epoch: 1 average loss: 1.647\n",
      "Test Accuracy : 66.5%, Test Loss: 1.31552017852664\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.00 seconds\n",
      "epoch: 2 average loss: 1.361\n",
      "Test Accuracy : 68.2%, Test Loss: 1.2091489844024181\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.05 seconds\n",
      "epoch: 3 average loss: 1.256\n",
      "Test Accuracy : 70.4%, Test Loss: 1.1499424185603857\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.08 seconds\n",
      "epoch: 4 average loss: 1.197\n",
      "Test Accuracy : 70.5%, Test Loss: 1.0926485192030668\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.07 seconds\n",
      "epoch: 5 average loss: 1.136\n",
      "Test Accuracy : 71.5%, Test Loss: 1.0376551914960146\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.07 seconds\n",
      "epoch: 6 average loss: 1.099\n",
      "Test Accuracy : 70.7%, Test Loss: 1.014068279415369\n",
      "Epoch Time (Training + Test) = 8.99 seconds\n",
      "epoch: 7 average loss: 1.055\n",
      "Test Accuracy : 71.3%, Test Loss: 0.9991081058979034\n",
      "Epoch Time (Training + Test) = 8.97 seconds\n",
      "epoch: 8 average loss: 1.022\n",
      "Test Accuracy : 72.0%, Test Loss: 0.9596137795597315\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.03 seconds\n",
      "epoch: 9 average loss: 1.001\n",
      "Test Accuracy : 72.5%, Test Loss: 0.936413673684001\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.06 seconds\n",
      "epoch: 10 average loss: 0.982\n",
      "Test Accuracy : 72.9%, Test Loss: 0.9125031903386116\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.06 seconds\n",
      "epoch: 11 average loss: 0.950\n",
      "Test Accuracy : 72.5%, Test Loss: 0.910287095233798\n",
      "Epoch Time (Training + Test) = 9.03 seconds\n",
      "epoch: 12 average loss: 0.928\n",
      "Test Accuracy : 72.2%, Test Loss: 0.9012973736971617\n",
      "Epoch Time (Training + Test) = 9.06 seconds\n",
      "epoch: 13 average loss: 0.916\n",
      "Test Accuracy : 73.2%, Test Loss: 0.9051601532846689\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.04 seconds\n",
      "epoch: 14 average loss: 0.895\n",
      "Test Accuracy : 72.6%, Test Loss: 0.8816303759813309\n",
      "Epoch Time (Training + Test) = 9.00 seconds\n",
      "epoch: 15 average loss: 0.884\n",
      "Test Accuracy : 72.9%, Test Loss: 0.8942459709942341\n",
      "Epoch Time (Training + Test) = 8.95 seconds\n",
      "epoch: 16 average loss: 0.876\n",
      "Test Accuracy : 73.4%, Test Loss: 0.8607565946877003\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.08 seconds\n",
      "epoch: 17 average loss: 0.854\n",
      "Test Accuracy : 73.9%, Test Loss: 0.847392899915576\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.03 seconds\n",
      "epoch: 18 average loss: 0.831\n",
      "Test Accuracy : 73.9%, Test Loss: 0.8211659509688616\n",
      "Epoch Time (Training + Test) = 9.06 seconds\n",
      "epoch: 19 average loss: 0.826\n",
      "Test Accuracy : 74.4%, Test Loss: 0.8398921620100737\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.13 seconds\n",
      "epoch: 20 average loss: 0.807\n",
      "Test Accuracy : 74.2%, Test Loss: 0.8266969416290522\n",
      "Epoch Time (Training + Test) = 9.02 seconds\n",
      "epoch: 21 average loss: 0.795\n",
      "Test Accuracy : 73.8%, Test Loss: 0.8207640927284956\n",
      "Epoch Time (Training + Test) = 9.05 seconds\n",
      "epoch: 22 average loss: 0.791\n",
      "Test Accuracy : 74.2%, Test Loss: 0.8297067079693079\n",
      "Epoch Time (Training + Test) = 9.03 seconds\n",
      "epoch: 23 average loss: 0.786\n",
      "Test Accuracy : 75.5%, Test Loss: 0.8118741791695356\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.12 seconds\n",
      "epoch: 24 average loss: 0.755\n",
      "Test Accuracy : 74.7%, Test Loss: 0.8329342752695084\n",
      "Epoch Time (Training + Test) = 9.16 seconds\n",
      "epoch: 25 average loss: 0.750\n",
      "Test Accuracy : 75.6%, Test Loss: 0.7958180550485849\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.01 seconds\n",
      "epoch: 26 average loss: 0.735\n",
      "Test Accuracy : 74.6%, Test Loss: 0.8172261733561754\n",
      "Epoch Time (Training + Test) = 8.99 seconds\n",
      "epoch: 27 average loss: 0.733\n",
      "Test Accuracy : 73.2%, Test Loss: 0.8063797242939472\n",
      "Epoch Time (Training + Test) = 9.22 seconds\n",
      "epoch: 28 average loss: 0.733\n",
      "Test Accuracy : 74.4%, Test Loss: 0.8236899953335524\n",
      "Epoch Time (Training + Test) = 9.03 seconds\n",
      "epoch: 29 average loss: 0.717\n",
      "Test Accuracy : 73.6%, Test Loss: 0.8279474377632141\n",
      "Epoch Time (Training + Test) = 9.08 seconds\n",
      "epoch: 30 average loss: 0.703\n",
      "Test Accuracy : 72.5%, Test Loss: 0.8375838585197926\n",
      "Epoch Time (Training + Test) = 9.03 seconds\n",
      "epoch: 31 average loss: 0.687\n",
      "Test Accuracy : 74.2%, Test Loss: 0.8405372034758329\n",
      "Epoch Time (Training + Test) = 9.02 seconds\n",
      "epoch: 32 average loss: 0.697\n",
      "Test Accuracy : 74.2%, Test Loss: 0.8094222471117973\n",
      "Epoch Time (Training + Test) = 8.96 seconds\n",
      "epoch: 33 average loss: 0.693\n",
      "Test Accuracy : 75.0%, Test Loss: 0.8072231337428093\n",
      "Epoch Time (Training + Test) = 8.97 seconds\n",
      "epoch: 34 average loss: 0.678\n",
      "Test Accuracy : 74.9%, Test Loss: 0.8271215669810772\n",
      "Epoch Time (Training + Test) = 9.07 seconds\n",
      "epoch: 35 average loss: 0.672\n",
      "Test Accuracy : 74.8%, Test Loss: 0.7977322824299335\n",
      "Epoch Time (Training + Test) = 9.03 seconds\n",
      "epoch: 36 average loss: 0.671\n",
      "Test Accuracy : 74.7%, Test Loss: 0.8092926144599915\n",
      "Epoch Time (Training + Test) = 8.96 seconds\n",
      "epoch: 37 average loss: 0.651\n",
      "Test Accuracy : 74.9%, Test Loss: 0.7992523480206728\n",
      "Epoch Time (Training + Test) = 9.04 seconds\n",
      "epoch: 38 average loss: 0.643\n",
      "Test Accuracy : 75.3%, Test Loss: 0.7665253188461065\n",
      "Epoch Time (Training + Test) = 9.02 seconds\n",
      "epoch: 39 average loss: 0.644\n",
      "Test Accuracy : 75.0%, Test Loss: 0.8073872160166502\n",
      "Epoch Time (Training + Test) = 9.01 seconds\n",
      "epoch: 40 average loss: 0.627\n",
      "Test Accuracy : 74.8%, Test Loss: 0.8075413070619106\n",
      "Epoch Time (Training + Test) = 8.96 seconds\n",
      "epoch: 41 average loss: 0.629\n",
      "Test Accuracy : 74.9%, Test Loss: 0.8026587199419737\n",
      "Epoch Time (Training + Test) = 8.89 seconds\n",
      "epoch: 42 average loss: 0.626\n",
      "Test Accuracy : 75.7%, Test Loss: 0.7722782827913761\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.01 seconds\n",
      "epoch: 43 average loss: 0.620\n",
      "Test Accuracy : 73.9%, Test Loss: 0.81664096750319\n",
      "Epoch Time (Training + Test) = 9.03 seconds\n",
      "epoch: 44 average loss: 0.628\n",
      "Test Accuracy : 74.0%, Test Loss: 0.8076475709676743\n",
      "Epoch Time (Training + Test) = 9.02 seconds\n",
      "epoch: 45 average loss: 0.617\n",
      "Test Accuracy : 75.2%, Test Loss: 0.7749838028103113\n",
      "Epoch Time (Training + Test) = 9.05 seconds\n",
      "epoch: 46 average loss: 0.594\n",
      "Test Accuracy : 74.7%, Test Loss: 0.7870634626597166\n",
      "Epoch Time (Training + Test) = 8.95 seconds\n",
      "epoch: 47 average loss: 0.592\n",
      "Test Accuracy : 75.8%, Test Loss: 0.7734080143272877\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.05 seconds\n",
      "epoch: 48 average loss: 0.594\n",
      "Test Accuracy : 76.7%, Test Loss: 0.7631824053823948\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.02 seconds\n",
      "epoch: 49 average loss: 0.578\n",
      "Test Accuracy : 75.7%, Test Loss: 0.775191767141223\n",
      "Epoch Time (Training + Test) = 9.05 seconds\n",
      "epoch: 50 average loss: 0.573\n",
      "Test Accuracy : 73.4%, Test Loss: 0.8016878217458725\n",
      "Epoch Time (Training + Test) = 9.06 seconds\n",
      "epoch: 51 average loss: 0.559\n",
      "Test Accuracy : 75.1%, Test Loss: 0.7996890675276518\n",
      "Epoch Time (Training + Test) = 9.05 seconds\n",
      "epoch: 52 average loss: 0.574\n",
      "Test Accuracy : 76.0%, Test Loss: 0.78464200720191\n",
      "Epoch Time (Training + Test) = 8.92 seconds\n",
      "epoch: 53 average loss: 0.563\n",
      "Test Accuracy : 75.2%, Test Loss: 0.7865566723048687\n",
      "Epoch Time (Training + Test) = 9.01 seconds\n",
      "epoch: 54 average loss: 0.558\n",
      "Test Accuracy : 75.1%, Test Loss: 0.7963597439229488\n",
      "Epoch Time (Training + Test) = 9.07 seconds\n",
      "epoch: 55 average loss: 0.554\n",
      "Test Accuracy : 74.7%, Test Loss: 0.8219607230275869\n",
      "Epoch Time (Training + Test) = 9.08 seconds\n",
      "epoch: 56 average loss: 0.547\n",
      "Test Accuracy : 77.0%, Test Loss: 0.7851031441241503\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.03 seconds\n",
      "epoch: 57 average loss: 0.534\n",
      "Test Accuracy : 74.7%, Test Loss: 0.7845121417194605\n",
      "Epoch Time (Training + Test) = 9.05 seconds\n",
      "epoch: 58 average loss: 0.530\n",
      "Test Accuracy : 76.3%, Test Loss: 0.8035100921988487\n",
      "Epoch Time (Training + Test) = 9.09 seconds\n",
      "epoch: 59 average loss: 0.540\n",
      "Test Accuracy : 75.7%, Test Loss: 0.7823508493602276\n",
      "Epoch Time (Training + Test) = 9.18 seconds\n",
      "epoch: 60 average loss: 0.513\n",
      "Test Accuracy : 74.4%, Test Loss: 0.8010920118540525\n",
      "Epoch Time (Training + Test) = 8.96 seconds\n",
      "epoch: 61 average loss: 0.536\n",
      "Test Accuracy : 75.9%, Test Loss: 0.8103475160896778\n",
      "Epoch Time (Training + Test) = 9.01 seconds\n",
      "epoch: 62 average loss: 0.521\n",
      "Test Accuracy : 76.1%, Test Loss: 0.7839441690593958\n",
      "Epoch Time (Training + Test) = 9.01 seconds\n",
      "epoch: 63 average loss: 0.530\n",
      "Test Accuracy : 75.0%, Test Loss: 0.8015710134059191\n",
      "Epoch Time (Training + Test) = 9.01 seconds\n",
      "epoch: 64 average loss: 0.519\n",
      "Test Accuracy : 76.1%, Test Loss: 0.784853020682931\n",
      "Epoch Time (Training + Test) = 8.99 seconds\n",
      "epoch: 65 average loss: 0.499\n",
      "Test Accuracy : 75.8%, Test Loss: 0.7956161983311176\n",
      "Epoch Time (Training + Test) = 9.06 seconds\n",
      "epoch: 66 average loss: 0.497\n",
      "Test Accuracy : 76.1%, Test Loss: 0.7720987256616354\n",
      "Epoch Time (Training + Test) = 9.07 seconds\n",
      "epoch: 67 average loss: 0.503\n",
      "Test Accuracy : 75.6%, Test Loss: 0.8009471809491515\n",
      "Epoch Time (Training + Test) = 8.99 seconds\n",
      "epoch: 68 average loss: 0.499\n",
      "Test Accuracy : 77.0%, Test Loss: 0.7828491376712918\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 8.98 seconds\n",
      "epoch: 69 average loss: 0.505\n",
      "Test Accuracy : 75.6%, Test Loss: 0.7974949646741152\n",
      "Epoch Time (Training + Test) = 9.03 seconds\n",
      "epoch: 70 average loss: 0.498\n",
      "Test Accuracy : 75.6%, Test Loss: 0.8101236801594496\n",
      "Epoch Time (Training + Test) = 9.00 seconds\n",
      "epoch: 71 average loss: 0.485\n",
      "Test Accuracy : 76.0%, Test Loss: 0.7861721608787775\n",
      "Epoch Time (Training + Test) = 9.04 seconds\n",
      "epoch: 72 average loss: 0.486\n",
      "Test Accuracy : 75.5%, Test Loss: 0.8169875480234623\n",
      "Epoch Time (Training + Test) = 9.03 seconds\n",
      "epoch: 73 average loss: 0.477\n",
      "Test Accuracy : 75.4%, Test Loss: 0.7994226440787315\n",
      "Epoch Time (Training + Test) = 9.07 seconds\n",
      "epoch: 74 average loss: 0.483\n",
      "Test Accuracy : 76.1%, Test Loss: 0.7838949784636497\n",
      "Epoch Time (Training + Test) = 9.06 seconds\n",
      "epoch: 75 average loss: 0.483\n",
      "Test Accuracy : 75.0%, Test Loss: 0.796964562498033\n",
      "Epoch Time (Training + Test) = 9.00 seconds\n",
      "Data Saved to Grouped_Convolution_PW_Group_1.csv\n",
      "Finished Training: \n",
      "Total Time 0.376343 hours\n",
      " Average Time Per Epoch 18.06 seconds\n",
      "None 2\n",
      "BNN_Resnet_UniAdapt(\n",
      "  (conv1): BinarizeConv2d(3, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (bn1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "  (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(80, 160, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): BinarizeConv2d(80, 160, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(160, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): BinarizeConv2d(160, 320, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)\n",
      "  (bn2): BatchNorm1d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (bn3): BatchNorm1d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (logsoftmax): LogSoftmax(dim=-1)\n",
      "  (fc): BinarizeLinear(in_features=320, out_features=80, bias=True)\n",
      "  (adapter_dict): ModuleDict()\n",
      "  (head_bn): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (head): BinarizeLinear(in_features=640, out_features=20, bias=True)\n",
      "  (encoder_net): placeholder()\n",
      "  (uniAdaptNet): Sequential(\n",
      "    (0): uniAdapt_Net(\n",
      "      (layer1): thinBlock2(\n",
      "        (conv2): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=80, bias=False)\n",
      "        (bn2): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(80, 160, kernel_size=(1, 1), stride=(1, 1), groups=2, bias=False)\n",
      "        (bn3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (layer2): thinBlock2(\n",
      "        (conv2): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=160, bias=False)\n",
      "        (bn2): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(160, 320, kernel_size=(1, 1), stride=(1, 1), groups=2, bias=False)\n",
      "        (bn3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (layer3): thinBlock2(\n",
      "        (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=320, bias=False)\n",
      "        (bn2): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1), groups=2, bias=False)\n",
      "        (bn3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (pool1): AdaptiveAvgPool2d(output_size=16)\n",
      "      (pool2): AdaptiveAvgPool2d(output_size=8)\n",
      "      (pool3): AdaptiveAvgPool2d(output_size=1)\n",
      "    )\n",
      "    (1): Flatten(start_dim=1, end_dim=-1)\n",
      "  )\n",
      ")\n",
      "No lr scheduler\n",
      "Optimizer Set to New: {'state': {}, 'param_groups': [{'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': False, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]}]}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2or9hrr2) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Current Best Acc</td><td>▁▇▇▇▇███████████████████████████████████</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>epoch time (s)</td><td>▁▇▇▇▇▇▇▇▇▇█▇█▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇▇▇▇▇▇▇▇</td></tr><tr><td>lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_accuracy</td><td>▁▇▇▇▇███████████████████████████████████</td></tr><tr><td>test_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>training_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Best Accuracy</td><td>0.77</td></tr><tr><td>Current Best Acc</td><td>0.77</td></tr><tr><td>Total Time (hours)</td><td>0.37634</td></tr><tr><td>epoch</td><td>75</td></tr><tr><td>epoch time (s)</td><td>8.99798</td></tr><tr><td>lr</td><td>0.001</td></tr><tr><td>test_accuracy</td><td>0.7505</td></tr><tr><td>test_loss</td><td>0.79696</td></tr><tr><td>training_loss</td><td>0.48255</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">restful-cherry-1</strong>: <a href=\"https://wandb.ai/johnny_suu/FINAL_CIFAR100_GROUPED_CONV/runs/2or9hrr2\" target=\"_blank\">https://wandb.ai/johnny_suu/FINAL_CIFAR100_GROUPED_CONV/runs/2or9hrr2</a><br/>Synced 5 W&B file(s), 1 media file(s), 1 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230510_114032-2or9hrr2\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2or9hrr2). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb21377efd1d4f1e9873d3db622e630b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01691666666107873, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\John Su\\Downloads\\SydneyUni\\2022\\thesis\\Thesis\\wandb\\run-20230510_115204-ol7sbgdz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/johnny_suu/FINAL_CIFAR100_GROUPED_CONV/runs/ol7sbgdz\" target=\"_blank\">iconic-silence-2</a></strong> to <a href=\"https://wandb.ai/johnny_suu/FINAL_CIFAR100_GROUPED_CONV\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "Project Name: FINAL_CIFAR100_GROUPED_CONV, Run Name Grouped_Convolution_PW_Group_2 \n",
      "\n",
      "\n",
      "Run Start : 2023-05-10 11-52-04\n",
      "start_epoch : 0\n",
      "initial_lr : 0.001\n",
      "batch_size : 64\n",
      "epochs : 75\n",
      "epoch_chkpts : []\n",
      "device : cuda:0\n",
      "criterion : CrossEntropyLoss()\n",
      "optimizer : Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: False\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "seed : None\n",
      "model_architecture : <class 'NN_Thesis.nn_classes.BNN_Resnet_UniAdapt'>\n",
      "binerised_training : True\n",
      "Number of Elements : 4464460\n",
      "Initial accuracy:\n",
      "Test Accuracy : 2.6%, Test Loss: 52.408757616759864\n",
      "best_acc.pth saved!\n",
      "Test Accuracy : 2.1%, Test Loss: 52.519824266433716\n",
      "epoch: 1 average loss: 1.687\n",
      "Test Accuracy : 66.5%, Test Loss: 1.3659312166273594\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 17.44 seconds\n",
      "epoch: 2 average loss: 1.384\n",
      "Test Accuracy : 67.8%, Test Loss: 1.2432621195912361\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 17.40 seconds\n",
      "epoch: 3 average loss: 1.292\n",
      "Test Accuracy : 68.8%, Test Loss: 1.177035391330719\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 17.48 seconds\n",
      "epoch: 4 average loss: 1.227\n",
      "Test Accuracy : 69.3%, Test Loss: 1.1197564341127872\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 17.43 seconds\n",
      "epoch: 5 average loss: 1.165\n",
      "Test Accuracy : 69.3%, Test Loss: 1.0773672759532928\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 17.50 seconds\n",
      "epoch: 6 average loss: 1.143\n",
      "Test Accuracy : 71.5%, Test Loss: 1.0382231026887894\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 17.42 seconds\n",
      "epoch: 7 average loss: 1.102\n",
      "Test Accuracy : 70.4%, Test Loss: 1.0146491155028343\n",
      "Epoch Time (Training + Test) = 17.42 seconds\n",
      "epoch: 8 average loss: 1.063\n",
      "Test Accuracy : 70.8%, Test Loss: 0.9935316704213619\n",
      "Epoch Time (Training + Test) = 17.37 seconds\n",
      "epoch: 9 average loss: 1.031\n",
      "Test Accuracy : 71.7%, Test Loss: 0.9680507890880108\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 17.44 seconds\n",
      "epoch: 10 average loss: 1.006\n",
      "Test Accuracy : 72.2%, Test Loss: 0.9571830481290817\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 17.34 seconds\n",
      "epoch: 11 average loss: 0.984\n",
      "Test Accuracy : 71.5%, Test Loss: 0.938256623223424\n",
      "Epoch Time (Training + Test) = 17.38 seconds\n",
      "epoch: 12 average loss: 0.958\n",
      "Test Accuracy : 73.6%, Test Loss: 0.9163789693266153\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 16.57 seconds\n",
      "epoch: 13 average loss: 0.947\n",
      "Test Accuracy : 73.3%, Test Loss: 0.9148116894066334\n",
      "Epoch Time (Training + Test) = 16.52 seconds\n",
      "epoch: 14 average loss: 0.945\n",
      "Test Accuracy : 73.5%, Test Loss: 0.8906834106892347\n",
      "Epoch Time (Training + Test) = 16.49 seconds\n",
      "epoch: 15 average loss: 0.931\n",
      "Test Accuracy : 72.9%, Test Loss: 0.9048868790268898\n",
      "Epoch Time (Training + Test) = 16.50 seconds\n",
      "epoch: 16 average loss: 0.906\n",
      "Test Accuracy : 72.2%, Test Loss: 0.903294725343585\n",
      "Epoch Time (Training + Test) = 16.48 seconds\n",
      "epoch: 17 average loss: 0.903\n",
      "Test Accuracy : 73.8%, Test Loss: 0.8602621369063854\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 16.50 seconds\n",
      "epoch: 18 average loss: 0.884\n",
      "Test Accuracy : 73.5%, Test Loss: 0.8869583085179329\n",
      "Epoch Time (Training + Test) = 16.46 seconds\n",
      "epoch: 19 average loss: 0.866\n",
      "Test Accuracy : 73.0%, Test Loss: 0.8607906568795443\n",
      "Epoch Time (Training + Test) = 16.50 seconds\n",
      "epoch: 20 average loss: 0.867\n",
      "Test Accuracy : 73.2%, Test Loss: 0.863685255870223\n",
      "Epoch Time (Training + Test) = 16.46 seconds\n",
      "epoch: 21 average loss: 0.857\n",
      "Test Accuracy : 74.0%, Test Loss: 0.8468422144651413\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 16.51 seconds\n",
      "epoch: 22 average loss: 0.849\n",
      "Test Accuracy : 73.0%, Test Loss: 0.8655659202486277\n",
      "Epoch Time (Training + Test) = 16.45 seconds\n",
      "epoch: 23 average loss: 0.842\n",
      "Test Accuracy : 73.5%, Test Loss: 0.8693195283412933\n",
      "Epoch Time (Training + Test) = 16.48 seconds\n",
      "epoch: 24 average loss: 0.841\n",
      "Test Accuracy : 74.6%, Test Loss: 0.8483463190495968\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 16.51 seconds\n",
      "epoch: 25 average loss: 0.835\n",
      "Test Accuracy : 73.8%, Test Loss: 0.8386957850307226\n",
      "Epoch Time (Training + Test) = 16.48 seconds\n",
      "epoch: 26 average loss: 0.818\n",
      "Test Accuracy : 72.8%, Test Loss: 0.8436902724206448\n",
      "Epoch Time (Training + Test) = 16.49 seconds\n",
      "epoch: 27 average loss: 0.808\n",
      "Test Accuracy : 72.8%, Test Loss: 0.8460572734475136\n",
      "Epoch Time (Training + Test) = 16.49 seconds\n",
      "epoch: 28 average loss: 0.812\n",
      "Test Accuracy : 73.8%, Test Loss: 0.8374604750424623\n",
      "Epoch Time (Training + Test) = 16.42 seconds\n",
      "epoch: 29 average loss: 0.802\n",
      "Test Accuracy : 73.9%, Test Loss: 0.8272959440946579\n",
      "Epoch Time (Training + Test) = 16.52 seconds\n",
      "epoch: 30 average loss: 0.800\n",
      "Test Accuracy : 73.2%, Test Loss: 0.83675442263484\n",
      "Epoch Time (Training + Test) = 16.47 seconds\n",
      "epoch: 31 average loss: 0.783\n",
      "Test Accuracy : 73.4%, Test Loss: 0.8288616519421339\n",
      "Epoch Time (Training + Test) = 16.48 seconds\n",
      "epoch: 32 average loss: 0.788\n",
      "Test Accuracy : 72.8%, Test Loss: 0.8774910103529692\n",
      "Epoch Time (Training + Test) = 16.47 seconds\n",
      "epoch: 33 average loss: 0.777\n",
      "Test Accuracy : 73.0%, Test Loss: 0.858053807169199\n",
      "Epoch Time (Training + Test) = 16.49 seconds\n",
      "epoch: 34 average loss: 0.771\n",
      "Test Accuracy : 74.1%, Test Loss: 0.8179239016026258\n",
      "Epoch Time (Training + Test) = 16.48 seconds\n",
      "epoch: 35 average loss: 0.761\n",
      "Test Accuracy : 73.4%, Test Loss: 0.8507422050461173\n",
      "Epoch Time (Training + Test) = 16.48 seconds\n",
      "epoch: 36 average loss: 0.769\n",
      "Test Accuracy : 73.9%, Test Loss: 0.8197142593562603\n",
      "Epoch Time (Training + Test) = 16.44 seconds\n",
      "epoch: 37 average loss: 0.753\n",
      "Test Accuracy : 74.4%, Test Loss: 0.809024840593338\n",
      "Epoch Time (Training + Test) = 16.60 seconds\n",
      "epoch: 38 average loss: 0.757\n",
      "Test Accuracy : 74.3%, Test Loss: 0.8120095748454332\n",
      "Epoch Time (Training + Test) = 16.52 seconds\n",
      "epoch: 39 average loss: 0.746\n",
      "Test Accuracy : 74.0%, Test Loss: 0.8067006338387728\n",
      "Epoch Time (Training + Test) = 16.54 seconds\n",
      "epoch: 40 average loss: 0.751\n",
      "Test Accuracy : 74.1%, Test Loss: 0.8124808669090271\n",
      "Epoch Time (Training + Test) = 16.49 seconds\n",
      "epoch: 41 average loss: 0.735\n",
      "Test Accuracy : 73.4%, Test Loss: 0.8090804740786552\n",
      "Epoch Time (Training + Test) = 16.52 seconds\n",
      "epoch: 42 average loss: 0.739\n",
      "Test Accuracy : 75.4%, Test Loss: 0.7795266211032867\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 16.50 seconds\n",
      "epoch: 43 average loss: 0.720\n",
      "Test Accuracy : 74.6%, Test Loss: 0.7978007085621357\n",
      "Epoch Time (Training + Test) = 16.48 seconds\n",
      "epoch: 44 average loss: 0.728\n",
      "Test Accuracy : 74.3%, Test Loss: 0.80055370926857\n",
      "Epoch Time (Training + Test) = 16.45 seconds\n",
      "epoch: 45 average loss: 0.707\n",
      "Test Accuracy : 73.9%, Test Loss: 0.8282401002943516\n",
      "Epoch Time (Training + Test) = 16.51 seconds\n",
      "epoch: 46 average loss: 0.729\n",
      "Test Accuracy : 74.6%, Test Loss: 0.8139535132795572\n",
      "Epoch Time (Training + Test) = 16.45 seconds\n",
      "epoch: 47 average loss: 0.732\n",
      "Test Accuracy : 73.3%, Test Loss: 0.8216082621365786\n",
      "Epoch Time (Training + Test) = 16.49 seconds\n",
      "epoch: 48 average loss: 0.719\n",
      "Test Accuracy : 74.6%, Test Loss: 0.8172404374927282\n",
      "Epoch Time (Training + Test) = 16.48 seconds\n",
      "epoch: 49 average loss: 0.701\n",
      "Test Accuracy : 74.4%, Test Loss: 0.7994608823210001\n",
      "Epoch Time (Training + Test) = 16.50 seconds\n",
      "epoch: 50 average loss: 0.708\n",
      "Test Accuracy : 73.8%, Test Loss: 0.8201179765164852\n",
      "Epoch Time (Training + Test) = 16.48 seconds\n",
      "epoch: 51 average loss: 0.699\n",
      "Test Accuracy : 75.3%, Test Loss: 0.79512769728899\n",
      "Epoch Time (Training + Test) = 16.49 seconds\n",
      "epoch: 52 average loss: 0.693\n",
      "Test Accuracy : 74.5%, Test Loss: 0.8220654129981995\n",
      "Epoch Time (Training + Test) = 16.47 seconds\n",
      "epoch: 53 average loss: 0.695\n",
      "Test Accuracy : 72.5%, Test Loss: 0.8253112807869911\n",
      "Epoch Time (Training + Test) = 16.50 seconds\n",
      "epoch: 54 average loss: 0.704\n",
      "Test Accuracy : 73.7%, Test Loss: 0.8258032221347094\n",
      "Epoch Time (Training + Test) = 16.45 seconds\n",
      "epoch: 55 average loss: 0.692\n",
      "Test Accuracy : 73.7%, Test Loss: 0.8111905213445425\n",
      "Epoch Time (Training + Test) = 16.50 seconds\n",
      "epoch: 56 average loss: 0.682\n",
      "Test Accuracy : 75.0%, Test Loss: 0.7961028814315796\n",
      "Epoch Time (Training + Test) = 16.50 seconds\n",
      "epoch: 57 average loss: 0.689\n",
      "Test Accuracy : 74.4%, Test Loss: 0.8075041174888611\n",
      "Epoch Time (Training + Test) = 16.46 seconds\n",
      "epoch: 58 average loss: 0.674\n",
      "Test Accuracy : 74.5%, Test Loss: 0.8110471293330193\n",
      "Epoch Time (Training + Test) = 16.48 seconds\n",
      "epoch: 59 average loss: 0.687\n",
      "Test Accuracy : 74.0%, Test Loss: 0.7854360472410917\n",
      "Epoch Time (Training + Test) = 16.48 seconds\n",
      "epoch: 60 average loss: 0.675\n",
      "Test Accuracy : 74.9%, Test Loss: 0.8003104627132416\n",
      "Epoch Time (Training + Test) = 16.51 seconds\n",
      "epoch: 61 average loss: 0.674\n",
      "Test Accuracy : 73.0%, Test Loss: 0.8135228361934423\n",
      "Epoch Time (Training + Test) = 16.48 seconds\n",
      "epoch: 62 average loss: 0.662\n",
      "Test Accuracy : 75.8%, Test Loss: 0.8129018843173981\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 16.50 seconds\n",
      "epoch: 63 average loss: 0.666\n",
      "Test Accuracy : 73.5%, Test Loss: 0.8266808688640594\n",
      "Epoch Time (Training + Test) = 16.46 seconds\n",
      "epoch: 64 average loss: 0.669\n",
      "Test Accuracy : 74.2%, Test Loss: 0.8017139416188002\n",
      "Epoch Time (Training + Test) = 16.45 seconds\n",
      "epoch: 65 average loss: 0.661\n",
      "Test Accuracy : 74.2%, Test Loss: 0.8184011224657297\n",
      "Epoch Time (Training + Test) = 16.51 seconds\n",
      "epoch: 66 average loss: 0.665\n",
      "Test Accuracy : 75.3%, Test Loss: 0.7890922594815493\n",
      "Epoch Time (Training + Test) = 16.50 seconds\n",
      "epoch: 67 average loss: 0.657\n",
      "Test Accuracy : 74.6%, Test Loss: 0.8051327355206013\n",
      "Epoch Time (Training + Test) = 16.47 seconds\n",
      "epoch: 68 average loss: 0.648\n",
      "Test Accuracy : 72.4%, Test Loss: 0.838050676509738\n",
      "Epoch Time (Training + Test) = 16.49 seconds\n",
      "epoch: 69 average loss: 0.652\n",
      "Test Accuracy : 74.8%, Test Loss: 0.8050508219748735\n",
      "Epoch Time (Training + Test) = 16.45 seconds\n",
      "epoch: 70 average loss: 0.649\n",
      "Test Accuracy : 73.4%, Test Loss: 0.8112239707261324\n",
      "Epoch Time (Training + Test) = 16.47 seconds\n",
      "epoch: 71 average loss: 0.619\n",
      "Test Accuracy : 74.7%, Test Loss: 0.7984680458903313\n",
      "Epoch Time (Training + Test) = 16.49 seconds\n",
      "epoch: 72 average loss: 0.629\n",
      "Test Accuracy : 75.2%, Test Loss: 0.8221235703676939\n",
      "Epoch Time (Training + Test) = 16.49 seconds\n",
      "epoch: 73 average loss: 0.642\n",
      "Test Accuracy : 73.7%, Test Loss: 0.811125323176384\n",
      "Epoch Time (Training + Test) = 16.45 seconds\n",
      "epoch: 74 average loss: 0.634\n",
      "Test Accuracy : 75.1%, Test Loss: 0.7866431772708893\n",
      "Epoch Time (Training + Test) = 16.51 seconds\n",
      "epoch: 75 average loss: 0.628\n",
      "Test Accuracy : 74.7%, Test Loss: 0.7966925324872136\n",
      "Epoch Time (Training + Test) = 16.47 seconds\n",
      "Data Saved to Grouped_Convolution_PW_Group_2.csv\n",
      "Finished Training: \n",
      "Total Time 0.692665 hours\n",
      " Average Time Per Epoch 33.25 seconds\n",
      "None 5\n",
      "BNN_Resnet_UniAdapt(\n",
      "  (conv1): BinarizeConv2d(3, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (bn1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "  (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(80, 160, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): BinarizeConv2d(80, 160, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(160, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): BinarizeConv2d(160, 320, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)\n",
      "  (bn2): BatchNorm1d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (bn3): BatchNorm1d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (logsoftmax): LogSoftmax(dim=-1)\n",
      "  (fc): BinarizeLinear(in_features=320, out_features=80, bias=True)\n",
      "  (adapter_dict): ModuleDict()\n",
      "  (head_bn): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (head): BinarizeLinear(in_features=640, out_features=20, bias=True)\n",
      "  (encoder_net): placeholder()\n",
      "  (uniAdaptNet): Sequential(\n",
      "    (0): uniAdapt_Net(\n",
      "      (layer1): thinBlock2(\n",
      "        (conv2): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=80, bias=False)\n",
      "        (bn2): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(80, 160, kernel_size=(1, 1), stride=(1, 1), groups=5, bias=False)\n",
      "        (bn3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (layer2): thinBlock2(\n",
      "        (conv2): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=160, bias=False)\n",
      "        (bn2): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(160, 320, kernel_size=(1, 1), stride=(1, 1), groups=5, bias=False)\n",
      "        (bn3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (layer3): thinBlock2(\n",
      "        (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=320, bias=False)\n",
      "        (bn2): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1), groups=5, bias=False)\n",
      "        (bn3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (pool1): AdaptiveAvgPool2d(output_size=16)\n",
      "      (pool2): AdaptiveAvgPool2d(output_size=8)\n",
      "      (pool3): AdaptiveAvgPool2d(output_size=1)\n",
      "    )\n",
      "    (1): Flatten(start_dim=1, end_dim=-1)\n",
      "  )\n",
      ")\n",
      "No lr scheduler\n",
      "Optimizer Set to New: {'state': {}, 'param_groups': [{'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': False, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]}]}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:ol7sbgdz) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Current Best Acc</td><td>▁▇▇▇████████████████████████████████████</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>epoch time (s)</td><td>▁██████▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_accuracy</td><td>▁▇▇▇████████████████████████████████████</td></tr><tr><td>test_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>training_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Best Accuracy</td><td>0.7575</td></tr><tr><td>Current Best Acc</td><td>0.7575</td></tr><tr><td>Total Time (hours)</td><td>0.69267</td></tr><tr><td>epoch</td><td>75</td></tr><tr><td>epoch time (s)</td><td>16.47213</td></tr><tr><td>lr</td><td>0.001</td></tr><tr><td>test_accuracy</td><td>0.7465</td></tr><tr><td>test_loss</td><td>0.79669</td></tr><tr><td>training_loss</td><td>0.62766</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">iconic-silence-2</strong>: <a href=\"https://wandb.ai/johnny_suu/FINAL_CIFAR100_GROUPED_CONV/runs/ol7sbgdz\" target=\"_blank\">https://wandb.ai/johnny_suu/FINAL_CIFAR100_GROUPED_CONV/runs/ol7sbgdz</a><br/>Synced 5 W&B file(s), 1 media file(s), 1 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230510_115204-ol7sbgdz\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:ol7sbgdz). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a15f1edb1fb640e2b8e7ca805340f157",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\John Su\\Downloads\\SydneyUni\\2022\\thesis\\Thesis\\wandb\\run-20230510_121315-17g2q2z1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/johnny_suu/FINAL_CIFAR100_GROUPED_CONV/runs/17g2q2z1\" target=\"_blank\">valiant-valley-3</a></strong> to <a href=\"https://wandb.ai/johnny_suu/FINAL_CIFAR100_GROUPED_CONV\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "Project Name: FINAL_CIFAR100_GROUPED_CONV, Run Name Grouped_Convolution_PW_Group_5 \n",
      "\n",
      "\n",
      "Run Start : 2023-05-10 12-13-15\n",
      "start_epoch : 0\n",
      "initial_lr : 0.001\n",
      "batch_size : 64\n",
      "epochs : 75\n",
      "epoch_chkpts : []\n",
      "device : cuda:0\n",
      "criterion : CrossEntropyLoss()\n",
      "optimizer : Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: False\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "seed : None\n",
      "model_architecture : <class 'NN_Thesis.nn_classes.BNN_Resnet_UniAdapt'>\n",
      "binerised_training : True\n",
      "Number of Elements : 4414540\n",
      "Initial accuracy:\n",
      "Test Accuracy : 6.6%, Test Loss: 42.112415823966835\n",
      "best_acc.pth saved!\n",
      "Test Accuracy : 5.7%, Test Loss: 41.87220501899719\n",
      "epoch: 1 average loss: 1.741\n",
      "Test Accuracy : 63.5%, Test Loss: 1.4132484942674637\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 8.59 seconds\n",
      "epoch: 2 average loss: 1.433\n",
      "Test Accuracy : 67.7%, Test Loss: 1.2814552932977676\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 8.58 seconds\n",
      "epoch: 3 average loss: 1.345\n",
      "Test Accuracy : 67.5%, Test Loss: 1.213485635817051\n",
      "Epoch Time (Training + Test) = 8.52 seconds\n",
      "epoch: 4 average loss: 1.272\n",
      "Test Accuracy : 68.8%, Test Loss: 1.1519468147307634\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 8.67 seconds\n",
      "epoch: 5 average loss: 1.225\n",
      "Test Accuracy : 68.5%, Test Loss: 1.129947192966938\n",
      "Epoch Time (Training + Test) = 8.68 seconds\n",
      "epoch: 6 average loss: 1.191\n",
      "Test Accuracy : 68.7%, Test Loss: 1.0983734969049692\n",
      "Epoch Time (Training + Test) = 8.64 seconds\n",
      "epoch: 7 average loss: 1.143\n",
      "Test Accuracy : 70.3%, Test Loss: 1.0608073361217976\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 8.57 seconds\n",
      "epoch: 8 average loss: 1.121\n",
      "Test Accuracy : 68.9%, Test Loss: 1.044121466577053\n",
      "Epoch Time (Training + Test) = 8.55 seconds\n",
      "epoch: 9 average loss: 1.097\n",
      "Test Accuracy : 70.8%, Test Loss: 1.0004742667078972\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 8.54 seconds\n",
      "epoch: 10 average loss: 1.069\n",
      "Test Accuracy : 70.2%, Test Loss: 0.9968523066490889\n",
      "Epoch Time (Training + Test) = 8.58 seconds\n",
      "epoch: 11 average loss: 1.056\n",
      "Test Accuracy : 69.7%, Test Loss: 0.9969270564615726\n",
      "Epoch Time (Training + Test) = 8.64 seconds\n",
      "epoch: 12 average loss: 1.039\n",
      "Test Accuracy : 71.0%, Test Loss: 0.9711914677172899\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 8.54 seconds\n",
      "epoch: 13 average loss: 1.032\n",
      "Test Accuracy : 69.5%, Test Loss: 0.9931002296507359\n",
      "Epoch Time (Training + Test) = 8.58 seconds\n",
      "epoch: 14 average loss: 1.010\n",
      "Test Accuracy : 70.6%, Test Loss: 0.9580717645585537\n",
      "Epoch Time (Training + Test) = 8.51 seconds\n",
      "epoch: 15 average loss: 1.000\n",
      "Test Accuracy : 71.1%, Test Loss: 0.9585154540836811\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 8.61 seconds\n",
      "epoch: 16 average loss: 0.989\n",
      "Test Accuracy : 71.5%, Test Loss: 0.9171217437833548\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 8.64 seconds\n",
      "epoch: 17 average loss: 0.966\n",
      "Test Accuracy : 70.2%, Test Loss: 0.9583495706319809\n",
      "Epoch Time (Training + Test) = 8.45 seconds\n",
      "epoch: 18 average loss: 0.960\n",
      "Test Accuracy : 71.5%, Test Loss: 0.9278919752687216\n",
      "Epoch Time (Training + Test) = 8.50 seconds\n",
      "epoch: 19 average loss: 0.959\n",
      "Test Accuracy : 72.9%, Test Loss: 0.8858303353190422\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 8.52 seconds\n",
      "epoch: 20 average loss: 0.949\n",
      "Test Accuracy : 71.9%, Test Loss: 0.9023663736879826\n",
      "Epoch Time (Training + Test) = 8.54 seconds\n",
      "epoch: 21 average loss: 0.942\n",
      "Test Accuracy : 72.2%, Test Loss: 0.9153863452374935\n",
      "Epoch Time (Training + Test) = 8.46 seconds\n",
      "epoch: 22 average loss: 0.939\n",
      "Test Accuracy : 72.4%, Test Loss: 0.9007308036088943\n",
      "Epoch Time (Training + Test) = 8.62 seconds\n",
      "epoch: 23 average loss: 0.938\n",
      "Test Accuracy : 69.8%, Test Loss: 0.9263645280152559\n",
      "Epoch Time (Training + Test) = 8.52 seconds\n",
      "epoch: 24 average loss: 0.922\n",
      "Test Accuracy : 71.7%, Test Loss: 0.9101730734109879\n",
      "Epoch Time (Training + Test) = 8.57 seconds\n",
      "epoch: 25 average loss: 0.912\n",
      "Test Accuracy : 72.9%, Test Loss: 0.8696314021945\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 8.53 seconds\n",
      "epoch: 26 average loss: 0.915\n",
      "Test Accuracy : 72.4%, Test Loss: 0.8746487442404032\n",
      "Epoch Time (Training + Test) = 8.59 seconds\n",
      "epoch: 27 average loss: 0.914\n",
      "Test Accuracy : 72.0%, Test Loss: 0.8858666066080332\n",
      "Epoch Time (Training + Test) = 8.47 seconds\n",
      "epoch: 28 average loss: 0.898\n",
      "Test Accuracy : 72.2%, Test Loss: 0.8715585228055716\n",
      "Epoch Time (Training + Test) = 8.55 seconds\n",
      "epoch: 29 average loss: 0.893\n",
      "Test Accuracy : 71.8%, Test Loss: 0.8758950140327215\n",
      "Epoch Time (Training + Test) = 8.58 seconds\n",
      "epoch: 30 average loss: 0.902\n",
      "Test Accuracy : 72.6%, Test Loss: 0.8802312705665827\n",
      "Epoch Time (Training + Test) = 8.49 seconds\n",
      "epoch: 31 average loss: 0.891\n",
      "Test Accuracy : 72.1%, Test Loss: 0.8787166811525822\n",
      "Epoch Time (Training + Test) = 8.60 seconds\n",
      "epoch: 32 average loss: 0.883\n",
      "Test Accuracy : 73.0%, Test Loss: 0.85818687453866\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 8.66 seconds\n",
      "epoch: 33 average loss: 0.882\n",
      "Test Accuracy : 71.6%, Test Loss: 0.892610339447856\n",
      "Epoch Time (Training + Test) = 8.44 seconds\n",
      "epoch: 34 average loss: 0.879\n",
      "Test Accuracy : 73.0%, Test Loss: 0.8738669995218515\n",
      "Epoch Time (Training + Test) = 8.54 seconds\n",
      "epoch: 35 average loss: 0.865\n",
      "Test Accuracy : 73.1%, Test Loss: 0.8655267488211393\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 8.59 seconds\n",
      "epoch: 36 average loss: 0.870\n",
      "Test Accuracy : 71.5%, Test Loss: 0.8782285265624523\n",
      "Epoch Time (Training + Test) = 8.51 seconds\n",
      "epoch: 37 average loss: 0.869\n",
      "Test Accuracy : 73.0%, Test Loss: 0.8435975704342127\n",
      "Epoch Time (Training + Test) = 8.60 seconds\n",
      "epoch: 38 average loss: 0.842\n",
      "Test Accuracy : 71.7%, Test Loss: 0.8678879346698523\n",
      "Epoch Time (Training + Test) = 8.51 seconds\n",
      "epoch: 39 average loss: 0.856\n",
      "Test Accuracy : 72.0%, Test Loss: 0.8699242454022169\n",
      "Epoch Time (Training + Test) = 8.60 seconds\n",
      "epoch: 40 average loss: 0.857\n",
      "Test Accuracy : 72.8%, Test Loss: 0.8613812793046236\n",
      "Epoch Time (Training + Test) = 8.52 seconds\n",
      "epoch: 41 average loss: 0.859\n",
      "Test Accuracy : 72.4%, Test Loss: 0.8517549224197865\n",
      "Epoch Time (Training + Test) = 8.46 seconds\n",
      "epoch: 42 average loss: 0.841\n",
      "Test Accuracy : 74.1%, Test Loss: 0.8433168455958366\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 8.61 seconds\n",
      "epoch: 43 average loss: 0.850\n",
      "Test Accuracy : 74.1%, Test Loss: 0.8149714134633541\n",
      "Epoch Time (Training + Test) = 8.61 seconds\n",
      "epoch: 44 average loss: 0.849\n",
      "Test Accuracy : 73.0%, Test Loss: 0.8521815091371536\n",
      "Epoch Time (Training + Test) = 8.48 seconds\n",
      "epoch: 45 average loss: 0.843\n",
      "Test Accuracy : 72.9%, Test Loss: 0.855072271078825\n",
      "Epoch Time (Training + Test) = 8.52 seconds\n",
      "epoch: 46 average loss: 0.839\n",
      "Test Accuracy : 73.1%, Test Loss: 0.8423674609512091\n",
      "Epoch Time (Training + Test) = 8.60 seconds\n",
      "epoch: 47 average loss: 0.837\n",
      "Test Accuracy : 73.6%, Test Loss: 0.8421203251928091\n",
      "Epoch Time (Training + Test) = 8.48 seconds\n",
      "epoch: 48 average loss: 0.830\n",
      "Test Accuracy : 72.0%, Test Loss: 0.8433985393494368\n",
      "Epoch Time (Training + Test) = 8.52 seconds\n",
      "epoch: 49 average loss: 0.819\n",
      "Test Accuracy : 74.1%, Test Loss: 0.8549842536449432\n",
      "Epoch Time (Training + Test) = 8.57 seconds\n",
      "epoch: 50 average loss: 0.828\n",
      "Test Accuracy : 72.6%, Test Loss: 0.8552839923650026\n",
      "Epoch Time (Training + Test) = 8.53 seconds\n",
      "epoch: 51 average loss: 0.818\n",
      "Test Accuracy : 72.4%, Test Loss: 0.8483307044953108\n",
      "Epoch Time (Training + Test) = 8.59 seconds\n",
      "epoch: 52 average loss: 0.826\n",
      "Test Accuracy : 73.5%, Test Loss: 0.8326533101499081\n",
      "Epoch Time (Training + Test) = 8.51 seconds\n",
      "epoch: 53 average loss: 0.818\n",
      "Test Accuracy : 72.0%, Test Loss: 0.8542040642350912\n",
      "Epoch Time (Training + Test) = 8.55 seconds\n",
      "epoch: 54 average loss: 0.821\n",
      "Test Accuracy : 73.7%, Test Loss: 0.8324489556252956\n",
      "Epoch Time (Training + Test) = 8.46 seconds\n",
      "epoch: 55 average loss: 0.812\n",
      "Test Accuracy : 72.2%, Test Loss: 0.8597341571003199\n",
      "Epoch Time (Training + Test) = 8.51 seconds\n",
      "epoch: 56 average loss: 0.818\n",
      "Test Accuracy : 74.2%, Test Loss: 0.8171489834785461\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 8.61 seconds\n",
      "epoch: 57 average loss: 0.813\n",
      "Test Accuracy : 72.7%, Test Loss: 0.8558369241654873\n",
      "Epoch Time (Training + Test) = 8.54 seconds\n",
      "epoch: 58 average loss: 0.811\n",
      "Test Accuracy : 73.5%, Test Loss: 0.8244623355567455\n",
      "Epoch Time (Training + Test) = 8.59 seconds\n",
      "epoch: 59 average loss: 0.792\n",
      "Test Accuracy : 74.5%, Test Loss: 0.8189794681966305\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 8.66 seconds\n",
      "epoch: 60 average loss: 0.809\n",
      "Test Accuracy : 73.8%, Test Loss: 0.8365280050784349\n",
      "Epoch Time (Training + Test) = 8.52 seconds\n",
      "epoch: 61 average loss: 0.801\n",
      "Test Accuracy : 73.3%, Test Loss: 0.8261278942227364\n",
      "Epoch Time (Training + Test) = 8.57 seconds\n",
      "epoch: 62 average loss: 0.803\n",
      "Test Accuracy : 73.0%, Test Loss: 0.8333517070859671\n",
      "Epoch Time (Training + Test) = 8.47 seconds\n",
      "epoch: 63 average loss: 0.800\n",
      "Test Accuracy : 73.9%, Test Loss: 0.8072928171604872\n",
      "Epoch Time (Training + Test) = 8.53 seconds\n",
      "epoch: 64 average loss: 0.783\n",
      "Test Accuracy : 73.8%, Test Loss: 0.8165338188409805\n",
      "Epoch Time (Training + Test) = 8.53 seconds\n",
      "epoch: 65 average loss: 0.800\n",
      "Test Accuracy : 74.2%, Test Loss: 0.8316565323621035\n",
      "Epoch Time (Training + Test) = 8.47 seconds\n",
      "epoch: 66 average loss: 0.799\n",
      "Test Accuracy : 71.9%, Test Loss: 0.8316367510706186\n",
      "Epoch Time (Training + Test) = 8.65 seconds\n",
      "epoch: 67 average loss: 0.792\n",
      "Test Accuracy : 72.8%, Test Loss: 0.8430502489209175\n",
      "Epoch Time (Training + Test) = 8.49 seconds\n",
      "epoch: 68 average loss: 0.795\n",
      "Test Accuracy : 73.0%, Test Loss: 0.8198574278503656\n",
      "Epoch Time (Training + Test) = 8.49 seconds\n",
      "epoch: 69 average loss: 0.795\n",
      "Test Accuracy : 72.8%, Test Loss: 0.846126351505518\n",
      "Epoch Time (Training + Test) = 8.51 seconds\n",
      "epoch: 70 average loss: 0.784\n",
      "Test Accuracy : 73.0%, Test Loss: 0.8196511417627335\n",
      "Epoch Time (Training + Test) = 8.53 seconds\n",
      "epoch: 71 average loss: 0.816\n",
      "Test Accuracy : 73.4%, Test Loss: 0.8505976144224405\n",
      "Epoch Time (Training + Test) = 8.51 seconds\n",
      "epoch: 72 average loss: 0.799\n",
      "Test Accuracy : 72.5%, Test Loss: 0.863597834482789\n",
      "Epoch Time (Training + Test) = 8.61 seconds\n",
      "epoch: 73 average loss: 0.780\n",
      "Test Accuracy : 72.7%, Test Loss: 0.8522825427353382\n",
      "Epoch Time (Training + Test) = 8.55 seconds\n",
      "epoch: 74 average loss: 0.806\n",
      "Test Accuracy : 74.4%, Test Loss: 0.8191414345055819\n",
      "Epoch Time (Training + Test) = 8.48 seconds\n",
      "epoch: 75 average loss: 0.778\n",
      "Test Accuracy : 72.5%, Test Loss: 0.8405136298388243\n",
      "Epoch Time (Training + Test) = 8.56 seconds\n",
      "Data Saved to Grouped_Convolution_PW_Group_5.csv\n",
      "Finished Training: \n",
      "Total Time 0.356239 hours\n",
      " Average Time Per Epoch 17.10 seconds\n",
      "None 10\n",
      "BNN_Resnet_UniAdapt(\n",
      "  (conv1): BinarizeConv2d(3, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (bn1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "  (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(80, 160, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): BinarizeConv2d(80, 160, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(160, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): BinarizeConv2d(160, 320, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)\n",
      "  (bn2): BatchNorm1d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (bn3): BatchNorm1d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (logsoftmax): LogSoftmax(dim=-1)\n",
      "  (fc): BinarizeLinear(in_features=320, out_features=80, bias=True)\n",
      "  (adapter_dict): ModuleDict()\n",
      "  (head_bn): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (head): BinarizeLinear(in_features=640, out_features=20, bias=True)\n",
      "  (encoder_net): placeholder()\n",
      "  (uniAdaptNet): Sequential(\n",
      "    (0): uniAdapt_Net(\n",
      "      (layer1): thinBlock2(\n",
      "        (conv2): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=80, bias=False)\n",
      "        (bn2): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(80, 160, kernel_size=(1, 1), stride=(1, 1), groups=10, bias=False)\n",
      "        (bn3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (layer2): thinBlock2(\n",
      "        (conv2): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=160, bias=False)\n",
      "        (bn2): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(160, 320, kernel_size=(1, 1), stride=(1, 1), groups=10, bias=False)\n",
      "        (bn3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (layer3): thinBlock2(\n",
      "        (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=320, bias=False)\n",
      "        (bn2): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1), groups=10, bias=False)\n",
      "        (bn3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (pool1): AdaptiveAvgPool2d(output_size=16)\n",
      "      (pool2): AdaptiveAvgPool2d(output_size=8)\n",
      "      (pool3): AdaptiveAvgPool2d(output_size=1)\n",
      "    )\n",
      "    (1): Flatten(start_dim=1, end_dim=-1)\n",
      "  )\n",
      ")\n",
      "No lr scheduler\n",
      "Optimizer Set to New: {'state': {}, 'param_groups': [{'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': False, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]}]}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:17g2q2z1) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Current Best Acc</td><td>▁▇▇▇████████████████████████████████████</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>epoch time (s)</td><td>▁▇▇█▇▇█▇█▇▇▇▇▇▇▇▇█▇▇▇▇█▇█▇▇▇▇▇▇█▇▇▇▇▇▇▇▇</td></tr><tr><td>lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_accuracy</td><td>▁▇▇▇███▇████████████████████████████████</td></tr><tr><td>test_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>training_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Best Accuracy</td><td>0.745</td></tr><tr><td>Current Best Acc</td><td>0.745</td></tr><tr><td>Total Time (hours)</td><td>0.35624</td></tr><tr><td>epoch</td><td>75</td></tr><tr><td>epoch time (s)</td><td>8.56024</td></tr><tr><td>lr</td><td>0.001</td></tr><tr><td>test_accuracy</td><td>0.7255</td></tr><tr><td>test_loss</td><td>0.84051</td></tr><tr><td>training_loss</td><td>0.77811</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">valiant-valley-3</strong>: <a href=\"https://wandb.ai/johnny_suu/FINAL_CIFAR100_GROUPED_CONV/runs/17g2q2z1\" target=\"_blank\">https://wandb.ai/johnny_suu/FINAL_CIFAR100_GROUPED_CONV/runs/17g2q2z1</a><br/>Synced 5 W&B file(s), 1 media file(s), 1 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230510_121315-17g2q2z1\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:17g2q2z1). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4efc78889f734b729fffcebcad04d61f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\John Su\\Downloads\\SydneyUni\\2022\\thesis\\Thesis\\wandb\\run-20230510_122415-1upoqo7a</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/johnny_suu/FINAL_CIFAR100_GROUPED_CONV/runs/1upoqo7a\" target=\"_blank\">hopeful-lion-4</a></strong> to <a href=\"https://wandb.ai/johnny_suu/FINAL_CIFAR100_GROUPED_CONV\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "Project Name: FINAL_CIFAR100_GROUPED_CONV, Run Name Grouped_Convolution_PW_Group_10 \n",
      "\n",
      "\n",
      "Run Start : 2023-05-10 12-24-15\n",
      "start_epoch : 0\n",
      "initial_lr : 0.001\n",
      "batch_size : 64\n",
      "epochs : 75\n",
      "epoch_chkpts : []\n",
      "device : cuda:0\n",
      "criterion : CrossEntropyLoss()\n",
      "optimizer : Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: False\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "seed : None\n",
      "model_architecture : <class 'NN_Thesis.nn_classes.BNN_Resnet_UniAdapt'>\n",
      "binerised_training : True\n",
      "Number of Elements : 4397900\n",
      "Initial accuracy:\n",
      "Test Accuracy : 4.4%, Test Loss: 52.47440204498874\n",
      "best_acc.pth saved!\n",
      "Test Accuracy : 4.3%, Test Loss: 50.28965163230896\n",
      "epoch: 1 average loss: 1.792\n",
      "Test Accuracy : 64.8%, Test Loss: 1.4008913673460484\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 8.77 seconds\n",
      "epoch: 2 average loss: 1.456\n",
      "Test Accuracy : 67.3%, Test Loss: 1.3142907433211803\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 8.67 seconds\n",
      "epoch: 3 average loss: 1.363\n",
      "Test Accuracy : 67.8%, Test Loss: 1.230976153165102\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 8.53 seconds\n",
      "epoch: 4 average loss: 1.307\n",
      "Test Accuracy : 67.7%, Test Loss: 1.178915424272418\n",
      "Epoch Time (Training + Test) = 8.78 seconds\n",
      "epoch: 5 average loss: 1.250\n",
      "Test Accuracy : 68.8%, Test Loss: 1.1455545406788588\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 8.65 seconds\n",
      "epoch: 6 average loss: 1.211\n",
      "Test Accuracy : 69.2%, Test Loss: 1.1153118424117565\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 8.70 seconds\n",
      "epoch: 7 average loss: 1.188\n",
      "Test Accuracy : 68.7%, Test Loss: 1.1036841087043285\n",
      "Epoch Time (Training + Test) = 8.58 seconds\n",
      "epoch: 8 average loss: 1.154\n",
      "Test Accuracy : 69.0%, Test Loss: 1.0706629287451506\n",
      "Epoch Time (Training + Test) = 8.66 seconds\n",
      "epoch: 9 average loss: 1.143\n",
      "Test Accuracy : 69.0%, Test Loss: 1.0694194082170725\n",
      "Epoch Time (Training + Test) = 8.53 seconds\n",
      "epoch: 10 average loss: 1.120\n",
      "Test Accuracy : 68.8%, Test Loss: 1.0527821648865938\n",
      "Epoch Time (Training + Test) = 8.60 seconds\n",
      "epoch: 11 average loss: 1.092\n",
      "Test Accuracy : 69.2%, Test Loss: 1.0340310633182526\n",
      "Epoch Time (Training + Test) = 8.62 seconds\n",
      "epoch: 12 average loss: 1.078\n",
      "Test Accuracy : 70.0%, Test Loss: 1.0112493876367807\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 8.57 seconds\n",
      "epoch: 13 average loss: 1.072\n",
      "Test Accuracy : 69.3%, Test Loss: 1.0118437968194485\n",
      "Epoch Time (Training + Test) = 8.58 seconds\n",
      "epoch: 14 average loss: 1.058\n",
      "Test Accuracy : 69.8%, Test Loss: 1.0053107179701328\n",
      "Epoch Time (Training + Test) = 8.63 seconds\n",
      "epoch: 15 average loss: 1.036\n",
      "Test Accuracy : 70.4%, Test Loss: 0.9825936686247587\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 8.56 seconds\n",
      "epoch: 16 average loss: 1.045\n",
      "Test Accuracy : 71.0%, Test Loss: 0.9727482311427593\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 8.67 seconds\n",
      "epoch: 17 average loss: 1.037\n",
      "Test Accuracy : 71.5%, Test Loss: 0.939965981990099\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 8.58 seconds\n",
      "epoch: 18 average loss: 1.009\n",
      "Test Accuracy : 71.9%, Test Loss: 0.9566339012235403\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 8.65 seconds\n",
      "epoch: 19 average loss: 1.029\n",
      "Test Accuracy : 71.0%, Test Loss: 0.9509349633008242\n",
      "Epoch Time (Training + Test) = 8.45 seconds\n",
      "epoch: 20 average loss: 1.015\n",
      "Test Accuracy : 70.5%, Test Loss: 0.9400331359356642\n",
      "Epoch Time (Training + Test) = 8.52 seconds\n",
      "epoch: 21 average loss: 0.984\n",
      "Test Accuracy : 70.0%, Test Loss: 0.957093758508563\n",
      "Epoch Time (Training + Test) = 8.60 seconds\n",
      "epoch: 22 average loss: 0.987\n",
      "Test Accuracy : 69.8%, Test Loss: 0.9648584183305502\n",
      "Epoch Time (Training + Test) = 8.62 seconds\n",
      "epoch: 23 average loss: 0.980\n",
      "Test Accuracy : 71.7%, Test Loss: 0.9470712170004845\n",
      "Epoch Time (Training + Test) = 8.65 seconds\n",
      "epoch: 24 average loss: 0.986\n",
      "Test Accuracy : 71.0%, Test Loss: 0.9357700143009424\n",
      "Epoch Time (Training + Test) = 8.48 seconds\n",
      "epoch: 25 average loss: 0.980\n",
      "Test Accuracy : 72.4%, Test Loss: 0.9148577600717545\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 8.54 seconds\n",
      "epoch: 26 average loss: 0.957\n",
      "Test Accuracy : 71.9%, Test Loss: 0.9093832150101662\n",
      "Epoch Time (Training + Test) = 8.60 seconds\n",
      "epoch: 27 average loss: 0.967\n",
      "Test Accuracy : 71.4%, Test Loss: 0.9265017285943031\n",
      "Epoch Time (Training + Test) = 8.52 seconds\n",
      "epoch: 28 average loss: 0.946\n",
      "Test Accuracy : 70.8%, Test Loss: 0.9383205305784941\n",
      "Epoch Time (Training + Test) = 8.55 seconds\n",
      "epoch: 29 average loss: 0.960\n",
      "Test Accuracy : 71.8%, Test Loss: 0.9141440223902464\n",
      "Epoch Time (Training + Test) = 8.65 seconds\n",
      "epoch: 30 average loss: 0.960\n",
      "Test Accuracy : 70.0%, Test Loss: 0.929479330778122\n",
      "Epoch Time (Training + Test) = 8.48 seconds\n",
      "epoch: 31 average loss: 0.941\n",
      "Test Accuracy : 70.4%, Test Loss: 0.9449658412486315\n",
      "Epoch Time (Training + Test) = 8.61 seconds\n",
      "epoch: 32 average loss: 0.953\n",
      "Test Accuracy : 71.4%, Test Loss: 0.9266431070864201\n",
      "Epoch Time (Training + Test) = 8.56 seconds\n",
      "epoch: 33 average loss: 0.947\n",
      "Test Accuracy : 69.4%, Test Loss: 0.92069255374372\n",
      "Epoch Time (Training + Test) = 8.55 seconds\n",
      "epoch: 34 average loss: 0.941\n",
      "Test Accuracy : 70.3%, Test Loss: 0.9055331610143185\n",
      "Epoch Time (Training + Test) = 8.50 seconds\n",
      "epoch: 35 average loss: 0.932\n",
      "Test Accuracy : 70.8%, Test Loss: 0.9111331552267075\n",
      "Epoch Time (Training + Test) = 8.60 seconds\n",
      "epoch: 36 average loss: 0.922\n",
      "Test Accuracy : 71.2%, Test Loss: 0.8995474800467491\n",
      "Epoch Time (Training + Test) = 8.69 seconds\n",
      "epoch: 37 average loss: 0.939\n",
      "Test Accuracy : 70.4%, Test Loss: 0.9224694855511189\n",
      "Epoch Time (Training + Test) = 8.47 seconds\n",
      "epoch: 38 average loss: 0.944\n",
      "Test Accuracy : 70.8%, Test Loss: 0.9257179703563452\n",
      "Epoch Time (Training + Test) = 8.53 seconds\n",
      "epoch: 39 average loss: 0.918\n",
      "Test Accuracy : 71.4%, Test Loss: 0.9015523176640272\n",
      "Epoch Time (Training + Test) = 8.63 seconds\n",
      "epoch: 40 average loss: 0.913\n",
      "Test Accuracy : 71.9%, Test Loss: 0.9056772366166115\n",
      "Epoch Time (Training + Test) = 8.64 seconds\n",
      "epoch: 41 average loss: 0.925\n",
      "Test Accuracy : 71.2%, Test Loss: 0.9159202706068754\n",
      "Epoch Time (Training + Test) = 8.60 seconds\n",
      "epoch: 42 average loss: 0.923\n",
      "Test Accuracy : 70.1%, Test Loss: 0.9254567325115204\n",
      "Epoch Time (Training + Test) = 8.51 seconds\n",
      "epoch: 43 average loss: 0.904\n",
      "Test Accuracy : 71.7%, Test Loss: 0.8879095893353224\n",
      "Epoch Time (Training + Test) = 8.63 seconds\n",
      "epoch: 44 average loss: 0.909\n",
      "Test Accuracy : 71.8%, Test Loss: 0.9010312110185623\n",
      "Epoch Time (Training + Test) = 8.63 seconds\n",
      "epoch: 45 average loss: 0.916\n",
      "Test Accuracy : 71.9%, Test Loss: 0.9024043194949627\n",
      "Epoch Time (Training + Test) = 8.51 seconds\n",
      "epoch: 46 average loss: 0.905\n",
      "Test Accuracy : 71.2%, Test Loss: 0.9230829365551472\n",
      "Epoch Time (Training + Test) = 8.49 seconds\n",
      "epoch: 47 average loss: 0.901\n",
      "Test Accuracy : 71.4%, Test Loss: 0.9019094407558441\n",
      "Epoch Time (Training + Test) = 8.61 seconds\n",
      "epoch: 48 average loss: 0.906\n",
      "Test Accuracy : 71.5%, Test Loss: 0.8929530456662178\n",
      "Epoch Time (Training + Test) = 8.57 seconds\n",
      "epoch: 49 average loss: 0.904\n",
      "Test Accuracy : 73.2%, Test Loss: 0.8739539980888367\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 8.55 seconds\n",
      "epoch: 50 average loss: 0.895\n",
      "Test Accuracy : 70.8%, Test Loss: 0.8912531975656748\n",
      "Epoch Time (Training + Test) = 8.61 seconds\n",
      "epoch: 51 average loss: 0.892\n",
      "Test Accuracy : 70.8%, Test Loss: 0.9133353400975466\n",
      "Epoch Time (Training + Test) = 8.61 seconds\n",
      "epoch: 52 average loss: 0.899\n",
      "Test Accuracy : 71.3%, Test Loss: 0.8880882896482944\n",
      "Epoch Time (Training + Test) = 8.52 seconds\n",
      "epoch: 53 average loss: 0.900\n",
      "Test Accuracy : 69.5%, Test Loss: 0.9229451846331358\n",
      "Epoch Time (Training + Test) = 8.61 seconds\n",
      "epoch: 54 average loss: 0.890\n",
      "Test Accuracy : 71.1%, Test Loss: 0.8855270072817802\n",
      "Epoch Time (Training + Test) = 8.53 seconds\n",
      "epoch: 55 average loss: 0.900\n",
      "Test Accuracy : 71.4%, Test Loss: 0.881777286529541\n",
      "Epoch Time (Training + Test) = 8.61 seconds\n",
      "epoch: 56 average loss: 0.889\n",
      "Test Accuracy : 73.2%, Test Loss: 0.8593257945030928\n",
      "Epoch Time (Training + Test) = 8.62 seconds\n",
      "epoch: 57 average loss: 0.893\n",
      "Test Accuracy : 71.4%, Test Loss: 0.8671299051493406\n",
      "Epoch Time (Training + Test) = 8.49 seconds\n",
      "epoch: 58 average loss: 0.887\n",
      "Test Accuracy : 71.4%, Test Loss: 0.8981164041906595\n",
      "Epoch Time (Training + Test) = 8.54 seconds\n",
      "epoch: 59 average loss: 0.883\n",
      "Test Accuracy : 72.0%, Test Loss: 0.8850819393992424\n",
      "Epoch Time (Training + Test) = 8.56 seconds\n",
      "epoch: 60 average loss: 0.906\n",
      "Test Accuracy : 71.8%, Test Loss: 0.8854004461318254\n",
      "Epoch Time (Training + Test) = 8.51 seconds\n",
      "epoch: 61 average loss: 0.889\n",
      "Test Accuracy : 69.7%, Test Loss: 0.9145796839147806\n",
      "Epoch Time (Training + Test) = 8.74 seconds\n",
      "epoch: 62 average loss: 0.888\n",
      "Test Accuracy : 71.7%, Test Loss: 0.8837944101542234\n",
      "Epoch Time (Training + Test) = 8.48 seconds\n",
      "epoch: 63 average loss: 0.877\n",
      "Test Accuracy : 71.7%, Test Loss: 0.9093773104250431\n",
      "Epoch Time (Training + Test) = 8.52 seconds\n",
      "epoch: 64 average loss: 0.866\n",
      "Test Accuracy : 71.9%, Test Loss: 0.8755181189626455\n",
      "Epoch Time (Training + Test) = 8.64 seconds\n",
      "epoch: 65 average loss: 0.882\n",
      "Test Accuracy : 72.0%, Test Loss: 0.8827474806457758\n",
      "Epoch Time (Training + Test) = 8.51 seconds\n",
      "epoch: 66 average loss: 0.880\n",
      "Test Accuracy : 71.0%, Test Loss: 0.9153785910457373\n",
      "Epoch Time (Training + Test) = 8.55 seconds\n",
      "epoch: 67 average loss: 0.877\n",
      "Test Accuracy : 70.8%, Test Loss: 0.8987366482615471\n",
      "Epoch Time (Training + Test) = 8.63 seconds\n",
      "epoch: 68 average loss: 0.870\n",
      "Test Accuracy : 71.2%, Test Loss: 0.8816687259823084\n",
      "Epoch Time (Training + Test) = 8.71 seconds\n",
      "epoch: 69 average loss: 0.863\n",
      "Test Accuracy : 71.9%, Test Loss: 0.8800684101879597\n",
      "Epoch Time (Training + Test) = 8.70 seconds\n",
      "epoch: 70 average loss: 0.881\n",
      "Test Accuracy : 71.6%, Test Loss: 0.8828191552311182\n",
      "Epoch Time (Training + Test) = 8.60 seconds\n",
      "epoch: 71 average loss: 0.862\n",
      "Test Accuracy : 70.6%, Test Loss: 0.8950391802936792\n",
      "Epoch Time (Training + Test) = 8.64 seconds\n",
      "epoch: 72 average loss: 0.873\n",
      "Test Accuracy : 71.0%, Test Loss: 0.880339240655303\n",
      "Epoch Time (Training + Test) = 8.48 seconds\n",
      "epoch: 73 average loss: 0.881\n",
      "Test Accuracy : 71.2%, Test Loss: 0.8924375027418137\n",
      "Epoch Time (Training + Test) = 8.52 seconds\n",
      "epoch: 74 average loss: 0.859\n",
      "Test Accuracy : 71.5%, Test Loss: 0.8875112142413855\n",
      "Epoch Time (Training + Test) = 8.60 seconds\n",
      "epoch: 75 average loss: 0.859\n",
      "Test Accuracy : 72.0%, Test Loss: 0.8825697954744101\n",
      "Epoch Time (Training + Test) = 8.53 seconds\n",
      "Data Saved to Grouped_Convolution_PW_Group_10.csv\n",
      "Finished Training: \n",
      "Total Time 0.357747 hours\n",
      " Average Time Per Epoch 17.17 seconds\n",
      "None 20\n",
      "BNN_Resnet_UniAdapt(\n",
      "  (conv1): BinarizeConv2d(3, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (bn1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "  (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(80, 160, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): BinarizeConv2d(80, 160, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(160, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): BinarizeConv2d(160, 320, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)\n",
      "  (bn2): BatchNorm1d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (bn3): BatchNorm1d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (logsoftmax): LogSoftmax(dim=-1)\n",
      "  (fc): BinarizeLinear(in_features=320, out_features=80, bias=True)\n",
      "  (adapter_dict): ModuleDict()\n",
      "  (head_bn): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (head): BinarizeLinear(in_features=640, out_features=20, bias=True)\n",
      "  (encoder_net): placeholder()\n",
      "  (uniAdaptNet): Sequential(\n",
      "    (0): uniAdapt_Net(\n",
      "      (layer1): thinBlock2(\n",
      "        (conv2): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=80, bias=False)\n",
      "        (bn2): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(80, 160, kernel_size=(1, 1), stride=(1, 1), groups=20, bias=False)\n",
      "        (bn3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (layer2): thinBlock2(\n",
      "        (conv2): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=160, bias=False)\n",
      "        (bn2): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(160, 320, kernel_size=(1, 1), stride=(1, 1), groups=20, bias=False)\n",
      "        (bn3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (layer3): thinBlock2(\n",
      "        (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=320, bias=False)\n",
      "        (bn2): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1), groups=20, bias=False)\n",
      "        (bn3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (pool1): AdaptiveAvgPool2d(output_size=16)\n",
      "      (pool2): AdaptiveAvgPool2d(output_size=8)\n",
      "      (pool3): AdaptiveAvgPool2d(output_size=1)\n",
      "    )\n",
      "    (1): Flatten(start_dim=1, end_dim=-1)\n",
      "  )\n",
      ")\n",
      "No lr scheduler\n",
      "Optimizer Set to New: {'state': {}, 'param_groups': [{'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': False, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]}]}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1upoqo7a) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Current Best Acc</td><td>▁▇▇█████████████████████████████████████</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>epoch time (s)</td><td>▁█▇▇▇▇▇▇▇▇▆▇▇▇▇▇▆▇▆█▇▇▆▇▆▇▇▇▇▇▆▇█▇▆▇█▇▆▇</td></tr><tr><td>lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_accuracy</td><td>▁▇██████████████████████████████████████</td></tr><tr><td>test_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>training_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Best Accuracy</td><td>0.7325</td></tr><tr><td>Current Best Acc</td><td>0.7325</td></tr><tr><td>Total Time (hours)</td><td>0.35775</td></tr><tr><td>epoch</td><td>75</td></tr><tr><td>epoch time (s)</td><td>8.53103</td></tr><tr><td>lr</td><td>0.001</td></tr><tr><td>test_accuracy</td><td>0.7205</td></tr><tr><td>test_loss</td><td>0.88257</td></tr><tr><td>training_loss</td><td>0.85899</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">hopeful-lion-4</strong>: <a href=\"https://wandb.ai/johnny_suu/FINAL_CIFAR100_GROUPED_CONV/runs/1upoqo7a\" target=\"_blank\">https://wandb.ai/johnny_suu/FINAL_CIFAR100_GROUPED_CONV/runs/1upoqo7a</a><br/>Synced 5 W&B file(s), 1 media file(s), 1 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230510_122415-1upoqo7a\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1upoqo7a). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3ba08ba76ed4394a25502087709f99c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016933333330477276, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\John Su\\Downloads\\SydneyUni\\2022\\thesis\\Thesis\\wandb\\run-20230510_123518-2dm24pt2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/johnny_suu/FINAL_CIFAR100_GROUPED_CONV/runs/2dm24pt2\" target=\"_blank\">dry-pine-5</a></strong> to <a href=\"https://wandb.ai/johnny_suu/FINAL_CIFAR100_GROUPED_CONV\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "Project Name: FINAL_CIFAR100_GROUPED_CONV, Run Name Grouped_Convolution_PW_Group_20 \n",
      "\n",
      "\n",
      "Run Start : 2023-05-10 12-35-18\n",
      "start_epoch : 0\n",
      "initial_lr : 0.001\n",
      "batch_size : 64\n",
      "epochs : 75\n",
      "epoch_chkpts : []\n",
      "device : cuda:0\n",
      "criterion : CrossEntropyLoss()\n",
      "optimizer : Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: False\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "seed : None\n",
      "model_architecture : <class 'NN_Thesis.nn_classes.BNN_Resnet_UniAdapt'>\n",
      "binerised_training : True\n",
      "Number of Elements : 4389580\n",
      "Initial accuracy:\n",
      "Test Accuracy : 4.6%, Test Loss: 47.34286396974211\n",
      "best_acc.pth saved!\n",
      "Test Accuracy : 5.7%, Test Loss: 47.4720504283905\n",
      "best_acc.pth saved!\n",
      "epoch: 1 average loss: 1.813\n",
      "Test Accuracy : 63.8%, Test Loss: 1.4363582953810692\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 8.59 seconds\n",
      "epoch: 2 average loss: 1.463\n",
      "Test Accuracy : 65.1%, Test Loss: 1.3291062489151955\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 8.62 seconds\n",
      "epoch: 3 average loss: 1.373\n",
      "Test Accuracy : 67.3%, Test Loss: 1.2401612624526024\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 8.67 seconds\n",
      "epoch: 4 average loss: 1.317\n",
      "Test Accuracy : 67.8%, Test Loss: 1.1964939050376415\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 8.67 seconds\n",
      "epoch: 5 average loss: 1.271\n",
      "Test Accuracy : 67.5%, Test Loss: 1.1567960307002068\n",
      "Epoch Time (Training + Test) = 8.51 seconds\n",
      "epoch: 6 average loss: 1.234\n",
      "Test Accuracy : 68.7%, Test Loss: 1.123250911012292\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 8.64 seconds\n",
      "epoch: 7 average loss: 1.216\n",
      "Test Accuracy : 68.6%, Test Loss: 1.1004749666899443\n",
      "Epoch Time (Training + Test) = 8.59 seconds\n",
      "epoch: 8 average loss: 1.184\n",
      "Test Accuracy : 69.5%, Test Loss: 1.0834835395216942\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 8.67 seconds\n",
      "epoch: 9 average loss: 1.153\n",
      "Test Accuracy : 68.0%, Test Loss: 1.077384451404214\n",
      "Epoch Time (Training + Test) = 8.54 seconds\n",
      "epoch: 10 average loss: 1.130\n",
      "Test Accuracy : 68.4%, Test Loss: 1.0582130942493677\n",
      "Epoch Time (Training + Test) = 8.59 seconds\n",
      "epoch: 11 average loss: 1.128\n",
      "Test Accuracy : 69.2%, Test Loss: 1.0379282012581825\n",
      "Epoch Time (Training + Test) = 8.60 seconds\n",
      "epoch: 12 average loss: 1.110\n",
      "Test Accuracy : 68.0%, Test Loss: 1.040095403790474\n",
      "Epoch Time (Training + Test) = 8.64 seconds\n",
      "epoch: 13 average loss: 1.096\n",
      "Test Accuracy : 69.0%, Test Loss: 1.0313264168798923\n",
      "Epoch Time (Training + Test) = 8.53 seconds\n",
      "epoch: 14 average loss: 1.098\n",
      "Test Accuracy : 70.4%, Test Loss: 1.0129956901073456\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 8.68 seconds\n",
      "epoch: 15 average loss: 1.075\n",
      "Test Accuracy : 69.1%, Test Loss: 1.0103374905884266\n",
      "Epoch Time (Training + Test) = 8.61 seconds\n",
      "epoch: 16 average loss: 1.082\n",
      "Test Accuracy : 70.2%, Test Loss: 1.0000307615846395\n",
      "Epoch Time (Training + Test) = 8.53 seconds\n",
      "epoch: 17 average loss: 1.061\n",
      "Test Accuracy : 69.3%, Test Loss: 1.017913606017828\n",
      "Epoch Time (Training + Test) = 8.56 seconds\n",
      "epoch: 18 average loss: 1.047\n",
      "Test Accuracy : 70.0%, Test Loss: 0.9740178026258945\n",
      "Epoch Time (Training + Test) = 8.63 seconds\n",
      "epoch: 19 average loss: 1.045\n",
      "Test Accuracy : 70.0%, Test Loss: 0.9830009192228317\n",
      "Epoch Time (Training + Test) = 8.57 seconds\n",
      "epoch: 20 average loss: 1.040\n",
      "Test Accuracy : 70.9%, Test Loss: 0.9764867499470711\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 8.65 seconds\n",
      "epoch: 21 average loss: 1.034\n",
      "Test Accuracy : 70.0%, Test Loss: 0.9586131032556295\n",
      "Epoch Time (Training + Test) = 8.62 seconds\n",
      "epoch: 22 average loss: 1.028\n",
      "Test Accuracy : 69.7%, Test Loss: 0.9757228102535009\n",
      "Epoch Time (Training + Test) = 8.62 seconds\n",
      "epoch: 23 average loss: 1.028\n",
      "Test Accuracy : 70.5%, Test Loss: 0.9649819582700729\n",
      "Epoch Time (Training + Test) = 8.52 seconds\n",
      "epoch: 24 average loss: 1.018\n",
      "Test Accuracy : 71.1%, Test Loss: 0.9549918733537197\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 8.64 seconds\n",
      "epoch: 25 average loss: 1.012\n",
      "Test Accuracy : 69.6%, Test Loss: 0.9838985335081816\n",
      "Epoch Time (Training + Test) = 8.56 seconds\n",
      "epoch: 26 average loss: 1.008\n",
      "Test Accuracy : 69.2%, Test Loss: 0.9678446352481842\n",
      "Epoch Time (Training + Test) = 8.53 seconds\n",
      "epoch: 27 average loss: 1.010\n",
      "Test Accuracy : 69.8%, Test Loss: 0.9519583247601986\n",
      "Epoch Time (Training + Test) = 8.49 seconds\n",
      "epoch: 28 average loss: 1.001\n",
      "Test Accuracy : 70.3%, Test Loss: 0.9659418798983097\n",
      "Epoch Time (Training + Test) = 8.58 seconds\n",
      "epoch: 29 average loss: 0.971\n",
      "Test Accuracy : 71.2%, Test Loss: 0.9602159913629293\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 8.63 seconds\n",
      "epoch: 30 average loss: 1.001\n",
      "Test Accuracy : 70.7%, Test Loss: 0.9472614321857691\n",
      "Epoch Time (Training + Test) = 8.54 seconds\n",
      "epoch: 31 average loss: 0.987\n",
      "Test Accuracy : 70.2%, Test Loss: 0.9437918476760387\n",
      "Epoch Time (Training + Test) = 8.58 seconds\n",
      "epoch: 32 average loss: 0.990\n",
      "Test Accuracy : 69.3%, Test Loss: 0.9482163023203611\n",
      "Epoch Time (Training + Test) = 8.55 seconds\n",
      "epoch: 33 average loss: 0.982\n",
      "Test Accuracy : 70.1%, Test Loss: 0.9511027242988348\n",
      "Epoch Time (Training + Test) = 8.46 seconds\n",
      "epoch: 34 average loss: 0.981\n",
      "Test Accuracy : 69.8%, Test Loss: 0.9378943387418985\n",
      "Epoch Time (Training + Test) = 8.59 seconds\n",
      "epoch: 35 average loss: 0.983\n",
      "Test Accuracy : 71.2%, Test Loss: 0.9526728056371212\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 8.49 seconds\n",
      "epoch: 36 average loss: 0.968\n",
      "Test Accuracy : 70.2%, Test Loss: 0.9341244399547577\n",
      "Epoch Time (Training + Test) = 8.60 seconds\n",
      "epoch: 37 average loss: 0.970\n",
      "Test Accuracy : 70.3%, Test Loss: 0.9243620876222849\n",
      "Epoch Time (Training + Test) = 8.52 seconds\n",
      "epoch: 38 average loss: 0.966\n",
      "Test Accuracy : 71.6%, Test Loss: 0.9303199537098408\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 8.52 seconds\n",
      "epoch: 39 average loss: 0.967\n",
      "Test Accuracy : 71.1%, Test Loss: 0.920916773378849\n",
      "Epoch Time (Training + Test) = 8.54 seconds\n",
      "epoch: 40 average loss: 0.977\n",
      "Test Accuracy : 70.6%, Test Loss: 0.9379021357744932\n",
      "Epoch Time (Training + Test) = 8.54 seconds\n",
      "epoch: 41 average loss: 0.960\n",
      "Test Accuracy : 70.5%, Test Loss: 0.9317988157272339\n",
      "Epoch Time (Training + Test) = 8.58 seconds\n",
      "epoch: 42 average loss: 0.968\n",
      "Test Accuracy : 70.0%, Test Loss: 0.9482535887509584\n",
      "Epoch Time (Training + Test) = 8.64 seconds\n",
      "epoch: 43 average loss: 0.971\n",
      "Test Accuracy : 70.7%, Test Loss: 0.9335834830999374\n",
      "Epoch Time (Training + Test) = 8.54 seconds\n",
      "epoch: 44 average loss: 0.939\n",
      "Test Accuracy : 70.7%, Test Loss: 0.9264075495302677\n",
      "Epoch Time (Training + Test) = 8.59 seconds\n",
      "epoch: 45 average loss: 0.958\n",
      "Test Accuracy : 71.1%, Test Loss: 0.905930770561099\n",
      "Epoch Time (Training + Test) = 8.64 seconds\n",
      "epoch: 46 average loss: 0.957\n",
      "Test Accuracy : 70.3%, Test Loss: 0.9363662675023079\n",
      "Epoch Time (Training + Test) = 8.62 seconds\n",
      "epoch: 47 average loss: 0.952\n",
      "Test Accuracy : 70.0%, Test Loss: 0.940257852897048\n",
      "Epoch Time (Training + Test) = 8.54 seconds\n",
      "epoch: 48 average loss: 0.954\n",
      "Test Accuracy : 69.8%, Test Loss: 0.9184665065258741\n",
      "Epoch Time (Training + Test) = 8.58 seconds\n",
      "epoch: 49 average loss: 0.953\n",
      "Test Accuracy : 69.8%, Test Loss: 0.9504208508878946\n",
      "Epoch Time (Training + Test) = 8.54 seconds\n",
      "epoch: 50 average loss: 0.953\n",
      "Test Accuracy : 71.3%, Test Loss: 0.9186845701187849\n",
      "Epoch Time (Training + Test) = 8.56 seconds\n",
      "epoch: 51 average loss: 0.957\n",
      "Test Accuracy : 70.9%, Test Loss: 0.9109021294862032\n",
      "Epoch Time (Training + Test) = 8.64 seconds\n",
      "epoch: 52 average loss: 0.954\n",
      "Test Accuracy : 70.8%, Test Loss: 0.9471285007894039\n",
      "Epoch Time (Training + Test) = 8.53 seconds\n",
      "epoch: 53 average loss: 0.956\n",
      "Test Accuracy : 71.1%, Test Loss: 0.9058144930750132\n",
      "Epoch Time (Training + Test) = 8.54 seconds\n",
      "epoch: 54 average loss: 0.935\n",
      "Test Accuracy : 71.0%, Test Loss: 0.9237111676484346\n",
      "Epoch Time (Training + Test) = 8.51 seconds\n",
      "epoch: 55 average loss: 0.933\n",
      "Test Accuracy : 71.2%, Test Loss: 0.923526817932725\n",
      "Epoch Time (Training + Test) = 8.65 seconds\n",
      "epoch: 56 average loss: 0.942\n",
      "Test Accuracy : 70.0%, Test Loss: 0.9257802087813616\n",
      "Epoch Time (Training + Test) = 8.55 seconds\n",
      "epoch: 57 average loss: 0.946\n",
      "Test Accuracy : 71.5%, Test Loss: 0.924555754289031\n",
      "Epoch Time (Training + Test) = 8.57 seconds\n",
      "epoch: 58 average loss: 0.933\n",
      "Test Accuracy : 70.2%, Test Loss: 0.9345775805413723\n",
      "Epoch Time (Training + Test) = 8.61 seconds\n",
      "epoch: 59 average loss: 0.948\n",
      "Test Accuracy : 71.2%, Test Loss: 0.9286864083260298\n",
      "Epoch Time (Training + Test) = 8.50 seconds\n",
      "epoch: 60 average loss: 0.941\n",
      "Test Accuracy : 72.2%, Test Loss: 0.9023488648235798\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 8.61 seconds\n",
      "epoch: 61 average loss: 0.947\n",
      "Test Accuracy : 70.9%, Test Loss: 0.9374800827354193\n",
      "Epoch Time (Training + Test) = 8.70 seconds\n",
      "epoch: 62 average loss: 0.934\n",
      "Test Accuracy : 71.0%, Test Loss: 0.9258976727724075\n",
      "Epoch Time (Training + Test) = 8.56 seconds\n",
      "epoch: 63 average loss: 0.938\n",
      "Test Accuracy : 70.4%, Test Loss: 0.9313430190086365\n",
      "Epoch Time (Training + Test) = 8.52 seconds\n",
      "epoch: 64 average loss: 0.926\n",
      "Test Accuracy : 70.7%, Test Loss: 0.9111654851585627\n",
      "Epoch Time (Training + Test) = 8.60 seconds\n",
      "epoch: 65 average loss: 0.946\n",
      "Test Accuracy : 71.0%, Test Loss: 0.9189404416829348\n",
      "Epoch Time (Training + Test) = 8.63 seconds\n",
      "epoch: 66 average loss: 0.942\n",
      "Test Accuracy : 71.2%, Test Loss: 0.9246385022997856\n",
      "Epoch Time (Training + Test) = 8.57 seconds\n",
      "epoch: 67 average loss: 0.933\n",
      "Test Accuracy : 71.5%, Test Loss: 0.9069611430168152\n",
      "Epoch Time (Training + Test) = 8.46 seconds\n",
      "epoch: 68 average loss: 0.927\n",
      "Test Accuracy : 70.0%, Test Loss: 0.9298686180263758\n",
      "Epoch Time (Training + Test) = 8.52 seconds\n",
      "epoch: 69 average loss: 0.934\n",
      "Test Accuracy : 71.2%, Test Loss: 0.9117393624037504\n",
      "Epoch Time (Training + Test) = 8.57 seconds\n",
      "epoch: 70 average loss: 0.945\n",
      "Test Accuracy : 70.5%, Test Loss: 0.9277319051325321\n",
      "Epoch Time (Training + Test) = 8.55 seconds\n",
      "epoch: 71 average loss: 0.913\n",
      "Test Accuracy : 70.8%, Test Loss: 0.9024159237742424\n",
      "Epoch Time (Training + Test) = 8.66 seconds\n",
      "epoch: 72 average loss: 0.930\n",
      "Test Accuracy : 70.8%, Test Loss: 0.922303594648838\n",
      "Epoch Time (Training + Test) = 8.51 seconds\n",
      "epoch: 73 average loss: 0.934\n",
      "Test Accuracy : 69.7%, Test Loss: 0.9326514508575201\n",
      "Epoch Time (Training + Test) = 8.50 seconds\n",
      "epoch: 74 average loss: 0.933\n",
      "Test Accuracy : 71.4%, Test Loss: 0.909403195604682\n",
      "Epoch Time (Training + Test) = 8.57 seconds\n",
      "epoch: 75 average loss: 0.934\n",
      "Test Accuracy : 71.2%, Test Loss: 0.8819309622049332\n",
      "Epoch Time (Training + Test) = 8.65 seconds\n",
      "Data Saved to Grouped_Convolution_PW_Group_20.csv\n",
      "Finished Training: \n",
      "Total Time 0.357404 hours\n",
      " Average Time Per Epoch 17.16 seconds\n",
      "None None\n",
      "BNN_Resnet_UniAdapt(\n",
      "  (conv1): BinarizeConv2d(3, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (bn1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "  (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(80, 160, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): BinarizeConv2d(80, 160, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(160, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): BinarizeConv2d(160, 320, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)\n",
      "  (bn2): BatchNorm1d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (bn3): BatchNorm1d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (logsoftmax): LogSoftmax(dim=-1)\n",
      "  (fc): BinarizeLinear(in_features=320, out_features=80, bias=True)\n",
      "  (adapter_dict): ModuleDict()\n",
      "  (head_bn): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (head): BinarizeLinear(in_features=640, out_features=20, bias=True)\n",
      "  (encoder_net): placeholder()\n",
      "  (uniAdaptNet): Sequential(\n",
      "    (0): uniAdapt_Net(\n",
      "      (layer1): thinBlock2(\n",
      "        (conv2): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=80, bias=False)\n",
      "        (bn2): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(80, 160, kernel_size=(1, 1), stride=(1, 1), groups=80, bias=False)\n",
      "        (bn3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (layer2): thinBlock2(\n",
      "        (conv2): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=160, bias=False)\n",
      "        (bn2): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(160, 320, kernel_size=(1, 1), stride=(1, 1), groups=160, bias=False)\n",
      "        (bn3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (layer3): thinBlock2(\n",
      "        (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=320, bias=False)\n",
      "        (bn2): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1), groups=320, bias=False)\n",
      "        (bn3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (pool1): AdaptiveAvgPool2d(output_size=16)\n",
      "      (pool2): AdaptiveAvgPool2d(output_size=8)\n",
      "      (pool3): AdaptiveAvgPool2d(output_size=1)\n",
      "    )\n",
      "    (1): Flatten(start_dim=1, end_dim=-1)\n",
      "  )\n",
      ")\n",
      "No lr scheduler\n",
      "Optimizer Set to New: {'state': {}, 'param_groups': [{'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': False, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]}]}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2dm24pt2) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Current Best Acc</td><td>▁▇▇█████████████████████████████████████</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>epoch time (s)</td><td>▁▇█▇▇▇▇▇▇▇▇█▇▇▇▇▇▇▇▇▇▇█▇█▇▇█▇█▇▇█▇█▇▇█▇█</td></tr><tr><td>lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_accuracy</td><td>▁▇██████████████████████████████████████</td></tr><tr><td>test_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>training_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Best Accuracy</td><td>0.722</td></tr><tr><td>Current Best Acc</td><td>0.722</td></tr><tr><td>Total Time (hours)</td><td>0.3574</td></tr><tr><td>epoch</td><td>75</td></tr><tr><td>epoch time (s)</td><td>8.64961</td></tr><tr><td>lr</td><td>0.001</td></tr><tr><td>test_accuracy</td><td>0.7125</td></tr><tr><td>test_loss</td><td>0.88193</td></tr><tr><td>training_loss</td><td>0.93393</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">dry-pine-5</strong>: <a href=\"https://wandb.ai/johnny_suu/FINAL_CIFAR100_GROUPED_CONV/runs/2dm24pt2\" target=\"_blank\">https://wandb.ai/johnny_suu/FINAL_CIFAR100_GROUPED_CONV/runs/2dm24pt2</a><br/>Synced 5 W&B file(s), 1 media file(s), 1 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230510_123518-2dm24pt2\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2dm24pt2). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d68dd13a0554c8792937e2317657003",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\John Su\\Downloads\\SydneyUni\\2022\\thesis\\Thesis\\wandb\\run-20230510_124621-n30xhxf2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/johnny_suu/FINAL_CIFAR100_GROUPED_CONV/runs/n30xhxf2\" target=\"_blank\">stilted-cherry-6</a></strong> to <a href=\"https://wandb.ai/johnny_suu/FINAL_CIFAR100_GROUPED_CONV\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "Project Name: FINAL_CIFAR100_GROUPED_CONV, Run Name Grouped_Convolution_PW_Group_None \n",
      "\n",
      "\n",
      "Run Start : 2023-05-10 12-46-21\n",
      "start_epoch : 0\n",
      "initial_lr : 0.001\n",
      "batch_size : 64\n",
      "epochs : 75\n",
      "epoch_chkpts : []\n",
      "device : cuda:0\n",
      "criterion : CrossEntropyLoss()\n",
      "optimizer : Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: False\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "seed : None\n",
      "model_architecture : <class 'NN_Thesis.nn_classes.BNN_Resnet_UniAdapt'>\n",
      "binerised_training : True\n",
      "Number of Elements : 4382060\n",
      "Initial accuracy:\n",
      "Test Accuracy : 5.4%, Test Loss: 46.566445198788\n",
      "best_acc.pth saved!\n",
      "Test Accuracy : 5.9%, Test Loss: 48.99974286556244\n",
      "best_acc.pth saved!\n",
      "epoch: 1 average loss: 1.838\n",
      "Test Accuracy : 62.9%, Test Loss: 1.4534292481839657\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 8.53 seconds\n",
      "epoch: 2 average loss: 1.481\n",
      "Test Accuracy : 67.0%, Test Loss: 1.3221109844744205\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 8.52 seconds\n",
      "epoch: 3 average loss: 1.387\n",
      "Test Accuracy : 67.0%, Test Loss: 1.2544922269880772\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 8.51 seconds\n",
      "epoch: 4 average loss: 1.336\n",
      "Test Accuracy : 66.8%, Test Loss: 1.2280299812555313\n",
      "Epoch Time (Training + Test) = 8.43 seconds\n",
      "epoch: 5 average loss: 1.308\n",
      "Test Accuracy : 65.3%, Test Loss: 1.2097154818475246\n",
      "Epoch Time (Training + Test) = 8.43 seconds\n",
      "epoch: 6 average loss: 1.266\n",
      "Test Accuracy : 67.5%, Test Loss: 1.1666456796228886\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 8.47 seconds\n",
      "epoch: 7 average loss: 1.244\n",
      "Test Accuracy : 69.1%, Test Loss: 1.1220327597111464\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 8.48 seconds\n",
      "epoch: 8 average loss: 1.219\n",
      "Test Accuracy : 68.7%, Test Loss: 1.124412888661027\n",
      "Epoch Time (Training + Test) = 8.44 seconds\n",
      "epoch: 9 average loss: 1.203\n",
      "Test Accuracy : 68.1%, Test Loss: 1.09459456987679\n",
      "Epoch Time (Training + Test) = 8.45 seconds\n",
      "epoch: 10 average loss: 1.190\n",
      "Test Accuracy : 69.0%, Test Loss: 1.076110104098916\n",
      "Epoch Time (Training + Test) = 8.47 seconds\n",
      "epoch: 11 average loss: 1.184\n",
      "Test Accuracy : 67.2%, Test Loss: 1.0873539242893457\n",
      "Epoch Time (Training + Test) = 8.44 seconds\n",
      "epoch: 12 average loss: 1.160\n",
      "Test Accuracy : 68.1%, Test Loss: 1.0783551391214132\n",
      "Epoch Time (Training + Test) = 8.45 seconds\n",
      "epoch: 13 average loss: 1.150\n",
      "Test Accuracy : 67.2%, Test Loss: 1.0599661264568567\n",
      "Epoch Time (Training + Test) = 8.48 seconds\n",
      "epoch: 14 average loss: 1.145\n",
      "Test Accuracy : 67.0%, Test Loss: 1.0752751901745796\n",
      "Epoch Time (Training + Test) = 8.43 seconds\n",
      "epoch: 15 average loss: 1.140\n",
      "Test Accuracy : 69.2%, Test Loss: 1.0331019759178162\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 8.51 seconds\n",
      "epoch: 16 average loss: 1.122\n",
      "Test Accuracy : 68.5%, Test Loss: 1.0486836973577738\n",
      "Epoch Time (Training + Test) = 8.42 seconds\n",
      "epoch: 17 average loss: 1.126\n",
      "Test Accuracy : 67.0%, Test Loss: 1.0595186613500118\n",
      "Epoch Time (Training + Test) = 8.44 seconds\n",
      "epoch: 18 average loss: 1.108\n",
      "Test Accuracy : 67.2%, Test Loss: 1.0332866720855236\n",
      "Epoch Time (Training + Test) = 8.45 seconds\n",
      "epoch: 19 average loss: 1.118\n",
      "Test Accuracy : 67.5%, Test Loss: 1.0396867375820875\n",
      "Epoch Time (Training + Test) = 8.45 seconds\n",
      "epoch: 20 average loss: 1.106\n",
      "Test Accuracy : 67.5%, Test Loss: 1.0312642194330692\n",
      "Epoch Time (Training + Test) = 8.46 seconds\n",
      "epoch: 21 average loss: 1.101\n",
      "Test Accuracy : 67.5%, Test Loss: 1.0207485184073448\n",
      "Epoch Time (Training + Test) = 8.47 seconds\n",
      "epoch: 22 average loss: 1.091\n",
      "Test Accuracy : 68.0%, Test Loss: 1.01908022724092\n",
      "Epoch Time (Training + Test) = 8.45 seconds\n",
      "epoch: 23 average loss: 1.094\n",
      "Test Accuracy : 67.9%, Test Loss: 1.0509244985878468\n",
      "Epoch Time (Training + Test) = 8.42 seconds\n",
      "epoch: 24 average loss: 1.091\n",
      "Test Accuracy : 68.0%, Test Loss: 1.0089343935251236\n",
      "Epoch Time (Training + Test) = 8.46 seconds\n",
      "epoch: 25 average loss: 1.092\n",
      "Test Accuracy : 69.0%, Test Loss: 1.0106396898627281\n",
      "Epoch Time (Training + Test) = 8.42 seconds\n",
      "epoch: 26 average loss: 1.077\n",
      "Test Accuracy : 68.7%, Test Loss: 1.0242403261363506\n",
      "Epoch Time (Training + Test) = 8.46 seconds\n",
      "epoch: 27 average loss: 1.075\n",
      "Test Accuracy : 68.2%, Test Loss: 1.0067562609910965\n",
      "Epoch Time (Training + Test) = 8.45 seconds\n",
      "epoch: 28 average loss: 1.076\n",
      "Test Accuracy : 68.5%, Test Loss: 1.0044655483216047\n",
      "Epoch Time (Training + Test) = 8.43 seconds\n",
      "epoch: 29 average loss: 1.071\n",
      "Test Accuracy : 68.2%, Test Loss: 1.0148540809750557\n",
      "Epoch Time (Training + Test) = 8.51 seconds\n",
      "epoch: 30 average loss: 1.069\n",
      "Test Accuracy : 69.1%, Test Loss: 0.973627608269453\n",
      "Epoch Time (Training + Test) = 8.45 seconds\n",
      "epoch: 31 average loss: 1.066\n",
      "Test Accuracy : 69.1%, Test Loss: 0.9969068933278322\n",
      "Epoch Time (Training + Test) = 8.49 seconds\n",
      "epoch: 32 average loss: 1.079\n",
      "Test Accuracy : 68.7%, Test Loss: 1.003083735704422\n",
      "Epoch Time (Training + Test) = 8.44 seconds\n",
      "epoch: 33 average loss: 1.056\n",
      "Test Accuracy : 69.3%, Test Loss: 0.9813709296286106\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 8.48 seconds\n",
      "epoch: 34 average loss: 1.061\n",
      "Test Accuracy : 70.7%, Test Loss: 0.9589039795100689\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 8.43 seconds\n",
      "epoch: 35 average loss: 1.070\n",
      "Test Accuracy : 69.2%, Test Loss: 0.9861329291015863\n",
      "Epoch Time (Training + Test) = 8.45 seconds\n",
      "epoch: 36 average loss: 1.055\n",
      "Test Accuracy : 69.1%, Test Loss: 1.0006956588476896\n",
      "Epoch Time (Training + Test) = 8.43 seconds\n",
      "epoch: 37 average loss: 1.053\n",
      "Test Accuracy : 68.7%, Test Loss: 0.9862456284463406\n",
      "Epoch Time (Training + Test) = 8.48 seconds\n",
      "epoch: 38 average loss: 1.055\n",
      "Test Accuracy : 68.3%, Test Loss: 0.9948267247527838\n",
      "Epoch Time (Training + Test) = 8.44 seconds\n",
      "epoch: 39 average loss: 1.052\n",
      "Test Accuracy : 69.2%, Test Loss: 0.9882945157587528\n",
      "Epoch Time (Training + Test) = 8.46 seconds\n",
      "epoch: 40 average loss: 1.052\n",
      "Test Accuracy : 67.5%, Test Loss: 0.9924207702279091\n",
      "Epoch Time (Training + Test) = 8.44 seconds\n",
      "epoch: 41 average loss: 1.045\n",
      "Test Accuracy : 70.2%, Test Loss: 0.9697549846023321\n",
      "Epoch Time (Training + Test) = 8.48 seconds\n",
      "epoch: 42 average loss: 1.056\n",
      "Test Accuracy : 67.8%, Test Loss: 0.9878179784864187\n",
      "Epoch Time (Training + Test) = 8.43 seconds\n",
      "epoch: 43 average loss: 1.037\n",
      "Test Accuracy : 67.0%, Test Loss: 0.984249172732234\n",
      "Epoch Time (Training + Test) = 8.47 seconds\n",
      "epoch: 44 average loss: 1.050\n",
      "Test Accuracy : 67.7%, Test Loss: 0.9872241113334894\n",
      "Epoch Time (Training + Test) = 8.42 seconds\n",
      "epoch: 45 average loss: 1.048\n",
      "Test Accuracy : 69.2%, Test Loss: 0.9704640004783869\n",
      "Epoch Time (Training + Test) = 8.46 seconds\n",
      "epoch: 46 average loss: 1.047\n",
      "Test Accuracy : 69.8%, Test Loss: 0.957365270704031\n",
      "Epoch Time (Training + Test) = 8.46 seconds\n",
      "epoch: 47 average loss: 1.054\n",
      "Test Accuracy : 69.4%, Test Loss: 0.9681441895663738\n",
      "Epoch Time (Training + Test) = 8.46 seconds\n",
      "epoch: 48 average loss: 1.046\n",
      "Test Accuracy : 69.8%, Test Loss: 0.9621542487293482\n",
      "Epoch Time (Training + Test) = 8.48 seconds\n",
      "epoch: 49 average loss: 1.037\n",
      "Test Accuracy : 68.8%, Test Loss: 0.9839165825396776\n",
      "Epoch Time (Training + Test) = 8.45 seconds\n",
      "epoch: 50 average loss: 1.034\n",
      "Test Accuracy : 67.0%, Test Loss: 1.0176923032850027\n",
      "Epoch Time (Training + Test) = 8.45 seconds\n",
      "epoch: 51 average loss: 1.042\n",
      "Test Accuracy : 68.7%, Test Loss: 0.973023833706975\n",
      "Epoch Time (Training + Test) = 8.48 seconds\n",
      "epoch: 52 average loss: 1.028\n",
      "Test Accuracy : 68.2%, Test Loss: 0.9739085119217634\n",
      "Epoch Time (Training + Test) = 8.46 seconds\n",
      "epoch: 53 average loss: 1.043\n",
      "Test Accuracy : 68.2%, Test Loss: 1.0003758035600185\n",
      "Epoch Time (Training + Test) = 8.42 seconds\n",
      "epoch: 54 average loss: 1.029\n",
      "Test Accuracy : 68.3%, Test Loss: 0.9705577734857798\n",
      "Epoch Time (Training + Test) = 8.49 seconds\n",
      "epoch: 55 average loss: 1.034\n",
      "Test Accuracy : 68.8%, Test Loss: 0.9664861653000116\n",
      "Epoch Time (Training + Test) = 8.48 seconds\n",
      "epoch: 56 average loss: 1.032\n",
      "Test Accuracy : 68.1%, Test Loss: 0.9824058748781681\n",
      "Epoch Time (Training + Test) = 8.43 seconds\n",
      "epoch: 57 average loss: 1.027\n",
      "Test Accuracy : 68.4%, Test Loss: 0.9675893802195787\n",
      "Epoch Time (Training + Test) = 8.48 seconds\n",
      "epoch: 58 average loss: 1.012\n",
      "Test Accuracy : 68.5%, Test Loss: 0.977095115929842\n",
      "Epoch Time (Training + Test) = 8.46 seconds\n",
      "epoch: 59 average loss: 1.036\n",
      "Test Accuracy : 67.8%, Test Loss: 0.9908287022262812\n",
      "Epoch Time (Training + Test) = 8.42 seconds\n",
      "epoch: 60 average loss: 1.031\n",
      "Test Accuracy : 69.0%, Test Loss: 0.9846130013465881\n",
      "Epoch Time (Training + Test) = 8.43 seconds\n",
      "epoch: 61 average loss: 1.023\n",
      "Test Accuracy : 69.1%, Test Loss: 0.9821178056299686\n",
      "Epoch Time (Training + Test) = 8.42 seconds\n",
      "epoch: 62 average loss: 1.042\n",
      "Test Accuracy : 67.8%, Test Loss: 0.9793388601392508\n",
      "Epoch Time (Training + Test) = 8.47 seconds\n",
      "epoch: 63 average loss: 1.026\n",
      "Test Accuracy : 67.8%, Test Loss: 0.9877922628074884\n",
      "Epoch Time (Training + Test) = 8.49 seconds\n",
      "epoch: 64 average loss: 1.028\n",
      "Test Accuracy : 69.4%, Test Loss: 0.9676220584660769\n",
      "Epoch Time (Training + Test) = 8.43 seconds\n",
      "epoch: 65 average loss: 1.021\n",
      "Test Accuracy : 69.7%, Test Loss: 0.9646117277443409\n",
      "Epoch Time (Training + Test) = 8.45 seconds\n",
      "epoch: 66 average loss: 1.030\n",
      "Test Accuracy : 68.8%, Test Loss: 0.9829617515206337\n",
      "Epoch Time (Training + Test) = 8.44 seconds\n",
      "epoch: 67 average loss: 1.022\n",
      "Test Accuracy : 69.5%, Test Loss: 0.9652601853013039\n",
      "Epoch Time (Training + Test) = 8.47 seconds\n",
      "epoch: 68 average loss: 1.034\n",
      "Test Accuracy : 68.5%, Test Loss: 0.9621950145810843\n",
      "Epoch Time (Training + Test) = 8.44 seconds\n",
      "epoch: 69 average loss: 1.024\n",
      "Test Accuracy : 68.3%, Test Loss: 0.9809077624231577\n",
      "Epoch Time (Training + Test) = 8.44 seconds\n",
      "epoch: 70 average loss: 1.031\n",
      "Test Accuracy : 68.9%, Test Loss: 0.9665336944162846\n",
      "Epoch Time (Training + Test) = 8.45 seconds\n",
      "epoch: 71 average loss: 1.013\n",
      "Test Accuracy : 69.5%, Test Loss: 0.9683850258588791\n",
      "Epoch Time (Training + Test) = 8.45 seconds\n",
      "epoch: 72 average loss: 1.027\n",
      "Test Accuracy : 69.2%, Test Loss: 0.9612214453518391\n",
      "Epoch Time (Training + Test) = 8.43 seconds\n",
      "epoch: 73 average loss: 1.018\n",
      "Test Accuracy : 68.4%, Test Loss: 0.9538430590182543\n",
      "Epoch Time (Training + Test) = 8.46 seconds\n",
      "epoch: 74 average loss: 1.025\n",
      "Test Accuracy : 68.2%, Test Loss: 0.9548246935009956\n",
      "Epoch Time (Training + Test) = 8.50 seconds\n",
      "epoch: 75 average loss: 1.018\n",
      "Test Accuracy : 68.2%, Test Loss: 0.9893180355429649\n",
      "Epoch Time (Training + Test) = 8.45 seconds\n",
      "Data Saved to Grouped_Convolution_PW_Group_None.csv\n",
      "Finished Training: \n",
      "Total Time 0.352335 hours\n",
      " Average Time Per Epoch 16.91 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "conv2_groups = [1,2,5,10,20,None]\n",
    "conv1_groups = [None]*len(conv2_groups)\n",
    "\n",
    "for conv1_group, conv2_group in zip(conv1_groups,conv2_groups):\n",
    "    print(conv1_group,conv2_group)\n",
    "    \n",
    "    BNN = BNN_Resnet_UniAdapt(80,freeze_pre_weights=True)\n",
    "    BNN.layer3[-1].do_bntan = True\n",
    "    BNN.load_state_dict(state,strict=False)\n",
    "    BNN.freeze()\n",
    "\n",
    "    BNN = BNN.to('cuda:0')\n",
    "    BNN.encoder_net = placeholder()\n",
    "\n",
    "    n1 = uniAdapt_Net([80,160,320],[80,160,320],block = thinBlock2,conv1_group = conv1_group,conv2_group = conv2_group)\n",
    "    n1 = n1.cuda()\n",
    "    n1.train()\n",
    "\n",
    "    adapt_net = nn.Sequential(n1,nn.Flatten())\n",
    "    \n",
    "    adapt_net.to('cuda:0')\n",
    "\n",
    "    BNN.head_bn =  nn.BatchNorm1d(20)\n",
    "    BNN.head =  BinarizeLinear(320 + 320,20)\n",
    "    BNN.add_UniAdapter(adapt_net)\n",
    "    BNN = BNN.to('cuda:0')\n",
    "    BNN.uniAdapt = True\n",
    "    print(BNN)\n",
    "    trainer = Trainer(BNN,model_name=f'Grouped_Convolution_PW_Group_{conv2_group}',project_name='FINAL_CIFAR100_GROUPED_CONV',binarise=True,classes=train_20.classes)\n",
    "    trainer.lr = 1e-3\n",
    "    trainer.batch_size = 64\n",
    "    trainer.epochs =75\n",
    "    trainer.epoch_chkpts = []\n",
    "    trainer.start_epoch = 0\n",
    "    # trainer.amount = [0.8,0]\n",
    "    # trainer.pruning_rounds = 1 # Only need this for iterative pruning\n",
    "    # trainer.model_to_prune = trainer.model.uniAdaptNet[0]\n",
    "    params = nn.ModuleList([BNN.uniAdaptNet,BNN.head_bn,BNN.head]).parameters()\n",
    "    # params = nn.ModuleList([BNN.head_bn,BNN.head]).parameters()\n",
    "    trainer.set_scheduler(None)\n",
    "    trainer.set_optimizer(torch.optim.Adam,params,lr = trainer.lr)\n",
    "    trainer.train(train20_DL,test20_DL,prune_strat=None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serial"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seperate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ALL 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4334980\n",
      "4341260\n",
      "6280\n",
      "4354240\n",
      "4360640\n",
      "6400\n",
      "4334980\n",
      "4341260\n",
      "6280\n",
      "group 1 Total Elements 136640 96\n",
      "4354240\n",
      "4360640\n",
      "6400\n",
      "4334980\n",
      "4341260\n",
      "6280\n",
      "group 2 Total Elements 69440 49\n",
      "4354240\n",
      "4360640\n",
      "6400\n",
      "4334980\n",
      "4341260\n",
      "6280\n",
      "group 5 Total Elements 29120 21\n",
      "4354240\n",
      "4360640\n",
      "6400\n",
      "4334980\n",
      "4341260\n",
      "6280\n",
      "group 10 Total Elements 15680 11\n",
      "4354240\n",
      "4360640\n",
      "6400\n",
      "4334980\n",
      "4341260\n",
      "6280\n",
      "group 20 Total Elements 8960 6\n"
     ]
    }
   ],
   "source": [
    "T,_,_ = total_elem(resnet18_adapt(20))\n",
    "for group in [1,2,5,10,20]:\n",
    "\n",
    "    conv_bn = conv_channel_adapter3(80,groups = group,kernel = 1, padding= 0,nonlinearity='relu')\n",
    "    conv_bn2 = conv_channel_adapter3(160,groups = group,kernel = 1, padding= 0,nonlinearity='relu')\n",
    "    conv_bn3 = conv_channel_adapter3(320,groups = group,kernel = 1, padding= 0,nonlinearity='relu')\n",
    "    T,_,_ = total_elem(resnet18_adapt(80))\n",
    "    total = 0\n",
    "    for cnn in [conv_bn,conv_bn2,conv_bn3]:\n",
    "        total += sum([p.numel() for p in cnn.parameters()])\n",
    "    \n",
    "    \n",
    "    print(f'group {group} Total Elements {total} {weight_pct(total,20)*100:.0f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No lr scheduler\n",
      "Optimizer Set to New: {'state': {}, 'param_groups': [{'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': False, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69]}]}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:n30xhxf2) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Current Best Acc</td><td>▁▇██████████████████████████████████████</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>epoch time (s)</td><td>▁██▇█▇▇██▇▇█▇▇█▇▇▇▇▇▇▇▇▇▇█▇█▇██▇▇█▇█▇▇█▇</td></tr><tr><td>lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_accuracy</td><td>▁▇█▇████████████████████████████████████</td></tr><tr><td>test_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>training_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Best Accuracy</td><td>0.7065</td></tr><tr><td>Current Best Acc</td><td>0.7065</td></tr><tr><td>Total Time (hours)</td><td>0.35233</td></tr><tr><td>epoch</td><td>75</td></tr><tr><td>epoch time (s)</td><td>8.45152</td></tr><tr><td>lr</td><td>0.001</td></tr><tr><td>test_accuracy</td><td>0.682</td></tr><tr><td>test_loss</td><td>0.98932</td></tr><tr><td>training_loss</td><td>1.01815</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">stilted-cherry-6</strong>: <a href=\"https://wandb.ai/johnny_suu/FINAL_CIFAR100_GROUPED_CONV/runs/n30xhxf2\" target=\"_blank\">https://wandb.ai/johnny_suu/FINAL_CIFAR100_GROUPED_CONV/runs/n30xhxf2</a><br/>Synced 5 W&B file(s), 1 media file(s), 1 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230510_124621-n30xhxf2\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:n30xhxf2). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c649a865b86499fa5d90875a2c34741",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\John Su\\Downloads\\SydneyUni\\2022\\thesis\\Thesis\\wandb\\run-20230510_125714-vl9go1rf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/johnny_suu/FINAL_CIFAR100_SERIAL_ALL3/runs/vl9go1rf\" target=\"_blank\">smart-resonance-1</a></strong> to <a href=\"https://wandb.ai/johnny_suu/FINAL_CIFAR100_SERIAL_ALL3\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "Project Name: FINAL_CIFAR100_SERIAL_ALL3, Run Name 3_adapter_groups_1 \n",
      "\n",
      "\n",
      "Run Start : 2023-05-10 12-57-14\n",
      "start_epoch : 0\n",
      "initial_lr : 0.001\n",
      "batch_size : 64\n",
      "epochs : 75\n",
      "epoch_chkpts : []\n",
      "device : cuda:0\n",
      "criterion : CrossEntropyLoss()\n",
      "optimizer : Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: False\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "seed : 123\n",
      "model_architecture : <class 'NN_Thesis.nn_classes.BNN_Resnet_UniAdapt'>\n",
      "binerised_training : True\n",
      "Number of Elements : 4517340\n",
      "Initial accuracy:\n",
      "Test Accuracy : 5.8%, Test Loss: 32.381502734627695\n",
      "best_acc.pth saved!\n",
      "Test Accuracy : 5.2%, Test Loss: 31.557703912258148\n",
      "epoch: 1 average loss: 1.769\n",
      "Test Accuracy : 63.6%, Test Loss: 1.382547788321972\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.21 seconds\n",
      "epoch: 2 average loss: 1.379\n",
      "Test Accuracy : 68.3%, Test Loss: 1.2128282450139523\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.33 seconds\n",
      "epoch: 3 average loss: 1.256\n",
      "Test Accuracy : 71.5%, Test Loss: 1.094719160348177\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.18 seconds\n",
      "epoch: 4 average loss: 1.173\n",
      "Test Accuracy : 71.5%, Test Loss: 1.0492797661572695\n",
      "Epoch Time (Training + Test) = 9.22 seconds\n",
      "epoch: 5 average loss: 1.104\n",
      "Test Accuracy : 72.2%, Test Loss: 1.011044267565012\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.28 seconds\n",
      "epoch: 6 average loss: 1.060\n",
      "Test Accuracy : 72.4%, Test Loss: 0.9559118691831827\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.25 seconds\n",
      "epoch: 7 average loss: 1.017\n",
      "Test Accuracy : 70.7%, Test Loss: 0.9752879161387682\n",
      "Epoch Time (Training + Test) = 9.25 seconds\n",
      "epoch: 8 average loss: 0.990\n",
      "Test Accuracy : 73.7%, Test Loss: 0.9224467668682337\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.26 seconds\n",
      "epoch: 9 average loss: 0.975\n",
      "Test Accuracy : 73.0%, Test Loss: 0.908525375649333\n",
      "Epoch Time (Training + Test) = 9.21 seconds\n",
      "epoch: 10 average loss: 0.937\n",
      "Test Accuracy : 73.7%, Test Loss: 0.8953848294913769\n",
      "Epoch Time (Training + Test) = 9.21 seconds\n",
      "epoch: 11 average loss: 0.913\n",
      "Test Accuracy : 73.4%, Test Loss: 0.8907863609492779\n",
      "Epoch Time (Training + Test) = 9.23 seconds\n",
      "epoch: 12 average loss: 0.898\n",
      "Test Accuracy : 73.9%, Test Loss: 0.876921271905303\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.23 seconds\n",
      "epoch: 13 average loss: 0.887\n",
      "Test Accuracy : 74.4%, Test Loss: 0.855211902409792\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.25 seconds\n",
      "epoch: 14 average loss: 0.880\n",
      "Test Accuracy : 74.4%, Test Loss: 0.8556803967803717\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.27 seconds\n",
      "epoch: 15 average loss: 0.858\n",
      "Test Accuracy : 74.2%, Test Loss: 0.8512584678828716\n",
      "Epoch Time (Training + Test) = 9.23 seconds\n",
      "epoch: 16 average loss: 0.856\n",
      "Test Accuracy : 73.5%, Test Loss: 0.84194640442729\n",
      "Epoch Time (Training + Test) = 9.20 seconds\n",
      "epoch: 17 average loss: 0.833\n",
      "Test Accuracy : 74.2%, Test Loss: 0.824480015784502\n",
      "Epoch Time (Training + Test) = 9.13 seconds\n",
      "epoch: 18 average loss: 0.822\n",
      "Test Accuracy : 74.4%, Test Loss: 0.838811295107007\n",
      "Epoch Time (Training + Test) = 9.24 seconds\n",
      "epoch: 19 average loss: 0.817\n",
      "Test Accuracy : 75.0%, Test Loss: 0.8129646424204111\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.26 seconds\n",
      "epoch: 20 average loss: 0.802\n",
      "Test Accuracy : 74.1%, Test Loss: 0.820885787717998\n",
      "Epoch Time (Training + Test) = 9.31 seconds\n",
      "epoch: 21 average loss: 0.782\n",
      "Test Accuracy : 74.1%, Test Loss: 0.8162937331944704\n",
      "Epoch Time (Training + Test) = 9.23 seconds\n",
      "epoch: 22 average loss: 0.785\n",
      "Test Accuracy : 74.3%, Test Loss: 0.8346848208457232\n",
      "Epoch Time (Training + Test) = 9.22 seconds\n",
      "epoch: 23 average loss: 0.780\n",
      "Test Accuracy : 75.0%, Test Loss: 0.8088184092193842\n",
      "Epoch Time (Training + Test) = 9.24 seconds\n",
      "epoch: 24 average loss: 0.780\n",
      "Test Accuracy : 75.3%, Test Loss: 0.8066616374999285\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.28 seconds\n",
      "epoch: 25 average loss: 0.754\n",
      "Test Accuracy : 74.1%, Test Loss: 0.8188890554010868\n",
      "Epoch Time (Training + Test) = 9.09 seconds\n",
      "epoch: 26 average loss: 0.762\n",
      "Test Accuracy : 74.7%, Test Loss: 0.7919650468975306\n",
      "Epoch Time (Training + Test) = 9.22 seconds\n",
      "epoch: 27 average loss: 0.748\n",
      "Test Accuracy : 74.5%, Test Loss: 0.8276120722293854\n",
      "Epoch Time (Training + Test) = 9.33 seconds\n",
      "epoch: 28 average loss: 0.757\n",
      "Test Accuracy : 75.0%, Test Loss: 0.8378628194332123\n",
      "Epoch Time (Training + Test) = 9.20 seconds\n",
      "epoch: 29 average loss: 0.743\n",
      "Test Accuracy : 73.6%, Test Loss: 0.8167186677455902\n",
      "Epoch Time (Training + Test) = 9.27 seconds\n",
      "epoch: 30 average loss: 0.744\n",
      "Test Accuracy : 74.5%, Test Loss: 0.8068581102415919\n",
      "Epoch Time (Training + Test) = 9.18 seconds\n",
      "epoch: 31 average loss: 0.734\n",
      "Test Accuracy : 75.4%, Test Loss: 0.79805126786232\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.18 seconds\n",
      "epoch: 32 average loss: 0.725\n",
      "Test Accuracy : 73.9%, Test Loss: 0.805645314976573\n",
      "Epoch Time (Training + Test) = 9.22 seconds\n",
      "epoch: 33 average loss: 0.707\n",
      "Test Accuracy : 74.1%, Test Loss: 0.8226279821246862\n",
      "Epoch Time (Training + Test) = 9.21 seconds\n",
      "epoch: 34 average loss: 0.726\n",
      "Test Accuracy : 73.9%, Test Loss: 0.8256660522893071\n",
      "Epoch Time (Training + Test) = 9.24 seconds\n",
      "epoch: 35 average loss: 0.722\n",
      "Test Accuracy : 74.1%, Test Loss: 0.8127684351056814\n",
      "Epoch Time (Training + Test) = 9.20 seconds\n",
      "epoch: 36 average loss: 0.721\n",
      "Test Accuracy : 75.0%, Test Loss: 0.8097205543890595\n",
      "Epoch Time (Training + Test) = 9.18 seconds\n",
      "epoch: 37 average loss: 0.695\n",
      "Test Accuracy : 74.0%, Test Loss: 0.8120921850204468\n",
      "Epoch Time (Training + Test) = 9.21 seconds\n",
      "epoch: 38 average loss: 0.717\n",
      "Test Accuracy : 75.1%, Test Loss: 0.7840502727776766\n",
      "Epoch Time (Training + Test) = 9.09 seconds\n",
      "epoch: 39 average loss: 0.705\n",
      "Test Accuracy : 74.0%, Test Loss: 0.8303570002317429\n",
      "Epoch Time (Training + Test) = 9.20 seconds\n",
      "epoch: 40 average loss: 0.695\n",
      "Test Accuracy : 76.1%, Test Loss: 0.7828527875244617\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.18 seconds\n",
      "epoch: 41 average loss: 0.680\n",
      "Test Accuracy : 74.1%, Test Loss: 0.8155798874795437\n",
      "Epoch Time (Training + Test) = 9.31 seconds\n",
      "epoch: 42 average loss: 0.690\n",
      "Test Accuracy : 73.2%, Test Loss: 0.8110083844512701\n",
      "Epoch Time (Training + Test) = 9.15 seconds\n",
      "epoch: 43 average loss: 0.673\n",
      "Test Accuracy : 74.5%, Test Loss: 0.8091222457587719\n",
      "Epoch Time (Training + Test) = 9.21 seconds\n",
      "epoch: 44 average loss: 0.674\n",
      "Test Accuracy : 74.6%, Test Loss: 0.8018057476729155\n",
      "Epoch Time (Training + Test) = 9.40 seconds\n",
      "epoch: 45 average loss: 0.683\n",
      "Test Accuracy : 74.6%, Test Loss: 0.7856513867154717\n",
      "Epoch Time (Training + Test) = 9.39 seconds\n",
      "epoch: 46 average loss: 0.673\n",
      "Test Accuracy : 75.4%, Test Loss: 0.7889853315427899\n",
      "Epoch Time (Training + Test) = 9.24 seconds\n",
      "epoch: 47 average loss: 0.665\n",
      "Test Accuracy : 75.2%, Test Loss: 0.8080012034624815\n",
      "Epoch Time (Training + Test) = 9.27 seconds\n",
      "epoch: 48 average loss: 0.671\n",
      "Test Accuracy : 74.8%, Test Loss: 0.7913897559046745\n",
      "Epoch Time (Training + Test) = 9.28 seconds\n",
      "epoch: 49 average loss: 0.669\n",
      "Test Accuracy : 74.7%, Test Loss: 0.7954673264175653\n",
      "Epoch Time (Training + Test) = 9.15 seconds\n",
      "epoch: 50 average loss: 0.655\n",
      "Test Accuracy : 73.6%, Test Loss: 0.8289330564439297\n",
      "Epoch Time (Training + Test) = 9.16 seconds\n",
      "epoch: 51 average loss: 0.656\n",
      "Test Accuracy : 74.8%, Test Loss: 0.7868761196732521\n",
      "Epoch Time (Training + Test) = 9.22 seconds\n",
      "epoch: 52 average loss: 0.656\n",
      "Test Accuracy : 73.7%, Test Loss: 0.8042429201304913\n",
      "Epoch Time (Training + Test) = 9.21 seconds\n",
      "epoch: 53 average loss: 0.653\n",
      "Test Accuracy : 74.3%, Test Loss: 0.8056085705757141\n",
      "Epoch Time (Training + Test) = 9.23 seconds\n",
      "epoch: 54 average loss: 0.652\n",
      "Test Accuracy : 75.0%, Test Loss: 0.8000294156372547\n",
      "Epoch Time (Training + Test) = 9.17 seconds\n",
      "epoch: 55 average loss: 0.637\n",
      "Test Accuracy : 75.1%, Test Loss: 0.7880314588546753\n",
      "Epoch Time (Training + Test) = 9.24 seconds\n",
      "epoch: 56 average loss: 0.641\n",
      "Test Accuracy : 74.4%, Test Loss: 0.8166675344109535\n",
      "Epoch Time (Training + Test) = 9.22 seconds\n",
      "epoch: 57 average loss: 0.642\n",
      "Test Accuracy : 75.2%, Test Loss: 0.8117209356278181\n",
      "Epoch Time (Training + Test) = 9.11 seconds\n",
      "epoch: 58 average loss: 0.642\n",
      "Test Accuracy : 75.4%, Test Loss: 0.8143520373851061\n",
      "Epoch Time (Training + Test) = 9.22 seconds\n",
      "epoch: 59 average loss: 0.646\n",
      "Test Accuracy : 75.0%, Test Loss: 0.8160306811332703\n",
      "Epoch Time (Training + Test) = 9.11 seconds\n",
      "epoch: 60 average loss: 0.631\n",
      "Test Accuracy : 74.0%, Test Loss: 0.7999834064394236\n",
      "Epoch Time (Training + Test) = 9.21 seconds\n",
      "epoch: 61 average loss: 0.643\n",
      "Test Accuracy : 75.8%, Test Loss: 0.791266392916441\n",
      "Epoch Time (Training + Test) = 9.11 seconds\n",
      "epoch: 62 average loss: 0.635\n",
      "Test Accuracy : 74.7%, Test Loss: 0.8271609144285321\n",
      "Epoch Time (Training + Test) = 9.20 seconds\n",
      "epoch: 63 average loss: 0.645\n",
      "Test Accuracy : 74.9%, Test Loss: 0.8156607281416655\n",
      "Epoch Time (Training + Test) = 9.17 seconds\n",
      "epoch: 64 average loss: 0.641\n",
      "Test Accuracy : 74.5%, Test Loss: 0.8340856907889247\n",
      "Epoch Time (Training + Test) = 9.17 seconds\n",
      "epoch: 65 average loss: 0.624\n",
      "Test Accuracy : 75.5%, Test Loss: 0.8065840024501085\n",
      "Epoch Time (Training + Test) = 9.26 seconds\n",
      "epoch: 66 average loss: 0.628\n",
      "Test Accuracy : 73.8%, Test Loss: 0.8146305754780769\n",
      "Epoch Time (Training + Test) = 9.19 seconds\n",
      "epoch: 67 average loss: 0.623\n",
      "Test Accuracy : 75.0%, Test Loss: 0.7733851158991456\n",
      "Epoch Time (Training + Test) = 9.18 seconds\n",
      "epoch: 68 average loss: 0.613\n",
      "Test Accuracy : 74.6%, Test Loss: 0.8079132176935673\n",
      "Epoch Time (Training + Test) = 9.23 seconds\n",
      "epoch: 69 average loss: 0.621\n",
      "Test Accuracy : 73.8%, Test Loss: 0.8219004590064287\n",
      "Epoch Time (Training + Test) = 9.22 seconds\n",
      "epoch: 70 average loss: 0.625\n",
      "Test Accuracy : 74.9%, Test Loss: 0.8069779714569449\n",
      "Epoch Time (Training + Test) = 9.25 seconds\n",
      "epoch: 71 average loss: 0.614\n",
      "Test Accuracy : 75.0%, Test Loss: 0.8012042474001646\n",
      "Epoch Time (Training + Test) = 9.26 seconds\n",
      "epoch: 72 average loss: 0.608\n",
      "Test Accuracy : 74.7%, Test Loss: 0.8023228757083416\n",
      "Epoch Time (Training + Test) = 9.20 seconds\n",
      "epoch: 73 average loss: 0.623\n",
      "Test Accuracy : 74.2%, Test Loss: 0.8064426276832819\n",
      "Epoch Time (Training + Test) = 9.14 seconds\n",
      "epoch: 74 average loss: 0.617\n",
      "Test Accuracy : 74.8%, Test Loss: 0.8081997893750668\n",
      "Epoch Time (Training + Test) = 9.16 seconds\n",
      "epoch: 75 average loss: 0.621\n",
      "Test Accuracy : 75.0%, Test Loss: 0.8006384670734406\n",
      "Epoch Time (Training + Test) = 9.18 seconds\n",
      "Data Saved to 3_adapter_groups_1.csv\n",
      "Finished Training: \n",
      "Total Time 0.384062 hours\n",
      " Average Time Per Epoch 18.43 seconds\n",
      "No lr scheduler\n",
      "Optimizer Set to New: {'state': {}, 'param_groups': [{'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': False, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69]}]}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:vl9go1rf) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Current Best Acc</td><td>▁▇██████████████████████████████████████</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>epoch time (s)</td><td>▁▇▇█▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇█▇▇▇▇▇▇▇▇▇▇▇█▇▇</td></tr><tr><td>lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_accuracy</td><td>▁▇██▇███████████████████████████████████</td></tr><tr><td>test_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>training_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Best Accuracy</td><td>0.7615</td></tr><tr><td>Current Best Acc</td><td>0.7615</td></tr><tr><td>Total Time (hours)</td><td>0.38406</td></tr><tr><td>epoch</td><td>75</td></tr><tr><td>epoch time (s)</td><td>9.18146</td></tr><tr><td>lr</td><td>0.001</td></tr><tr><td>test_accuracy</td><td>0.75</td></tr><tr><td>test_loss</td><td>0.80064</td></tr><tr><td>training_loss</td><td>0.62101</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">smart-resonance-1</strong>: <a href=\"https://wandb.ai/johnny_suu/FINAL_CIFAR100_SERIAL_ALL3/runs/vl9go1rf\" target=\"_blank\">https://wandb.ai/johnny_suu/FINAL_CIFAR100_SERIAL_ALL3/runs/vl9go1rf</a><br/>Synced 5 W&B file(s), 1 media file(s), 1 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230510_125714-vl9go1rf\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:vl9go1rf). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f23d6d423de47b1b44324765de22330",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01691666666107873, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\John Su\\Downloads\\SydneyUni\\2022\\thesis\\Thesis\\wandb\\run-20230510_130904-1tw5uaz2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/johnny_suu/FINAL_CIFAR100_SERIAL_ALL3/runs/1tw5uaz2\" target=\"_blank\">fresh-bush-2</a></strong> to <a href=\"https://wandb.ai/johnny_suu/FINAL_CIFAR100_SERIAL_ALL3\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "Project Name: FINAL_CIFAR100_SERIAL_ALL3, Run Name 3_adapter_groups_2 \n",
      "\n",
      "\n",
      "Run Start : 2023-05-10 13-09-04\n",
      "start_epoch : 0\n",
      "initial_lr : 0.001\n",
      "batch_size : 64\n",
      "epochs : 75\n",
      "epoch_chkpts : []\n",
      "device : cuda:0\n",
      "criterion : CrossEntropyLoss()\n",
      "optimizer : Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: False\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "seed : 123\n",
      "model_architecture : <class 'NN_Thesis.nn_classes.BNN_Resnet_UniAdapt'>\n",
      "binerised_training : True\n",
      "Number of Elements : 4450140\n",
      "Initial accuracy:\n",
      "Test Accuracy : 4.2%, Test Loss: 32.15387351649582\n",
      "best_acc.pth saved!\n",
      "Test Accuracy : 5.2%, Test Loss: 32.55930799245834\n",
      "best_acc.pth saved!\n",
      "epoch: 1 average loss: 1.887\n",
      "Test Accuracy : 62.4%, Test Loss: 1.4604874551296234\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 11.85 seconds\n",
      "epoch: 2 average loss: 1.468\n",
      "Test Accuracy : 65.6%, Test Loss: 1.2774135395884514\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 11.79 seconds\n",
      "epoch: 3 average loss: 1.338\n",
      "Test Accuracy : 68.0%, Test Loss: 1.183182567358017\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 11.76 seconds\n",
      "epoch: 4 average loss: 1.249\n",
      "Test Accuracy : 69.3%, Test Loss: 1.098377238959074\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 11.85 seconds\n",
      "epoch: 5 average loss: 1.176\n",
      "Test Accuracy : 69.8%, Test Loss: 1.0567188821732998\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 11.87 seconds\n",
      "epoch: 6 average loss: 1.129\n",
      "Test Accuracy : 70.8%, Test Loss: 1.006832342594862\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 11.82 seconds\n",
      "epoch: 7 average loss: 1.088\n",
      "Test Accuracy : 69.5%, Test Loss: 1.0094414632767439\n",
      "Epoch Time (Training + Test) = 11.92 seconds\n",
      "epoch: 8 average loss: 1.051\n",
      "Test Accuracy : 72.8%, Test Loss: 0.9658211208879948\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 11.86 seconds\n",
      "epoch: 9 average loss: 1.034\n",
      "Test Accuracy : 70.2%, Test Loss: 0.9807904623448849\n",
      "Epoch Time (Training + Test) = 13.04 seconds\n",
      "epoch: 10 average loss: 1.003\n",
      "Test Accuracy : 72.2%, Test Loss: 0.9324527494609356\n",
      "Epoch Time (Training + Test) = 12.82 seconds\n",
      "epoch: 11 average loss: 0.988\n",
      "Test Accuracy : 70.0%, Test Loss: 0.9318526070564985\n",
      "Epoch Time (Training + Test) = 12.58 seconds\n",
      "epoch: 12 average loss: 0.967\n",
      "Test Accuracy : 71.2%, Test Loss: 0.9380569290369749\n",
      "Epoch Time (Training + Test) = 12.51 seconds\n",
      "epoch: 13 average loss: 0.949\n",
      "Test Accuracy : 71.1%, Test Loss: 0.917991092428565\n",
      "Epoch Time (Training + Test) = 12.50 seconds\n",
      "epoch: 14 average loss: 0.935\n",
      "Test Accuracy : 73.0%, Test Loss: 0.8789892494678497\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 12.34 seconds\n",
      "epoch: 15 average loss: 0.921\n",
      "Test Accuracy : 73.9%, Test Loss: 0.8645431939512491\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 12.45 seconds\n",
      "epoch: 16 average loss: 0.900\n",
      "Test Accuracy : 71.5%, Test Loss: 0.8998487908393145\n",
      "Epoch Time (Training + Test) = 12.42 seconds\n",
      "epoch: 17 average loss: 0.890\n",
      "Test Accuracy : 73.0%, Test Loss: 0.8522033244371414\n",
      "Epoch Time (Training + Test) = 12.43 seconds\n",
      "epoch: 18 average loss: 0.896\n",
      "Test Accuracy : 73.4%, Test Loss: 0.8726461306214333\n",
      "Epoch Time (Training + Test) = 12.33 seconds\n",
      "epoch: 19 average loss: 0.881\n",
      "Test Accuracy : 72.5%, Test Loss: 0.8367037959396839\n",
      "Epoch Time (Training + Test) = 12.56 seconds\n",
      "epoch: 20 average loss: 0.874\n",
      "Test Accuracy : 74.8%, Test Loss: 0.8397356681525707\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 12.57 seconds\n",
      "epoch: 21 average loss: 0.863\n",
      "Test Accuracy : 73.0%, Test Loss: 0.8395856563001871\n",
      "Epoch Time (Training + Test) = 12.44 seconds\n",
      "epoch: 22 average loss: 0.855\n",
      "Test Accuracy : 72.5%, Test Loss: 0.8548979833722115\n",
      "Epoch Time (Training + Test) = 12.47 seconds\n",
      "epoch: 23 average loss: 0.836\n",
      "Test Accuracy : 74.6%, Test Loss: 0.8184927990660071\n",
      "Epoch Time (Training + Test) = 12.45 seconds\n",
      "epoch: 24 average loss: 0.839\n",
      "Test Accuracy : 74.4%, Test Loss: 0.836842255666852\n",
      "Epoch Time (Training + Test) = 12.46 seconds\n",
      "epoch: 25 average loss: 0.822\n",
      "Test Accuracy : 72.8%, Test Loss: 0.8510441277176142\n",
      "Epoch Time (Training + Test) = 12.44 seconds\n",
      "epoch: 26 average loss: 0.817\n",
      "Test Accuracy : 72.8%, Test Loss: 0.8478485159575939\n",
      "Epoch Time (Training + Test) = 12.36 seconds\n",
      "epoch: 27 average loss: 0.820\n",
      "Test Accuracy : 74.4%, Test Loss: 0.8305450603365898\n",
      "Epoch Time (Training + Test) = 12.49 seconds\n",
      "epoch: 28 average loss: 0.828\n",
      "Test Accuracy : 72.2%, Test Loss: 0.8825970850884914\n",
      "Epoch Time (Training + Test) = 12.64 seconds\n",
      "epoch: 29 average loss: 0.809\n",
      "Test Accuracy : 73.8%, Test Loss: 0.8454758897423744\n",
      "Epoch Time (Training + Test) = 12.44 seconds\n",
      "epoch: 30 average loss: 0.812\n",
      "Test Accuracy : 73.7%, Test Loss: 0.8342080041766167\n",
      "Epoch Time (Training + Test) = 12.58 seconds\n",
      "epoch: 31 average loss: 0.806\n",
      "Test Accuracy : 74.2%, Test Loss: 0.8367127925157547\n",
      "Epoch Time (Training + Test) = 12.37 seconds\n",
      "epoch: 32 average loss: 0.788\n",
      "Test Accuracy : 74.1%, Test Loss: 0.8234619665890932\n",
      "Epoch Time (Training + Test) = 12.44 seconds\n",
      "epoch: 33 average loss: 0.792\n",
      "Test Accuracy : 74.2%, Test Loss: 0.8022425565868616\n",
      "Epoch Time (Training + Test) = 12.34 seconds\n",
      "epoch: 34 average loss: 0.791\n",
      "Test Accuracy : 72.9%, Test Loss: 0.8341670800000429\n",
      "Epoch Time (Training + Test) = 12.35 seconds\n",
      "epoch: 35 average loss: 0.785\n",
      "Test Accuracy : 74.2%, Test Loss: 0.8057032804936171\n",
      "Epoch Time (Training + Test) = 12.35 seconds\n",
      "epoch: 36 average loss: 0.779\n",
      "Test Accuracy : 73.0%, Test Loss: 0.8561184704303741\n",
      "Epoch Time (Training + Test) = 12.40 seconds\n",
      "epoch: 37 average loss: 0.767\n",
      "Test Accuracy : 73.7%, Test Loss: 0.8266751002520323\n",
      "Epoch Time (Training + Test) = 12.34 seconds\n",
      "epoch: 38 average loss: 0.777\n",
      "Test Accuracy : 72.8%, Test Loss: 0.8138429028913379\n",
      "Epoch Time (Training + Test) = 12.41 seconds\n",
      "epoch: 39 average loss: 0.778\n",
      "Test Accuracy : 73.9%, Test Loss: 0.8395346468314528\n",
      "Epoch Time (Training + Test) = 12.34 seconds\n",
      "epoch: 40 average loss: 0.769\n",
      "Test Accuracy : 74.7%, Test Loss: 0.8254714589565992\n",
      "Epoch Time (Training + Test) = 12.35 seconds\n",
      "epoch: 41 average loss: 0.760\n",
      "Test Accuracy : 74.4%, Test Loss: 0.8109158976003528\n",
      "Epoch Time (Training + Test) = 12.33 seconds\n",
      "epoch: 42 average loss: 0.762\n",
      "Test Accuracy : 74.3%, Test Loss: 0.8024409282952547\n",
      "Epoch Time (Training + Test) = 12.39 seconds\n",
      "epoch: 43 average loss: 0.741\n",
      "Test Accuracy : 73.2%, Test Loss: 0.8195529188960791\n",
      "Epoch Time (Training + Test) = 12.29 seconds\n",
      "epoch: 44 average loss: 0.749\n",
      "Test Accuracy : 74.3%, Test Loss: 0.8018650785088539\n",
      "Epoch Time (Training + Test) = 12.36 seconds\n",
      "epoch: 45 average loss: 0.749\n",
      "Test Accuracy : 75.1%, Test Loss: 0.7981945462524891\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 12.43 seconds\n",
      "epoch: 46 average loss: 0.763\n",
      "Test Accuracy : 74.8%, Test Loss: 0.8124417494982481\n",
      "Epoch Time (Training + Test) = 12.39 seconds\n",
      "epoch: 47 average loss: 0.740\n",
      "Test Accuracy : 73.1%, Test Loss: 0.8275526519864798\n",
      "Epoch Time (Training + Test) = 12.37 seconds\n",
      "epoch: 48 average loss: 0.748\n",
      "Test Accuracy : 73.7%, Test Loss: 0.8320452030748129\n",
      "Epoch Time (Training + Test) = 12.37 seconds\n",
      "epoch: 49 average loss: 0.753\n",
      "Test Accuracy : 74.2%, Test Loss: 0.8209805013611913\n",
      "Epoch Time (Training + Test) = 12.29 seconds\n",
      "epoch: 50 average loss: 0.736\n",
      "Test Accuracy : 73.9%, Test Loss: 0.822676531970501\n",
      "Epoch Time (Training + Test) = 12.44 seconds\n",
      "epoch: 51 average loss: 0.742\n",
      "Test Accuracy : 73.7%, Test Loss: 0.8446874208748341\n",
      "Epoch Time (Training + Test) = 12.37 seconds\n",
      "epoch: 52 average loss: 0.741\n",
      "Test Accuracy : 73.7%, Test Loss: 0.8072137087583542\n",
      "Epoch Time (Training + Test) = 12.41 seconds\n",
      "epoch: 53 average loss: 0.726\n",
      "Test Accuracy : 73.5%, Test Loss: 0.8317621443420649\n",
      "Epoch Time (Training + Test) = 12.33 seconds\n",
      "epoch: 54 average loss: 0.731\n",
      "Test Accuracy : 75.2%, Test Loss: 0.8321730103343725\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 12.30 seconds\n",
      "epoch: 55 average loss: 0.732\n",
      "Test Accuracy : 73.0%, Test Loss: 0.8492993302643299\n",
      "Epoch Time (Training + Test) = 12.36 seconds\n",
      "epoch: 56 average loss: 0.723\n",
      "Test Accuracy : 73.3%, Test Loss: 0.8226974830031395\n",
      "Epoch Time (Training + Test) = 12.34 seconds\n",
      "epoch: 57 average loss: 0.721\n",
      "Test Accuracy : 72.9%, Test Loss: 0.8252722602337599\n",
      "Epoch Time (Training + Test) = 12.39 seconds\n",
      "epoch: 58 average loss: 0.729\n",
      "Test Accuracy : 72.5%, Test Loss: 0.8384288027882576\n",
      "Epoch Time (Training + Test) = 12.32 seconds\n",
      "epoch: 59 average loss: 0.735\n",
      "Test Accuracy : 75.1%, Test Loss: 0.792157357558608\n",
      "Epoch Time (Training + Test) = 12.36 seconds\n",
      "epoch: 60 average loss: 0.726\n",
      "Test Accuracy : 73.7%, Test Loss: 0.8019422572106123\n",
      "Epoch Time (Training + Test) = 12.26 seconds\n",
      "epoch: 61 average loss: 0.721\n",
      "Test Accuracy : 74.1%, Test Loss: 0.8121497761458158\n",
      "Epoch Time (Training + Test) = 12.30 seconds\n",
      "epoch: 62 average loss: 0.720\n",
      "Test Accuracy : 74.1%, Test Loss: 0.8101133648306131\n",
      "Epoch Time (Training + Test) = 12.44 seconds\n",
      "epoch: 63 average loss: 0.716\n",
      "Test Accuracy : 74.7%, Test Loss: 0.8171937577426434\n",
      "Epoch Time (Training + Test) = 12.30 seconds\n",
      "epoch: 64 average loss: 0.712\n",
      "Test Accuracy : 74.3%, Test Loss: 0.7974275788292289\n",
      "Epoch Time (Training + Test) = 12.42 seconds\n",
      "epoch: 65 average loss: 0.698\n",
      "Test Accuracy : 74.5%, Test Loss: 0.8191725071519613\n",
      "Epoch Time (Training + Test) = 12.29 seconds\n",
      "epoch: 66 average loss: 0.701\n",
      "Test Accuracy : 75.1%, Test Loss: 0.8072798056527972\n",
      "Epoch Time (Training + Test) = 12.35 seconds\n",
      "epoch: 67 average loss: 0.707\n",
      "Test Accuracy : 73.5%, Test Loss: 0.8238575141876936\n",
      "Epoch Time (Training + Test) = 12.32 seconds\n",
      "epoch: 68 average loss: 0.701\n",
      "Test Accuracy : 73.2%, Test Loss: 0.821371192112565\n",
      "Epoch Time (Training + Test) = 12.29 seconds\n",
      "epoch: 69 average loss: 0.703\n",
      "Test Accuracy : 73.9%, Test Loss: 0.8164351526647806\n",
      "Epoch Time (Training + Test) = 12.30 seconds\n",
      "epoch: 70 average loss: 0.697\n",
      "Test Accuracy : 74.8%, Test Loss: 0.7966577764600515\n",
      "Epoch Time (Training + Test) = 12.45 seconds\n",
      "epoch: 71 average loss: 0.701\n",
      "Test Accuracy : 72.9%, Test Loss: 0.8360292846336961\n",
      "Epoch Time (Training + Test) = 12.30 seconds\n",
      "epoch: 72 average loss: 0.707\n",
      "Test Accuracy : 73.5%, Test Loss: 0.822953388094902\n",
      "Epoch Time (Training + Test) = 12.44 seconds\n",
      "epoch: 73 average loss: 0.705\n",
      "Test Accuracy : 74.0%, Test Loss: 0.8211820777505636\n",
      "Epoch Time (Training + Test) = 12.32 seconds\n",
      "epoch: 74 average loss: 0.697\n",
      "Test Accuracy : 74.4%, Test Loss: 0.8171989526599646\n",
      "Epoch Time (Training + Test) = 12.27 seconds\n",
      "epoch: 75 average loss: 0.686\n",
      "Test Accuracy : 73.9%, Test Loss: 0.8049987656995654\n",
      "Epoch Time (Training + Test) = 12.40 seconds\n",
      "Data Saved to 3_adapter_groups_2.csv\n",
      "Finished Training: \n",
      "Total Time 0.514456 hours\n",
      " Average Time Per Epoch 24.69 seconds\n",
      "No lr scheduler\n",
      "Optimizer Set to New: {'state': {}, 'param_groups': [{'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': False, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69]}]}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1tw5uaz2) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Current Best Acc</td><td>▁▇▇▇████████████████████████████████████</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>epoch time (s)</td><td>▁▇▆▇▇█▇▇▇▇▇▇▇▇▇█▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_accuracy</td><td>▁▇▇▇▇█▇█████████████████████████████████</td></tr><tr><td>test_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>training_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Best Accuracy</td><td>0.752</td></tr><tr><td>Current Best Acc</td><td>0.752</td></tr><tr><td>Total Time (hours)</td><td>0.51446</td></tr><tr><td>epoch</td><td>75</td></tr><tr><td>epoch time (s)</td><td>12.4033</td></tr><tr><td>lr</td><td>0.001</td></tr><tr><td>test_accuracy</td><td>0.739</td></tr><tr><td>test_loss</td><td>0.805</td></tr><tr><td>training_loss</td><td>0.68601</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">fresh-bush-2</strong>: <a href=\"https://wandb.ai/johnny_suu/FINAL_CIFAR100_SERIAL_ALL3/runs/1tw5uaz2\" target=\"_blank\">https://wandb.ai/johnny_suu/FINAL_CIFAR100_SERIAL_ALL3/runs/1tw5uaz2</a><br/>Synced 5 W&B file(s), 1 media file(s), 1 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230510_130904-1tw5uaz2\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1tw5uaz2). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66176ec434a0465999cfa489a661e0f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016933333330477276, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\John Su\\Downloads\\SydneyUni\\2022\\thesis\\Thesis\\wandb\\run-20230510_132450-3idvify9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/johnny_suu/FINAL_CIFAR100_SERIAL_ALL3/runs/3idvify9\" target=\"_blank\">volcanic-flower-3</a></strong> to <a href=\"https://wandb.ai/johnny_suu/FINAL_CIFAR100_SERIAL_ALL3\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "Project Name: FINAL_CIFAR100_SERIAL_ALL3, Run Name 3_adapter_groups_5 \n",
      "\n",
      "\n",
      "Run Start : 2023-05-10 13-24-50\n",
      "start_epoch : 0\n",
      "initial_lr : 0.001\n",
      "batch_size : 64\n",
      "epochs : 75\n",
      "epoch_chkpts : []\n",
      "device : cuda:0\n",
      "criterion : CrossEntropyLoss()\n",
      "optimizer : Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: False\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "seed : 123\n",
      "model_architecture : <class 'NN_Thesis.nn_classes.BNN_Resnet_UniAdapt'>\n",
      "binerised_training : True\n",
      "Number of Elements : 4409820\n",
      "Initial accuracy:\n",
      "Test Accuracy : 5.1%, Test Loss: 39.75445556640625\n",
      "best_acc.pth saved!\n",
      "Test Accuracy : 5.5%, Test Loss: 38.93213331699371\n",
      "best_acc.pth saved!\n",
      "epoch: 1 average loss: 2.033\n",
      "Test Accuracy : 57.8%, Test Loss: 1.5792807824909687\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.44 seconds\n",
      "epoch: 2 average loss: 1.570\n",
      "Test Accuracy : 61.7%, Test Loss: 1.409106869250536\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.48 seconds\n",
      "epoch: 3 average loss: 1.447\n",
      "Test Accuracy : 63.9%, Test Loss: 1.2798614837229252\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.43 seconds\n",
      "epoch: 4 average loss: 1.360\n",
      "Test Accuracy : 66.3%, Test Loss: 1.2230263650417328\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.45 seconds\n",
      "epoch: 5 average loss: 1.284\n",
      "Test Accuracy : 67.8%, Test Loss: 1.1571493335068226\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.48 seconds\n",
      "epoch: 6 average loss: 1.238\n",
      "Test Accuracy : 68.7%, Test Loss: 1.121369706466794\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.46 seconds\n",
      "epoch: 7 average loss: 1.192\n",
      "Test Accuracy : 68.6%, Test Loss: 1.07611552067101\n",
      "Epoch Time (Training + Test) = 9.41 seconds\n",
      "epoch: 8 average loss: 1.160\n",
      "Test Accuracy : 69.9%, Test Loss: 1.037566738203168\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.55 seconds\n",
      "epoch: 9 average loss: 1.136\n",
      "Test Accuracy : 69.6%, Test Loss: 1.049887377768755\n",
      "Epoch Time (Training + Test) = 9.38 seconds\n",
      "epoch: 10 average loss: 1.105\n",
      "Test Accuracy : 69.9%, Test Loss: 1.0173045825213194\n",
      "Epoch Time (Training + Test) = 9.48 seconds\n",
      "epoch: 11 average loss: 1.087\n",
      "Test Accuracy : 70.1%, Test Loss: 1.0007362253963947\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.55 seconds\n",
      "epoch: 12 average loss: 1.074\n",
      "Test Accuracy : 70.0%, Test Loss: 0.9966202862560749\n",
      "Epoch Time (Training + Test) = 9.38 seconds\n",
      "epoch: 13 average loss: 1.052\n",
      "Test Accuracy : 70.8%, Test Loss: 0.9877399671822786\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.46 seconds\n",
      "epoch: 14 average loss: 1.031\n",
      "Test Accuracy : 71.4%, Test Loss: 0.9651947617530823\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.45 seconds\n",
      "epoch: 15 average loss: 1.020\n",
      "Test Accuracy : 70.0%, Test Loss: 0.9616559222340584\n",
      "Epoch Time (Training + Test) = 9.43 seconds\n",
      "epoch: 16 average loss: 1.002\n",
      "Test Accuracy : 71.8%, Test Loss: 0.9294502530246973\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.56 seconds\n",
      "epoch: 17 average loss: 0.995\n",
      "Test Accuracy : 70.8%, Test Loss: 0.9188766125589609\n",
      "Epoch Time (Training + Test) = 9.39 seconds\n",
      "epoch: 18 average loss: 0.981\n",
      "Test Accuracy : 70.9%, Test Loss: 0.9303540680557489\n",
      "Epoch Time (Training + Test) = 9.42 seconds\n",
      "epoch: 19 average loss: 0.984\n",
      "Test Accuracy : 72.1%, Test Loss: 0.9086187686771154\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.43 seconds\n",
      "epoch: 20 average loss: 0.960\n",
      "Test Accuracy : 72.2%, Test Loss: 0.9013603087514639\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.49 seconds\n",
      "epoch: 21 average loss: 0.967\n",
      "Test Accuracy : 71.0%, Test Loss: 0.9205681048333645\n",
      "Epoch Time (Training + Test) = 9.43 seconds\n",
      "epoch: 22 average loss: 0.950\n",
      "Test Accuracy : 71.2%, Test Loss: 0.9287703037261963\n",
      "Epoch Time (Training + Test) = 9.47 seconds\n",
      "epoch: 23 average loss: 0.941\n",
      "Test Accuracy : 72.5%, Test Loss: 0.8879148904234171\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.48 seconds\n",
      "epoch: 24 average loss: 0.946\n",
      "Test Accuracy : 70.7%, Test Loss: 0.8913307525217533\n",
      "Epoch Time (Training + Test) = 9.45 seconds\n",
      "epoch: 25 average loss: 0.939\n",
      "Test Accuracy : 72.0%, Test Loss: 0.8880058936774731\n",
      "Epoch Time (Training + Test) = 9.44 seconds\n",
      "epoch: 26 average loss: 0.932\n",
      "Test Accuracy : 71.5%, Test Loss: 0.9052019529044628\n",
      "Epoch Time (Training + Test) = 9.43 seconds\n",
      "epoch: 27 average loss: 0.919\n",
      "Test Accuracy : 70.5%, Test Loss: 0.9124039299786091\n",
      "Epoch Time (Training + Test) = 9.44 seconds\n",
      "epoch: 28 average loss: 0.933\n",
      "Test Accuracy : 71.5%, Test Loss: 0.9135445468127728\n",
      "Epoch Time (Training + Test) = 9.36 seconds\n",
      "epoch: 29 average loss: 0.919\n",
      "Test Accuracy : 71.2%, Test Loss: 0.9143273457884789\n",
      "Epoch Time (Training + Test) = 9.41 seconds\n",
      "epoch: 30 average loss: 0.915\n",
      "Test Accuracy : 72.9%, Test Loss: 0.8890169803053141\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.43 seconds\n",
      "epoch: 31 average loss: 0.902\n",
      "Test Accuracy : 72.0%, Test Loss: 0.8766795266419649\n",
      "Epoch Time (Training + Test) = 9.44 seconds\n",
      "epoch: 32 average loss: 0.900\n",
      "Test Accuracy : 71.7%, Test Loss: 0.888348750770092\n",
      "Epoch Time (Training + Test) = 9.42 seconds\n",
      "epoch: 33 average loss: 0.904\n",
      "Test Accuracy : 73.6%, Test Loss: 0.8774062041193247\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.36 seconds\n",
      "epoch: 34 average loss: 0.900\n",
      "Test Accuracy : 72.1%, Test Loss: 0.8962599132210016\n",
      "Epoch Time (Training + Test) = 9.47 seconds\n",
      "epoch: 35 average loss: 0.901\n",
      "Test Accuracy : 71.7%, Test Loss: 0.8681231755763292\n",
      "Epoch Time (Training + Test) = 9.41 seconds\n",
      "epoch: 36 average loss: 0.881\n",
      "Test Accuracy : 70.7%, Test Loss: 0.890776339918375\n",
      "Epoch Time (Training + Test) = 9.45 seconds\n",
      "epoch: 37 average loss: 0.881\n",
      "Test Accuracy : 71.5%, Test Loss: 0.8770501092076302\n",
      "Epoch Time (Training + Test) = 9.39 seconds\n",
      "epoch: 38 average loss: 0.886\n",
      "Test Accuracy : 71.9%, Test Loss: 0.8717938866466284\n",
      "Epoch Time (Training + Test) = 9.38 seconds\n",
      "epoch: 39 average loss: 0.887\n",
      "Test Accuracy : 71.0%, Test Loss: 0.8928319271653891\n",
      "Epoch Time (Training + Test) = 9.37 seconds\n",
      "epoch: 40 average loss: 0.882\n",
      "Test Accuracy : 73.0%, Test Loss: 0.8719380851835012\n",
      "Epoch Time (Training + Test) = 9.31 seconds\n",
      "epoch: 41 average loss: 0.859\n",
      "Test Accuracy : 72.0%, Test Loss: 0.8658946491777897\n",
      "Epoch Time (Training + Test) = 9.43 seconds\n",
      "epoch: 42 average loss: 0.876\n",
      "Test Accuracy : 71.5%, Test Loss: 0.8706493154168129\n",
      "Epoch Time (Training + Test) = 9.41 seconds\n",
      "epoch: 43 average loss: 0.857\n",
      "Test Accuracy : 73.0%, Test Loss: 0.8520570769906044\n",
      "Epoch Time (Training + Test) = 9.34 seconds\n",
      "epoch: 44 average loss: 0.862\n",
      "Test Accuracy : 73.4%, Test Loss: 0.8567667733877897\n",
      "Epoch Time (Training + Test) = 9.43 seconds\n",
      "epoch: 45 average loss: 0.882\n",
      "Test Accuracy : 73.4%, Test Loss: 0.8434285297989845\n",
      "Epoch Time (Training + Test) = 9.43 seconds\n",
      "epoch: 46 average loss: 0.858\n",
      "Test Accuracy : 73.4%, Test Loss: 0.8485471289604902\n",
      "Epoch Time (Training + Test) = 9.42 seconds\n",
      "epoch: 47 average loss: 0.856\n",
      "Test Accuracy : 72.3%, Test Loss: 0.8688977863639593\n",
      "Epoch Time (Training + Test) = 9.39 seconds\n",
      "epoch: 48 average loss: 0.857\n",
      "Test Accuracy : 73.0%, Test Loss: 0.869572477415204\n",
      "Epoch Time (Training + Test) = 9.39 seconds\n",
      "epoch: 49 average loss: 0.862\n",
      "Test Accuracy : 72.8%, Test Loss: 0.8563358411192894\n",
      "Epoch Time (Training + Test) = 9.40 seconds\n",
      "epoch: 50 average loss: 0.849\n",
      "Test Accuracy : 72.0%, Test Loss: 0.8670506365597248\n",
      "Epoch Time (Training + Test) = 9.38 seconds\n",
      "epoch: 51 average loss: 0.860\n",
      "Test Accuracy : 71.8%, Test Loss: 0.8675869330763817\n",
      "Epoch Time (Training + Test) = 9.39 seconds\n",
      "epoch: 52 average loss: 0.853\n",
      "Test Accuracy : 72.5%, Test Loss: 0.863487146794796\n",
      "Epoch Time (Training + Test) = 9.39 seconds\n",
      "epoch: 53 average loss: 0.844\n",
      "Test Accuracy : 72.8%, Test Loss: 0.8576812874525785\n",
      "Epoch Time (Training + Test) = 9.37 seconds\n",
      "epoch: 54 average loss: 0.850\n",
      "Test Accuracy : 71.9%, Test Loss: 0.8615871872752905\n",
      "Epoch Time (Training + Test) = 9.40 seconds\n",
      "epoch: 55 average loss: 0.851\n",
      "Test Accuracy : 72.1%, Test Loss: 0.8742810152471066\n",
      "Epoch Time (Training + Test) = 9.36 seconds\n",
      "epoch: 56 average loss: 0.845\n",
      "Test Accuracy : 74.0%, Test Loss: 0.8598447702825069\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.61 seconds\n",
      "epoch: 57 average loss: 0.837\n",
      "Test Accuracy : 73.0%, Test Loss: 0.84223248437047\n",
      "Epoch Time (Training + Test) = 9.18 seconds\n",
      "epoch: 58 average loss: 0.857\n",
      "Test Accuracy : 71.5%, Test Loss: 0.879004443064332\n",
      "Epoch Time (Training + Test) = 9.05 seconds\n",
      "epoch: 59 average loss: 0.864\n",
      "Test Accuracy : 72.2%, Test Loss: 0.8531355541199446\n",
      "Epoch Time (Training + Test) = 9.37 seconds\n",
      "epoch: 60 average loss: 0.844\n",
      "Test Accuracy : 73.0%, Test Loss: 0.8534079138189554\n",
      "Epoch Time (Training + Test) = 9.04 seconds\n",
      "epoch: 61 average loss: 0.841\n",
      "Test Accuracy : 71.0%, Test Loss: 0.8807102013379335\n",
      "Epoch Time (Training + Test) = 9.06 seconds\n",
      "epoch: 62 average loss: 0.845\n",
      "Test Accuracy : 72.2%, Test Loss: 0.8728942535817623\n",
      "Epoch Time (Training + Test) = 9.15 seconds\n",
      "epoch: 63 average loss: 0.848\n",
      "Test Accuracy : 73.5%, Test Loss: 0.8342327419668436\n",
      "Epoch Time (Training + Test) = 9.07 seconds\n",
      "epoch: 64 average loss: 0.836\n",
      "Test Accuracy : 72.6%, Test Loss: 0.8770872093737125\n",
      "Epoch Time (Training + Test) = 9.06 seconds\n",
      "epoch: 65 average loss: 0.841\n",
      "Test Accuracy : 72.1%, Test Loss: 0.8763292450457811\n",
      "Epoch Time (Training + Test) = 9.06 seconds\n",
      "epoch: 66 average loss: 0.821\n",
      "Test Accuracy : 72.1%, Test Loss: 0.8790132962167263\n",
      "Epoch Time (Training + Test) = 9.09 seconds\n",
      "epoch: 67 average loss: 0.829\n",
      "Test Accuracy : 72.6%, Test Loss: 0.8669561166316271\n",
      "Epoch Time (Training + Test) = 9.02 seconds\n",
      "epoch: 68 average loss: 0.825\n",
      "Test Accuracy : 72.6%, Test Loss: 0.8553952090442181\n",
      "Epoch Time (Training + Test) = 9.03 seconds\n",
      "epoch: 69 average loss: 0.831\n",
      "Test Accuracy : 71.5%, Test Loss: 0.8613302744925022\n",
      "Epoch Time (Training + Test) = 9.05 seconds\n",
      "epoch: 70 average loss: 0.833\n",
      "Test Accuracy : 72.8%, Test Loss: 0.8670961149036884\n",
      "Epoch Time (Training + Test) = 9.07 seconds\n",
      "epoch: 71 average loss: 0.826\n",
      "Test Accuracy : 71.2%, Test Loss: 0.8973162043839693\n",
      "Epoch Time (Training + Test) = 9.06 seconds\n",
      "epoch: 72 average loss: 0.826\n",
      "Test Accuracy : 72.5%, Test Loss: 0.847454646602273\n",
      "Epoch Time (Training + Test) = 9.06 seconds\n",
      "epoch: 73 average loss: 0.837\n",
      "Test Accuracy : 73.6%, Test Loss: 0.8599179368466139\n",
      "Epoch Time (Training + Test) = 9.03 seconds\n",
      "epoch: 74 average loss: 0.832\n",
      "Test Accuracy : 71.5%, Test Loss: 0.8716216273605824\n",
      "Epoch Time (Training + Test) = 9.08 seconds\n",
      "epoch: 75 average loss: 0.828\n",
      "Test Accuracy : 73.5%, Test Loss: 0.8424720857292414\n",
      "Epoch Time (Training + Test) = 9.05 seconds\n",
      "Data Saved to 3_adapter_groups_5.csv\n",
      "Finished Training: \n",
      "Total Time 0.389205 hours\n",
      " Average Time Per Epoch 18.68 seconds\n",
      "No lr scheduler\n",
      "Optimizer Set to New: {'state': {}, 'param_groups': [{'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': False, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69]}]}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3idvify9) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Current Best Acc</td><td>▁▆▇▇▇███████████████████████████████████</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>epoch time (s)</td><td>▁███▇▇███▇█████▇████▇▇▇██▇▇▇▇▇▇▇▆▆▆▆▆▆▆▆</td></tr><tr><td>lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_accuracy</td><td>▁▆▇▇▇███████████████████████████████████</td></tr><tr><td>test_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>training_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Best Accuracy</td><td>0.74</td></tr><tr><td>Current Best Acc</td><td>0.74</td></tr><tr><td>Total Time (hours)</td><td>0.3892</td></tr><tr><td>epoch</td><td>75</td></tr><tr><td>epoch time (s)</td><td>9.04752</td></tr><tr><td>lr</td><td>0.001</td></tr><tr><td>test_accuracy</td><td>0.735</td></tr><tr><td>test_loss</td><td>0.84247</td></tr><tr><td>training_loss</td><td>0.82783</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">volcanic-flower-3</strong>: <a href=\"https://wandb.ai/johnny_suu/FINAL_CIFAR100_SERIAL_ALL3/runs/3idvify9\" target=\"_blank\">https://wandb.ai/johnny_suu/FINAL_CIFAR100_SERIAL_ALL3/runs/3idvify9</a><br/>Synced 5 W&B file(s), 1 media file(s), 1 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230510_132450-3idvify9\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3idvify9). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb4d1e39ebc74e02ab41c87840fb7d4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016933333330477276, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\John Su\\Downloads\\SydneyUni\\2022\\thesis\\Thesis\\wandb\\run-20230510_133649-2kryw3a9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/johnny_suu/FINAL_CIFAR100_SERIAL_ALL3/runs/2kryw3a9\" target=\"_blank\">colorful-shape-4</a></strong> to <a href=\"https://wandb.ai/johnny_suu/FINAL_CIFAR100_SERIAL_ALL3\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "Project Name: FINAL_CIFAR100_SERIAL_ALL3, Run Name 3_adapter_groups_10 \n",
      "\n",
      "\n",
      "Run Start : 2023-05-10 13-36-49\n",
      "start_epoch : 0\n",
      "initial_lr : 0.001\n",
      "batch_size : 64\n",
      "epochs : 75\n",
      "epoch_chkpts : []\n",
      "device : cuda:0\n",
      "criterion : CrossEntropyLoss()\n",
      "optimizer : Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: False\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "seed : 123\n",
      "model_architecture : <class 'NN_Thesis.nn_classes.BNN_Resnet_UniAdapt'>\n",
      "binerised_training : True\n",
      "Number of Elements : 4396380\n",
      "Initial accuracy:\n",
      "Test Accuracy : 4.9%, Test Loss: 31.124293260513596\n",
      "best_acc.pth saved!\n",
      "Test Accuracy : 5.8%, Test Loss: 32.35272818803787\n",
      "best_acc.pth saved!\n",
      "epoch: 1 average loss: 2.100\n",
      "Test Accuracy : 57.0%, Test Loss: 1.6307434141635895\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.28 seconds\n",
      "epoch: 2 average loss: 1.636\n",
      "Test Accuracy : 60.7%, Test Loss: 1.4666546918451786\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.14 seconds\n",
      "epoch: 3 average loss: 1.506\n",
      "Test Accuracy : 61.9%, Test Loss: 1.387592140585184\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.20 seconds\n",
      "epoch: 4 average loss: 1.434\n",
      "Test Accuracy : 63.2%, Test Loss: 1.2950477488338947\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.26 seconds\n",
      "epoch: 5 average loss: 1.360\n",
      "Test Accuracy : 63.6%, Test Loss: 1.249875146895647\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.15 seconds\n",
      "epoch: 6 average loss: 1.312\n",
      "Test Accuracy : 65.1%, Test Loss: 1.194841718301177\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.19 seconds\n",
      "epoch: 7 average loss: 1.268\n",
      "Test Accuracy : 65.2%, Test Loss: 1.1513679847121239\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.23 seconds\n",
      "epoch: 8 average loss: 1.242\n",
      "Test Accuracy : 67.3%, Test Loss: 1.142395282164216\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.16 seconds\n",
      "epoch: 9 average loss: 1.211\n",
      "Test Accuracy : 66.5%, Test Loss: 1.1282651126384735\n",
      "Epoch Time (Training + Test) = 9.21 seconds\n",
      "epoch: 10 average loss: 1.192\n",
      "Test Accuracy : 67.3%, Test Loss: 1.0785214882344007\n",
      "Epoch Time (Training + Test) = 9.23 seconds\n",
      "epoch: 11 average loss: 1.169\n",
      "Test Accuracy : 66.9%, Test Loss: 1.084103837609291\n",
      "Epoch Time (Training + Test) = 9.19 seconds\n",
      "epoch: 12 average loss: 1.147\n",
      "Test Accuracy : 68.3%, Test Loss: 1.0639055948704481\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.19 seconds\n",
      "epoch: 13 average loss: 1.132\n",
      "Test Accuracy : 67.3%, Test Loss: 1.0446869395673275\n",
      "Epoch Time (Training + Test) = 9.21 seconds\n",
      "epoch: 14 average loss: 1.133\n",
      "Test Accuracy : 67.3%, Test Loss: 1.0535378381609917\n",
      "Epoch Time (Training + Test) = 9.17 seconds\n",
      "epoch: 15 average loss: 1.105\n",
      "Test Accuracy : 68.2%, Test Loss: 1.0248717796057463\n",
      "Epoch Time (Training + Test) = 9.15 seconds\n",
      "epoch: 16 average loss: 1.088\n",
      "Test Accuracy : 68.0%, Test Loss: 1.0150566007941961\n",
      "Epoch Time (Training + Test) = 9.20 seconds\n",
      "epoch: 17 average loss: 1.075\n",
      "Test Accuracy : 69.0%, Test Loss: 0.998386861756444\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.15 seconds\n",
      "epoch: 18 average loss: 1.062\n",
      "Test Accuracy : 68.0%, Test Loss: 1.0020774137228727\n",
      "Epoch Time (Training + Test) = 9.22 seconds\n",
      "epoch: 19 average loss: 1.081\n",
      "Test Accuracy : 68.0%, Test Loss: 0.9955623503774405\n",
      "Epoch Time (Training + Test) = 9.07 seconds\n",
      "epoch: 20 average loss: 1.055\n",
      "Test Accuracy : 69.4%, Test Loss: 0.9837863352149725\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.26 seconds\n",
      "epoch: 21 average loss: 1.052\n",
      "Test Accuracy : 69.3%, Test Loss: 0.9671326484531164\n",
      "Epoch Time (Training + Test) = 9.21 seconds\n",
      "epoch: 22 average loss: 1.044\n",
      "Test Accuracy : 69.1%, Test Loss: 0.9914689976722002\n",
      "Epoch Time (Training + Test) = 9.20 seconds\n",
      "epoch: 23 average loss: 1.022\n",
      "Test Accuracy : 68.6%, Test Loss: 0.9703170657157898\n",
      "Epoch Time (Training + Test) = 9.16 seconds\n",
      "epoch: 24 average loss: 1.033\n",
      "Test Accuracy : 67.9%, Test Loss: 0.9742279853671789\n",
      "Epoch Time (Training + Test) = 9.08 seconds\n",
      "epoch: 25 average loss: 1.024\n",
      "Test Accuracy : 70.2%, Test Loss: 0.9557331670075655\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.20 seconds\n",
      "epoch: 26 average loss: 1.007\n",
      "Test Accuracy : 68.4%, Test Loss: 0.9791264273226261\n",
      "Epoch Time (Training + Test) = 9.22 seconds\n",
      "epoch: 27 average loss: 1.014\n",
      "Test Accuracy : 70.2%, Test Loss: 0.9487193711102009\n",
      "Epoch Time (Training + Test) = 9.11 seconds\n",
      "epoch: 28 average loss: 1.013\n",
      "Test Accuracy : 69.5%, Test Loss: 0.9486637357622385\n",
      "Epoch Time (Training + Test) = 9.23 seconds\n",
      "epoch: 29 average loss: 1.002\n",
      "Test Accuracy : 69.0%, Test Loss: 0.9481275323778391\n",
      "Epoch Time (Training + Test) = 9.24 seconds\n",
      "epoch: 30 average loss: 1.005\n",
      "Test Accuracy : 69.5%, Test Loss: 0.9578046128153801\n",
      "Epoch Time (Training + Test) = 9.12 seconds\n",
      "epoch: 31 average loss: 0.992\n",
      "Test Accuracy : 71.1%, Test Loss: 0.9317436087876558\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.26 seconds\n",
      "epoch: 32 average loss: 0.984\n",
      "Test Accuracy : 70.2%, Test Loss: 0.9497357048094273\n",
      "Epoch Time (Training + Test) = 9.21 seconds\n",
      "epoch: 33 average loss: 0.981\n",
      "Test Accuracy : 72.0%, Test Loss: 0.9155999030917883\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.24 seconds\n",
      "epoch: 34 average loss: 1.001\n",
      "Test Accuracy : 70.0%, Test Loss: 0.9486936666071415\n",
      "Epoch Time (Training + Test) = 9.13 seconds\n",
      "epoch: 35 average loss: 0.987\n",
      "Test Accuracy : 69.0%, Test Loss: 0.9443351328372955\n",
      "Epoch Time (Training + Test) = 9.20 seconds\n",
      "epoch: 36 average loss: 0.990\n",
      "Test Accuracy : 69.0%, Test Loss: 0.9543008264154196\n",
      "Epoch Time (Training + Test) = 9.12 seconds\n",
      "epoch: 37 average loss: 0.968\n",
      "Test Accuracy : 69.4%, Test Loss: 0.9462911784648895\n",
      "Epoch Time (Training + Test) = 9.22 seconds\n",
      "epoch: 38 average loss: 0.970\n",
      "Test Accuracy : 69.6%, Test Loss: 0.936469042673707\n",
      "Epoch Time (Training + Test) = 9.07 seconds\n",
      "epoch: 39 average loss: 0.973\n",
      "Test Accuracy : 68.8%, Test Loss: 0.9621321950107813\n",
      "Epoch Time (Training + Test) = 9.18 seconds\n",
      "epoch: 40 average loss: 0.965\n",
      "Test Accuracy : 70.6%, Test Loss: 0.9324683751910925\n",
      "Epoch Time (Training + Test) = 9.18 seconds\n",
      "epoch: 41 average loss: 0.960\n",
      "Test Accuracy : 70.8%, Test Loss: 0.9137340988963842\n",
      "Epoch Time (Training + Test) = 9.24 seconds\n",
      "epoch: 42 average loss: 0.955\n",
      "Test Accuracy : 69.8%, Test Loss: 0.9236670974642038\n",
      "Epoch Time (Training + Test) = 9.22 seconds\n",
      "epoch: 43 average loss: 0.954\n",
      "Test Accuracy : 71.2%, Test Loss: 0.9154963176697493\n",
      "Epoch Time (Training + Test) = 9.24 seconds\n",
      "epoch: 44 average loss: 0.962\n",
      "Test Accuracy : 71.6%, Test Loss: 0.908404029905796\n",
      "Epoch Time (Training + Test) = 9.09 seconds\n",
      "epoch: 45 average loss: 0.960\n",
      "Test Accuracy : 70.6%, Test Loss: 0.918944425880909\n",
      "Epoch Time (Training + Test) = 9.19 seconds\n",
      "epoch: 46 average loss: 0.966\n",
      "Test Accuracy : 71.3%, Test Loss: 0.9218945540487766\n",
      "Epoch Time (Training + Test) = 9.13 seconds\n",
      "epoch: 47 average loss: 0.948\n",
      "Test Accuracy : 71.4%, Test Loss: 0.9025865606963634\n",
      "Epoch Time (Training + Test) = 9.29 seconds\n",
      "epoch: 48 average loss: 0.948\n",
      "Test Accuracy : 70.0%, Test Loss: 0.9331440720707178\n",
      "Epoch Time (Training + Test) = 9.22 seconds\n",
      "epoch: 49 average loss: 0.948\n",
      "Test Accuracy : 69.0%, Test Loss: 0.9456307496875525\n",
      "Epoch Time (Training + Test) = 9.18 seconds\n",
      "epoch: 50 average loss: 0.944\n",
      "Test Accuracy : 70.0%, Test Loss: 0.9356546588242054\n",
      "Epoch Time (Training + Test) = 9.13 seconds\n",
      "epoch: 51 average loss: 0.944\n",
      "Test Accuracy : 71.8%, Test Loss: 0.8966872971504927\n",
      "Epoch Time (Training + Test) = 9.22 seconds\n",
      "epoch: 52 average loss: 0.951\n",
      "Test Accuracy : 70.6%, Test Loss: 0.8972356617450714\n",
      "Epoch Time (Training + Test) = 9.12 seconds\n",
      "epoch: 53 average loss: 0.940\n",
      "Test Accuracy : 70.8%, Test Loss: 0.9129246696829796\n",
      "Epoch Time (Training + Test) = 9.20 seconds\n",
      "epoch: 54 average loss: 0.939\n",
      "Test Accuracy : 70.5%, Test Loss: 0.9207864496856928\n",
      "Epoch Time (Training + Test) = 9.14 seconds\n",
      "epoch: 55 average loss: 0.937\n",
      "Test Accuracy : 71.0%, Test Loss: 0.9146539364010096\n",
      "Epoch Time (Training + Test) = 9.22 seconds\n",
      "epoch: 56 average loss: 0.932\n",
      "Test Accuracy : 71.0%, Test Loss: 0.905171487480402\n",
      "Epoch Time (Training + Test) = 9.28 seconds\n",
      "epoch: 57 average loss: 0.940\n",
      "Test Accuracy : 71.0%, Test Loss: 0.9141580406576395\n",
      "Epoch Time (Training + Test) = 9.17 seconds\n",
      "epoch: 58 average loss: 0.938\n",
      "Test Accuracy : 70.2%, Test Loss: 0.9240529611706734\n",
      "Epoch Time (Training + Test) = 9.15 seconds\n",
      "epoch: 59 average loss: 0.938\n",
      "Test Accuracy : 71.0%, Test Loss: 0.89833396486938\n",
      "Epoch Time (Training + Test) = 9.19 seconds\n",
      "epoch: 60 average loss: 0.929\n",
      "Test Accuracy : 71.8%, Test Loss: 0.8725898917764425\n",
      "Epoch Time (Training + Test) = 9.18 seconds\n",
      "epoch: 61 average loss: 0.935\n",
      "Test Accuracy : 72.2%, Test Loss: 0.9140379931777716\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.21 seconds\n",
      "epoch: 62 average loss: 0.943\n",
      "Test Accuracy : 70.6%, Test Loss: 0.9098086077719927\n",
      "Epoch Time (Training + Test) = 9.16 seconds\n",
      "epoch: 63 average loss: 0.930\n",
      "Test Accuracy : 72.6%, Test Loss: 0.8832520935684443\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.14 seconds\n",
      "epoch: 64 average loss: 0.924\n",
      "Test Accuracy : 70.0%, Test Loss: 0.8978204242885113\n",
      "Epoch Time (Training + Test) = 9.21 seconds\n",
      "epoch: 65 average loss: 0.939\n",
      "Test Accuracy : 71.5%, Test Loss: 0.8947701845318079\n",
      "Epoch Time (Training + Test) = 9.11 seconds\n",
      "epoch: 66 average loss: 0.934\n",
      "Test Accuracy : 71.5%, Test Loss: 0.8810925744473934\n",
      "Epoch Time (Training + Test) = 9.18 seconds\n",
      "epoch: 67 average loss: 0.907\n",
      "Test Accuracy : 70.3%, Test Loss: 0.9129283409565687\n",
      "Epoch Time (Training + Test) = 9.18 seconds\n",
      "epoch: 68 average loss: 0.933\n",
      "Test Accuracy : 70.8%, Test Loss: 0.9022446125745773\n",
      "Epoch Time (Training + Test) = 9.16 seconds\n",
      "epoch: 69 average loss: 0.925\n",
      "Test Accuracy : 70.7%, Test Loss: 0.9217160083353519\n",
      "Epoch Time (Training + Test) = 9.19 seconds\n",
      "epoch: 70 average loss: 0.915\n",
      "Test Accuracy : 72.0%, Test Loss: 0.8932572323828936\n",
      "Epoch Time (Training + Test) = 9.14 seconds\n",
      "epoch: 71 average loss: 0.916\n",
      "Test Accuracy : 70.8%, Test Loss: 0.9190495181828737\n",
      "Epoch Time (Training + Test) = 9.23 seconds\n",
      "epoch: 72 average loss: 0.923\n",
      "Test Accuracy : 70.4%, Test Loss: 0.9292197059839964\n",
      "Epoch Time (Training + Test) = 9.10 seconds\n",
      "epoch: 73 average loss: 0.925\n",
      "Test Accuracy : 70.5%, Test Loss: 0.8920476194471121\n",
      "Epoch Time (Training + Test) = 9.18 seconds\n",
      "epoch: 74 average loss: 0.922\n",
      "Test Accuracy : 71.0%, Test Loss: 0.9257676862180233\n",
      "Epoch Time (Training + Test) = 9.15 seconds\n",
      "epoch: 75 average loss: 0.918\n",
      "Test Accuracy : 71.8%, Test Loss: 0.8802637457847595\n",
      "Epoch Time (Training + Test) = 9.20 seconds\n",
      "Data Saved to 3_adapter_groups_10.csv\n",
      "Finished Training: \n",
      "Total Time 0.382681 hours\n",
      " Average Time Per Epoch 18.37 seconds\n",
      "No lr scheduler\n",
      "Optimizer Set to New: {'state': {}, 'param_groups': [{'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': False, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69]}]}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2kryw3a9) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Current Best Acc</td><td>▁▆▇▇▇▇▇█████████████████████████████████</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>epoch time (s)</td><td>▁██▇████▇▇▇█████▇█▇▇▇██▇▇█▇██████▇▇█████</td></tr><tr><td>lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_accuracy</td><td>▁▆▇▇▇▇▇▇████████████████████████████████</td></tr><tr><td>test_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>training_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Best Accuracy</td><td>0.726</td></tr><tr><td>Current Best Acc</td><td>0.726</td></tr><tr><td>Total Time (hours)</td><td>0.38268</td></tr><tr><td>epoch</td><td>75</td></tr><tr><td>epoch time (s)</td><td>9.20145</td></tr><tr><td>lr</td><td>0.001</td></tr><tr><td>test_accuracy</td><td>0.7175</td></tr><tr><td>test_loss</td><td>0.88026</td></tr><tr><td>training_loss</td><td>0.91844</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">colorful-shape-4</strong>: <a href=\"https://wandb.ai/johnny_suu/FINAL_CIFAR100_SERIAL_ALL3/runs/2kryw3a9\" target=\"_blank\">https://wandb.ai/johnny_suu/FINAL_CIFAR100_SERIAL_ALL3/runs/2kryw3a9</a><br/>Synced 5 W&B file(s), 1 media file(s), 1 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230510_133649-2kryw3a9\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2kryw3a9). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c37ca9a693d8494d82b04cff69c11161",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\John Su\\Downloads\\SydneyUni\\2022\\thesis\\Thesis\\wandb\\run-20230510_134838-1kv78t7q</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/johnny_suu/FINAL_CIFAR100_SERIAL_ALL3/runs/1kv78t7q\" target=\"_blank\">fancy-dust-5</a></strong> to <a href=\"https://wandb.ai/johnny_suu/FINAL_CIFAR100_SERIAL_ALL3\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "Project Name: FINAL_CIFAR100_SERIAL_ALL3, Run Name 3_adapter_groups_20 \n",
      "\n",
      "\n",
      "Run Start : 2023-05-10 13-48-38\n",
      "start_epoch : 0\n",
      "initial_lr : 0.001\n",
      "batch_size : 64\n",
      "epochs : 75\n",
      "epoch_chkpts : []\n",
      "device : cuda:0\n",
      "criterion : CrossEntropyLoss()\n",
      "optimizer : Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: False\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "seed : 123\n",
      "model_architecture : <class 'NN_Thesis.nn_classes.BNN_Resnet_UniAdapt'>\n",
      "binerised_training : True\n",
      "Number of Elements : 4389660\n",
      "Initial accuracy:\n",
      "Test Accuracy : 4.2%, Test Loss: 34.03746459134825\n",
      "best_acc.pth saved!\n",
      "Test Accuracy : 4.2%, Test Loss: 33.77889347076416\n",
      "epoch: 1 average loss: 2.166\n",
      "Test Accuracy : 53.5%, Test Loss: 1.6771018952131271\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.14 seconds\n",
      "epoch: 2 average loss: 1.676\n",
      "Test Accuracy : 59.4%, Test Loss: 1.497622199356556\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.21 seconds\n",
      "epoch: 3 average loss: 1.562\n",
      "Test Accuracy : 60.0%, Test Loss: 1.4246716760098934\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.10 seconds\n",
      "epoch: 4 average loss: 1.489\n",
      "Test Accuracy : 60.6%, Test Loss: 1.3691207729279995\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.11 seconds\n",
      "epoch: 5 average loss: 1.426\n",
      "Test Accuracy : 62.6%, Test Loss: 1.290099948644638\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.20 seconds\n",
      "epoch: 6 average loss: 1.382\n",
      "Test Accuracy : 64.7%, Test Loss: 1.2504736743867397\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.23 seconds\n",
      "epoch: 7 average loss: 1.338\n",
      "Test Accuracy : 63.1%, Test Loss: 1.2349443659186363\n",
      "Epoch Time (Training + Test) = 9.09 seconds\n",
      "epoch: 8 average loss: 1.320\n",
      "Test Accuracy : 64.8%, Test Loss: 1.1999947670847178\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.10 seconds\n",
      "epoch: 9 average loss: 1.286\n",
      "Test Accuracy : 64.0%, Test Loss: 1.2087998501956463\n",
      "Epoch Time (Training + Test) = 9.16 seconds\n",
      "epoch: 10 average loss: 1.262\n",
      "Test Accuracy : 64.3%, Test Loss: 1.141883846372366\n",
      "Epoch Time (Training + Test) = 9.07 seconds\n",
      "epoch: 11 average loss: 1.234\n",
      "Test Accuracy : 66.4%, Test Loss: 1.1333679929375648\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.16 seconds\n",
      "epoch: 12 average loss: 1.230\n",
      "Test Accuracy : 65.5%, Test Loss: 1.1449171155691147\n",
      "Epoch Time (Training + Test) = 9.17 seconds\n",
      "epoch: 13 average loss: 1.202\n",
      "Test Accuracy : 66.5%, Test Loss: 1.1006228271871805\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.20 seconds\n",
      "epoch: 14 average loss: 1.193\n",
      "Test Accuracy : 65.5%, Test Loss: 1.0989456977695227\n",
      "Epoch Time (Training + Test) = 9.11 seconds\n",
      "epoch: 15 average loss: 1.177\n",
      "Test Accuracy : 66.8%, Test Loss: 1.0852398499846458\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.25 seconds\n",
      "epoch: 16 average loss: 1.179\n",
      "Test Accuracy : 66.1%, Test Loss: 1.0903150476515293\n",
      "Epoch Time (Training + Test) = 9.05 seconds\n",
      "epoch: 17 average loss: 1.160\n",
      "Test Accuracy : 68.1%, Test Loss: 1.0557187255471945\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.09 seconds\n",
      "epoch: 18 average loss: 1.154\n",
      "Test Accuracy : 67.5%, Test Loss: 1.0434510614722967\n",
      "Epoch Time (Training + Test) = 9.07 seconds\n",
      "epoch: 19 average loss: 1.153\n",
      "Test Accuracy : 68.0%, Test Loss: 1.038699759170413\n",
      "Epoch Time (Training + Test) = 9.19 seconds\n",
      "epoch: 20 average loss: 1.140\n",
      "Test Accuracy : 68.7%, Test Loss: 1.036064388230443\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.15 seconds\n",
      "epoch: 21 average loss: 1.132\n",
      "Test Accuracy : 67.6%, Test Loss: 1.056468965485692\n",
      "Epoch Time (Training + Test) = 9.16 seconds\n",
      "epoch: 22 average loss: 1.125\n",
      "Test Accuracy : 66.3%, Test Loss: 1.0255091954022646\n",
      "Epoch Time (Training + Test) = 9.08 seconds\n",
      "epoch: 23 average loss: 1.106\n",
      "Test Accuracy : 69.5%, Test Loss: 1.004258882254362\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.25 seconds\n",
      "epoch: 24 average loss: 1.124\n",
      "Test Accuracy : 68.8%, Test Loss: 1.0176329370588064\n",
      "Epoch Time (Training + Test) = 9.06 seconds\n",
      "epoch: 25 average loss: 1.109\n",
      "Test Accuracy : 68.5%, Test Loss: 1.0098310876637697\n",
      "Epoch Time (Training + Test) = 9.05 seconds\n",
      "epoch: 26 average loss: 1.094\n",
      "Test Accuracy : 66.3%, Test Loss: 1.0419570207595825\n",
      "Epoch Time (Training + Test) = 9.17 seconds\n",
      "epoch: 27 average loss: 1.088\n",
      "Test Accuracy : 68.1%, Test Loss: 1.0056875366717577\n",
      "Epoch Time (Training + Test) = 9.16 seconds\n",
      "epoch: 28 average loss: 1.105\n",
      "Test Accuracy : 68.4%, Test Loss: 1.0274436715990305\n",
      "Epoch Time (Training + Test) = 9.07 seconds\n",
      "epoch: 29 average loss: 1.090\n",
      "Test Accuracy : 67.9%, Test Loss: 0.9974166247993708\n",
      "Epoch Time (Training + Test) = 9.20 seconds\n",
      "epoch: 30 average loss: 1.092\n",
      "Test Accuracy : 69.2%, Test Loss: 1.008983876556158\n",
      "Epoch Time (Training + Test) = 9.08 seconds\n",
      "epoch: 31 average loss: 1.079\n",
      "Test Accuracy : 69.2%, Test Loss: 0.971664423123002\n",
      "Epoch Time (Training + Test) = 9.09 seconds\n",
      "epoch: 32 average loss: 1.073\n",
      "Test Accuracy : 68.5%, Test Loss: 0.9803112242370844\n",
      "Epoch Time (Training + Test) = 9.09 seconds\n",
      "epoch: 33 average loss: 1.077\n",
      "Test Accuracy : 70.0%, Test Loss: 0.9842280838638544\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.19 seconds\n",
      "epoch: 34 average loss: 1.074\n",
      "Test Accuracy : 67.5%, Test Loss: 0.9915077202022076\n",
      "Epoch Time (Training + Test) = 9.08 seconds\n",
      "epoch: 35 average loss: 1.065\n",
      "Test Accuracy : 68.4%, Test Loss: 1.0008682012557983\n",
      "Epoch Time (Training + Test) = 9.23 seconds\n",
      "epoch: 36 average loss: 1.073\n",
      "Test Accuracy : 67.7%, Test Loss: 1.0036513470113277\n",
      "Epoch Time (Training + Test) = 9.26 seconds\n",
      "epoch: 37 average loss: 1.058\n",
      "Test Accuracy : 69.3%, Test Loss: 0.9998577460646629\n",
      "Epoch Time (Training + Test) = 9.11 seconds\n",
      "epoch: 38 average loss: 1.073\n",
      "Test Accuracy : 68.5%, Test Loss: 0.9883006457239389\n",
      "Epoch Time (Training + Test) = 9.02 seconds\n",
      "epoch: 39 average loss: 1.069\n",
      "Test Accuracy : 67.3%, Test Loss: 0.9940552152693272\n",
      "Epoch Time (Training + Test) = 9.05 seconds\n",
      "epoch: 40 average loss: 1.059\n",
      "Test Accuracy : 68.0%, Test Loss: 1.0008016247302294\n",
      "Epoch Time (Training + Test) = 9.24 seconds\n",
      "epoch: 41 average loss: 1.044\n",
      "Test Accuracy : 66.6%, Test Loss: 1.0127826780080795\n",
      "Epoch Time (Training + Test) = 9.08 seconds\n",
      "epoch: 42 average loss: 1.058\n",
      "Test Accuracy : 68.2%, Test Loss: 1.0043581798672676\n",
      "Epoch Time (Training + Test) = 9.05 seconds\n",
      "epoch: 43 average loss: 1.030\n",
      "Test Accuracy : 69.5%, Test Loss: 0.9722120463848114\n",
      "Epoch Time (Training + Test) = 9.14 seconds\n",
      "epoch: 44 average loss: 1.042\n",
      "Test Accuracy : 69.0%, Test Loss: 0.9567228257656097\n",
      "Epoch Time (Training + Test) = 9.13 seconds\n",
      "epoch: 45 average loss: 1.044\n",
      "Test Accuracy : 69.5%, Test Loss: 0.9602961000055075\n",
      "Epoch Time (Training + Test) = 9.08 seconds\n",
      "epoch: 46 average loss: 1.051\n",
      "Test Accuracy : 67.3%, Test Loss: 0.9886596389114857\n",
      "Epoch Time (Training + Test) = 9.07 seconds\n",
      "epoch: 47 average loss: 1.034\n",
      "Test Accuracy : 68.0%, Test Loss: 0.9940319899469614\n",
      "Epoch Time (Training + Test) = 9.13 seconds\n",
      "epoch: 48 average loss: 1.036\n",
      "Test Accuracy : 68.8%, Test Loss: 0.9889177922159433\n",
      "Epoch Time (Training + Test) = 9.14 seconds\n",
      "epoch: 49 average loss: 1.049\n",
      "Test Accuracy : 68.5%, Test Loss: 0.993185069411993\n",
      "Epoch Time (Training + Test) = 9.08 seconds\n",
      "epoch: 50 average loss: 1.043\n",
      "Test Accuracy : 68.4%, Test Loss: 0.9713185094296932\n",
      "Epoch Time (Training + Test) = 9.08 seconds\n",
      "epoch: 51 average loss: 1.038\n",
      "Test Accuracy : 68.9%, Test Loss: 0.9892417341470718\n",
      "Epoch Time (Training + Test) = 9.08 seconds\n",
      "epoch: 52 average loss: 1.028\n",
      "Test Accuracy : 67.5%, Test Loss: 0.9663283936679363\n",
      "Epoch Time (Training + Test) = 9.20 seconds\n",
      "epoch: 53 average loss: 1.028\n",
      "Test Accuracy : 67.8%, Test Loss: 0.9666925109922886\n",
      "Epoch Time (Training + Test) = 9.12 seconds\n",
      "epoch: 54 average loss: 1.043\n",
      "Test Accuracy : 68.2%, Test Loss: 0.9803455024957657\n",
      "Epoch Time (Training + Test) = 9.17 seconds\n",
      "epoch: 55 average loss: 1.028\n",
      "Test Accuracy : 69.3%, Test Loss: 0.9594979584217072\n",
      "Epoch Time (Training + Test) = 9.08 seconds\n",
      "epoch: 56 average loss: 1.033\n",
      "Test Accuracy : 69.5%, Test Loss: 0.9736146405339241\n",
      "Epoch Time (Training + Test) = 9.09 seconds\n",
      "epoch: 57 average loss: 1.031\n",
      "Test Accuracy : 68.2%, Test Loss: 0.9844991602003574\n",
      "Epoch Time (Training + Test) = 9.11 seconds\n",
      "epoch: 58 average loss: 1.033\n",
      "Test Accuracy : 68.3%, Test Loss: 1.0215932559221983\n",
      "Epoch Time (Training + Test) = 9.18 seconds\n",
      "epoch: 59 average loss: 1.039\n",
      "Test Accuracy : 67.6%, Test Loss: 1.0048466008156538\n",
      "Epoch Time (Training + Test) = 9.09 seconds\n",
      "epoch: 60 average loss: 1.023\n",
      "Test Accuracy : 68.5%, Test Loss: 0.9785411302000284\n",
      "Epoch Time (Training + Test) = 9.05 seconds\n",
      "epoch: 61 average loss: 1.032\n",
      "Test Accuracy : 66.9%, Test Loss: 0.9726596362888813\n",
      "Epoch Time (Training + Test) = 9.20 seconds\n",
      "epoch: 62 average loss: 1.039\n",
      "Test Accuracy : 67.5%, Test Loss: 0.9720929842442274\n",
      "Epoch Time (Training + Test) = 9.07 seconds\n",
      "epoch: 63 average loss: 1.044\n",
      "Test Accuracy : 68.5%, Test Loss: 0.9706231541931629\n",
      "Epoch Time (Training + Test) = 9.09 seconds\n",
      "epoch: 64 average loss: 1.030\n",
      "Test Accuracy : 68.8%, Test Loss: 0.9814022202044725\n",
      "Epoch Time (Training + Test) = 9.06 seconds\n",
      "epoch: 65 average loss: 1.027\n",
      "Test Accuracy : 67.5%, Test Loss: 0.9921061396598816\n",
      "Epoch Time (Training + Test) = 9.04 seconds\n",
      "epoch: 66 average loss: 1.030\n",
      "Test Accuracy : 69.0%, Test Loss: 0.9702596683055162\n",
      "Epoch Time (Training + Test) = 9.06 seconds\n",
      "epoch: 67 average loss: 1.010\n",
      "Test Accuracy : 69.5%, Test Loss: 0.9516119658946991\n",
      "Epoch Time (Training + Test) = 9.13 seconds\n",
      "epoch: 68 average loss: 1.030\n",
      "Test Accuracy : 69.0%, Test Loss: 0.9551674854010344\n",
      "Epoch Time (Training + Test) = 9.16 seconds\n",
      "epoch: 69 average loss: 1.019\n",
      "Test Accuracy : 68.0%, Test Loss: 0.9776958450675011\n",
      "Epoch Time (Training + Test) = 9.08 seconds\n",
      "epoch: 70 average loss: 1.010\n",
      "Test Accuracy : 67.2%, Test Loss: 0.9767966493964195\n",
      "Epoch Time (Training + Test) = 9.07 seconds\n",
      "epoch: 71 average loss: 1.016\n",
      "Test Accuracy : 68.8%, Test Loss: 0.9694664943963289\n",
      "Epoch Time (Training + Test) = 9.08 seconds\n",
      "epoch: 72 average loss: 1.017\n",
      "Test Accuracy : 69.7%, Test Loss: 0.9345942698419094\n",
      "Epoch Time (Training + Test) = 9.17 seconds\n",
      "epoch: 73 average loss: 1.022\n",
      "Test Accuracy : 68.5%, Test Loss: 0.970696222037077\n",
      "Epoch Time (Training + Test) = 9.09 seconds\n",
      "epoch: 74 average loss: 1.018\n",
      "Test Accuracy : 69.8%, Test Loss: 0.9519494101405144\n",
      "Epoch Time (Training + Test) = 9.07 seconds\n",
      "epoch: 75 average loss: 1.011\n",
      "Test Accuracy : 69.5%, Test Loss: 0.962366845458746\n",
      "Epoch Time (Training + Test) = 9.05 seconds\n",
      "Data Saved to 3_adapter_groups_20.csv\n",
      "Finished Training: \n",
      "Total Time 0.380040 hours\n",
      " Average Time Per Epoch 18.24 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for group in [1,2,5,10,20]:\n",
    "\n",
    "    conv_bn = conv_channel_adapter3(80,groups = group,kernel = 1, padding= 0,nonlinearity='relu')\n",
    "    conv_bn2 = conv_channel_adapter3(160,groups = group,kernel = 1, padding= 0,nonlinearity='relu')\n",
    "    conv_bn3 = conv_channel_adapter3(320,groups = group,kernel = 1, padding= 0,nonlinearity='relu')\n",
    "    # adapt_fine2.add_adapter(after = 'bn2',adapter = bottleneck)\n",
    "    \n",
    "    # conv_bn = conv_channel_adapter3(input_channel,kernel = 1, padding= 0,groups= input_channel//10)\n",
    "    adapt_fine2 = BNN_Resnet_UniAdapt(num_classes=80)\n",
    "    \n",
    "    adapt_fine2.load_state_dict(state,strict=False)\n",
    "    \n",
    "    # adapt_fine2.head = nn.Linear(320+160,80)\n",
    "    # adapt_fine2.head_bn = nn.BatchNorm1d(480)\n",
    "    adapt_fine2.freeze()\n",
    "    # adapt_fine2.add_adapter(after = 'bn2',adapter = bottleneck)\n",
    "    adapt_fine2.add_adapter(after = 'layer1',adapter = conv_bn)\n",
    "    adapt_fine2.add_adapter(after = 'layer2',adapter = conv_bn2)\n",
    "    adapt_fine2.add_adapter(after = 'layer3',adapter = conv_bn3)\n",
    "\n",
    "\n",
    "    trainer = Trainer(model = adapt_fine2,seed = 123,model_name = f'3_adapter_groups_{group}',project_name = 'FINAL_CIFAR100_SERIAL_ALL3',classes = train_20.classes,binarise= True)\n",
    "    m = trainer.model\n",
    "    m.to(trainer.device)\n",
    "    m.fc = BinarizeLinear(320,20)\n",
    "    m.bn3 = nn.BatchNorm1d(20)\n",
    "    # for layer in [m.fc,m.bn3]:\n",
    "    #     for p in layer.parameters():\n",
    "    #         p.requires_grad = True\n",
    "    #     layer.train()\n",
    "    trainer.lr = 1e-3\n",
    "    trainer.batch_size = 64\n",
    "    trainer.epochs =75\n",
    "    trainer.epoch_chkpts = []\n",
    "    trainer.start_epoch = 0\n",
    "    trainer.set_scheduler(None)\n",
    "    # adpt = getattr(m,after_layer)[-1]\n",
    "    # print(adpt)\n",
    "    # print(m.fc.weight[0,0])\n",
    "    # params = nn.ModuleList([m.fc,m.bn3,adpt]).parameters()\n",
    "    trainer.set_optimizer(torch.optim.Adam)\n",
    "    trainer.train(train20_DL,test20_DL)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

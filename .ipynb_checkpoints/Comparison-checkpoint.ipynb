{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2caf320",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision.transforms import transforms\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249c4810",
   "metadata": {},
   "source": [
    "# Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "fbc17541",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(),\\\n",
    "                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\\\n",
    "                              transforms.ConvertImageDtype(dtype = torch.float32)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "f698b356",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train = torchvision.datasets.CIFAR10(root ='./data',train = True, download = True,transform = transform)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "b3a2019f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "trainloader = torch.utils.data.DataLoader(train,batch_size = batch_size,shuffle= True, num_workers = 0)\n",
    "os.listdir('./data')\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "b0e7fc1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<class 'torch.Tensor'>, <class 'torch.Tensor'>]\n",
      "tensor([7, 1, 2, 3]) torch.Size([4, 3, 32, 32])\n",
      "tensor([7, 1, 2, 3])\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "for i in iter(trainloader):\n",
    "    print([type(k) for k in i])\n",
    "    print(i[1],i[0].size())\n",
    "    print(i[1])\n",
    "    imgs = i[0]\n",
    "    img = i[0][0]\n",
    "    print(img.dtype)\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "fbe37050",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def imshow(img):\n",
    "    img = img.to(torch.float32)\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "b17dd7cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0767, 0.0641, 0.0757, 0.0747], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MyNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3,6,5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "#         self.type = self.conv1.weight.dtype\n",
    "        \n",
    "    def forward(self,x):\n",
    "#         x = x.type(self.conv1.weight.dtype)\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x,1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "#         x = F.softmax(x,dim = -1 )\n",
    "        return x\n",
    "\n",
    "#Create Models\n",
    "\n",
    "Model1 = MyNN()\n",
    "Model2 = MyNN().bfloat16()\n",
    "\n",
    "#Test forward function\n",
    "y = Model1.forward(imgs)\n",
    "# torch.sum(y,dim= -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "0bc87bc5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "def training(model,epochs,batch_size,train_set):\n",
    "    #Set Optimisers\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(Model1.parameters(), lr=0.001, momentum=0.9)\n",
    "    trainloader = torch.utils.data.DataLoader(train_set,batch_size = batch_size,shuffle= True, num_workers = 0)\n",
    "    \n",
    "    time_start = time.perf_counter()\n",
    "    print('hi')\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        running_loss = 0\n",
    "        for i,data in enumerate(iter(trainloader)):\n",
    "            images,labels = data\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "    total_time =time.perf_counter()-  time_start \n",
    "    \n",
    "    print(f'Finished Training \\nTotal Time {total_time}\\n Average Time Per Epoch {(total_time)/epochs}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "857f9df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save Model\n",
    "def saveModel(filename):\n",
    "    PATH = f'./{filename}.pth'\n",
    "    torch.save(Model1.state_dict(), PATH)\n",
    "    print(f'{filename}.pth saved!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "0a5ff495",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test(model,testloader):\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    correct_class = {classname: 0 for classname in classes}\n",
    "    total_class = {classname: 0 for classname in classes}\n",
    "    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i,data in enumerate(testloader):\n",
    "            images, labels = data\n",
    "            outputs = Model1(images)\n",
    "\n",
    "            _,predict = torch.max(outputs.data,-1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predict == labels).sum().item()\n",
    "            for label, prediction in zip(labels, predict):\n",
    "                if label == prediction:\n",
    "                    correct_class[classes[label]] += 1\n",
    "                total_class[classes[label]] += 1\n",
    "            \n",
    "    # print accuracy for each class\n",
    "    for classname, correct_count in correct_class.items():\n",
    "        accuracy = 100 * float(correct_count) / total_class[classname]\n",
    "        print(f'Accuracy for class: {classname:5s} is {accuracy:.1f} %')\n",
    "    #Overall Accuracy\n",
    "    print(f'Overall Accuracy : {correct/total*100:.1f}')\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "905eb3af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "[1,  3125] loss: 3.188\n",
      "[2,  3125] loss: 2.441\n",
      "[3,  3125] loss: 2.140\n",
      "[4,  3125] loss: 1.977\n",
      "[5,  3125] loss: 1.863\n",
      "[6,  3125] loss: 1.766\n",
      "[7,  3125] loss: 1.683\n",
      "[8,  3125] loss: 1.607\n",
      "[9,  3125] loss: 1.540\n",
      "[10,  3125] loss: 1.478\n",
      "[11,  3125] loss: 1.417\n",
      "[12,  3125] loss: 1.361\n",
      "[13,  3125] loss: 1.314\n",
      "[14,  3125] loss: 1.264\n",
      "[15,  3125] loss: 1.219\n",
      "[16,  3125] loss: 1.177\n",
      "[17,  3125] loss: 1.134\n",
      "[18,  3125] loss: 1.100\n",
      "[19,  3125] loss: 1.058\n",
      "[20,  3125] loss: 1.018\n",
      "Finished Training \n",
      "Total Time 400.8732778999838\n",
      " Average Time Per Epoch 20.04366389499919\n",
      "Model1.pth saved!\n",
      "Accuracy for class: plane is 70.4 %\n",
      "Accuracy for class: car   is 74.6 %\n",
      "Accuracy for class: bird  is 45.2 %\n",
      "Accuracy for class: cat   is 47.6 %\n",
      "Accuracy for class: deer  is 54.5 %\n",
      "Accuracy for class: dog   is 58.4 %\n",
      "Accuracy for class: frog  is 74.2 %\n",
      "Accuracy for class: horse is 58.6 %\n",
      "Accuracy for class: ship  is 79.5 %\n",
      "Accuracy for class: truck is 67.8 %\n",
      "Overall Accuracy : 63.1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "torch.manual_seed(123)\n",
    "random.seed(123)\n",
    "transform = transforms.Compose([transforms.ToTensor(),\\\n",
    "                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\\\n",
    "                              transforms.ConvertImageDtype(dtype = torch.float32)])\n",
    "\n",
    "train = torchvision.datasets.CIFAR10(root ='./data',train = True, download = True,transform = transform)\n",
    "\n",
    "Model1 = MyNN()\n",
    "\n",
    "training(Model1,epochs = 20,batch_size = 16,train_set = train)\n",
    "saveModel('Model1')\n",
    "test(Model1,testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "49a83608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "hi\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "\"slow_conv2d_cpu\" not implemented for 'Half'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [200]\u001b[0m, in \u001b[0;36m<cell line: 11>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m train \u001b[38;5;241m=\u001b[39m torchvision\u001b[38;5;241m.\u001b[39mdatasets\u001b[38;5;241m.\u001b[39mCIFAR10(root \u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data\u001b[39m\u001b[38;5;124m'\u001b[39m,train \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, download \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,transform \u001b[38;5;241m=\u001b[39m transform)\n\u001b[0;32m      9\u001b[0m Model2 \u001b[38;5;241m=\u001b[39m MyNN()\u001b[38;5;241m.\u001b[39mhalf()\n\u001b[1;32m---> 11\u001b[0m \u001b[43mtraining\u001b[49m\u001b[43m(\u001b[49m\u001b[43mModel2\u001b[49m\u001b[43m,\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m saveModel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModel2\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     13\u001b[0m test(Model1,testloader)\n",
      "Input \u001b[1;32mIn [198]\u001b[0m, in \u001b[0;36mtraining\u001b[1;34m(model, epochs, batch_size, train_set)\u001b[0m\n\u001b[0;32m     17\u001b[0m images,labels \u001b[38;5;241m=\u001b[39m data\n\u001b[0;32m     18\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 19\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m     21\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[1;32mIn [149]\u001b[0m, in \u001b[0;36mMyNN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,x):\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m#         x = x.type(self.conv1.weight.dtype)\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[0;32m     18\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x)))\n\u001b[0;32m     19\u001b[0m         x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mflatten(x,\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\conv.py:447\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    446\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 447\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\conv.py:443\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    439\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    441\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    442\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 443\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    444\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: \"slow_conv2d_cpu\" not implemented for 'Half'"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "random.seed(123)\n",
    "transform = transforms.Compose([transforms.ToTensor(),\\\n",
    "                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\\\n",
    "                              transforms.ConvertImageDtype(dtype = torch.half)])\n",
    "\n",
    "train = torchvision.datasets.CIFAR10(root ='./data',train = True, download = True,transform = transform)\n",
    "\n",
    "Model2 = MyNN().half()\n",
    "\n",
    "training(Model2,epochs = 20,batch_size = 16,train_set = train)\n",
    "saveModel('Model2')\n",
    "test(Model1,testloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

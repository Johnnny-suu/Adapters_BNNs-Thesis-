{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c5eb200",
   "metadata": {},
   "outputs": [],
   "source": [
    "from NN_Thesis.nn_classes import *\n",
    "from NN_Thesis.trainer import *\n",
    "from NN_Thesis.helper_functions import *\n",
    "from NN_Thesis.adapters import *\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "\n",
    "\n",
    "from torchvision.transforms import transforms\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "import random\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import BinaryNet.models as BNN_models\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from NN_Thesis.dataset import cifar_n_dataset,get_split_dataset,cifar_100_split\n",
    "from torch.utils.data import DataLoader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "195e55eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Cars_Train = torchvision.datasets.StanfordCars(root = './data/StanfordCars/',split='train',download=True,transform= None)\n",
    "Cars_Valid = torchvision.datasets.StanfordCars(root = './data/StanfordCars/',split='test',download=True,transform= None)\n",
    "\n",
    "Flower_Train = torchvision.datasets.Flowers102(root = './data/Flowers102/',split='train',download=True,transform= None)\n",
    "Flower_Valid = torchvision.datasets.Flowers102(root = './data/Flowers102/',split='val',download=True,transform= None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76871478",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1020"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Flower_Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "058c06e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class placeholder(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0dd1dfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 80])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = BNN_Resnet_Cifar100_UniAdapt(80,block=BasicBlock, layers=[4, 4, 4, 4],inflate = 1)\n",
    "x = torch.rand((4,3,32,32))\n",
    "net(x).shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7629c539",
   "metadata": {},
   "source": [
    "# CIFAR 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9dcee556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "torch.Size([50000, 3, 1024])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "def normalize_channels(data):\n",
    "    #We have a nxCxWxH array\n",
    "    d = data\n",
    "    d = torch.flatten(data,2,-1).to(dtype = torch.float32)\n",
    "    print(d.shape)\n",
    "    mean= torch.mean(d,dim = [0,2])\n",
    "    std = torch.std(d,dim = [0,2])\n",
    "    return mean,std\n",
    "stats_data = CIFAR10('./data',download=True,train=True,transform= transforms.ToTensor())\n",
    "stats_data = torch.stack([x[0] for x in stats_data],dim = 0)\n",
    "\n",
    "mean,std = normalize_channels(stats_data)\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.Normalize(mean, std)\n",
    "])\n",
    "\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean,std)\n",
    "])\n",
    "\n",
    "train_dataset = CIFAR10('./data',download=False,train=True,transform= train_transform)\n",
    "valid_dataset = CIFAR10('./data',download=False,train=False,transform= test_transform)\n",
    "trainloader = DataLoader(train_dataset,batch_size=64,shuffle=True)\n",
    "validloader = DataLoader(valid_dataset,batch_size=64)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2bf554f1",
   "metadata": {},
   "source": [
    "# Cifar 0-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a69ad25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('airplane', 'automobile', 'bird', 'cat', 'deer'), 25000)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train04_path = 'data/cifar_5/cifar_04/train/data'\n",
    "test04_path ='data/cifar_5/cifar_04/test/data'\n",
    "\n",
    "train04_data,test04_data,classes_04 = get_split_dataset(train04_path,test04_path,cifar_n_dataset)\n",
    "train04_loader = DataLoader(train04_data,batch_size=64,shuffle= True)\n",
    "test04_loader = DataLoader(test04_data,batch_size=64)\n",
    "\n",
    "classes_04,len(train04_data)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5cb92283",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Compose(\n",
       "    RandomCrop(size=(32, 32), padding=4)\n",
       "    RandomHorizontalFlip(p=0.5)\n",
       "    Normalize(mean=tensor([0.4906, 0.4856, 0.4508]), std=tensor([0.2454, 0.2415, 0.2620]))\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train04_data.transform"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c95e8063",
   "metadata": {},
   "source": [
    "# Cifar 5-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0cd4bd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('dog', 'frog', 'horse', 'ship', 'truck')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "train59_path = 'data/cifar_5/cifar_59/train/data'\n",
    "test59_path ='data/cifar_5/cifar_59/test/data'\n",
    "\n",
    "train59_data,test59_data,classes_59 = get_split_dataset(train59_path,test59_path,cifar_n_dataset)\n",
    "train59_loader = DataLoader(train59_data,batch_size=64,shuffle= True)\n",
    "test59_loader = DataLoader(test59_data,batch_size=64)\n",
    "classes_59"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b3c90706",
   "metadata": {},
   "source": [
    "# Intitial Train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "90ddf686",
   "metadata": {},
   "outputs": [],
   "source": [
    "BNN = BNN_Resnet_UniAdapt(5,freeze_pre_weights=False)\n",
    "BNN.layer3[-1].do_bntan = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "74407dd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No lr scheduler\n",
      "Optimizer Set to New: {'state': {}, 'param_groups': [{'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': False, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54]}]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjohnny_suu\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.14.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\John Su\\Downloads\\SydneyUni\\2022\\thesis\\Thesis\\wandb\\run-20230327_161906-1jfsaiav</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/johnny_suu/UniAdapt_Training_Encoder_Decoder/runs/1jfsaiav\" target=\"_blank\">crimson-sun-3</a></strong> to <a href=\"https://wandb.ai/johnny_suu/UniAdapt_Training_Encoder_Decoder\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "Project Name: UniAdapt_Training_Encoder_Decoder, Run Name BNN_Cifar10_Backbone_Cifar04 \n",
      "\n",
      "\n",
      "Run Start : 2023-03-27 16-19-04\n",
      "start_epoch : 0\n",
      "initial_lr : 0.001\n",
      "batch_size : 64\n",
      "epochs : 200\n",
      "epoch_chkpts : []\n",
      "device : cuda:0\n",
      "criterion : CrossEntropyLoss()\n",
      "optimizer : Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: False\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "seed : None\n",
      "model_architecture : <class 'NN_Thesis.nn_classes.BNN_Resnet_UniAdapt'>\n",
      "binerised_training : True\n",
      "Number of Elements : 4339780\n",
      "Initial accuracy:\n",
      "Test Accuracy : 20.2%, Test Loss: 20.296958028203083\n",
      "best_acc.pth saved!\n",
      "Test Accuracy : 20.5%, Test Loss: 19.803759055801585\n",
      "best_acc.pth saved!\n",
      "epoch: 1 average loss: 1.329\n",
      "Test Accuracy : 38.9%, Test Loss: 1.4328900273842147\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 27.42 seconds\n",
      "epoch: 2 average loss: 1.164\n",
      "Test Accuracy : 47.2%, Test Loss: 1.2687428872796553\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 26.87 seconds\n",
      "epoch: 3 average loss: 1.098\n",
      "Test Accuracy : 48.3%, Test Loss: 1.3000195660168612\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 26.82 seconds\n",
      "epoch: 4 average loss: 1.062\n",
      "Test Accuracy : 51.3%, Test Loss: 1.2313351299189315\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 26.86 seconds\n",
      "epoch: 5 average loss: 1.008\n",
      "Test Accuracy : 59.1%, Test Loss: 1.0341792551777031\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 26.82 seconds\n",
      "epoch: 6 average loss: 0.979\n",
      "Test Accuracy : 60.2%, Test Loss: 1.0133791120746467\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 26.86 seconds\n",
      "epoch: 7 average loss: 0.951\n",
      "Test Accuracy : 61.4%, Test Loss: 1.038829224019111\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 26.64 seconds\n",
      "epoch: 8 average loss: 0.907\n",
      "Test Accuracy : 63.7%, Test Loss: 0.9882890997053702\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 27.15 seconds\n",
      "epoch: 9 average loss: 0.886\n",
      "Test Accuracy : 69.0%, Test Loss: 0.8100663204736347\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 27.03 seconds\n",
      "epoch: 10 average loss: 0.845\n",
      "Test Accuracy : 56.1%, Test Loss: 1.183995246132718\n",
      "Epoch Time (Training + Test) = 26.86 seconds\n",
      "epoch: 11 average loss: 0.822\n",
      "Test Accuracy : 63.9%, Test Loss: 0.951321862166441\n",
      "Epoch Time (Training + Test) = 26.62 seconds\n",
      "epoch: 12 average loss: 0.794\n",
      "Test Accuracy : 61.2%, Test Loss: 1.1259421230871467\n",
      "Epoch Time (Training + Test) = 26.59 seconds\n",
      "epoch: 13 average loss: 0.779\n",
      "Test Accuracy : 59.3%, Test Loss: 1.1762555882900576\n",
      "Epoch Time (Training + Test) = 26.64 seconds\n",
      "epoch: 14 average loss: 0.744\n",
      "Test Accuracy : 63.2%, Test Loss: 0.9657328257077857\n",
      "Epoch Time (Training + Test) = 26.59 seconds\n",
      "epoch: 15 average loss: 0.718\n",
      "Test Accuracy : 74.1%, Test Loss: 0.6958156884471073\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 26.66 seconds\n",
      "epoch: 16 average loss: 0.710\n",
      "Test Accuracy : 65.1%, Test Loss: 0.954324557811399\n",
      "Epoch Time (Training + Test) = 26.62 seconds\n",
      "epoch: 17 average loss: 0.699\n",
      "Test Accuracy : 75.6%, Test Loss: 0.665513610538048\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 26.63 seconds\n",
      "epoch: 18 average loss: 0.678\n",
      "Test Accuracy : 76.4%, Test Loss: 0.6288583240931547\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 26.77 seconds\n",
      "epoch: 19 average loss: 0.648\n",
      "Test Accuracy : 75.2%, Test Loss: 0.6828229785720005\n",
      "Epoch Time (Training + Test) = 26.58 seconds\n",
      "epoch: 20 average loss: 0.635\n",
      "Test Accuracy : 71.3%, Test Loss: 0.7802110140836691\n",
      "Epoch Time (Training + Test) = 26.64 seconds\n",
      "epoch: 21 average loss: 0.622\n",
      "Test Accuracy : 75.5%, Test Loss: 0.651694194802755\n",
      "Epoch Time (Training + Test) = 26.56 seconds\n",
      "epoch: 22 average loss: 0.599\n",
      "Test Accuracy : 72.9%, Test Loss: 0.7035482997381235\n",
      "Epoch Time (Training + Test) = 26.63 seconds\n",
      "epoch: 23 average loss: 0.589\n",
      "Test Accuracy : 73.3%, Test Loss: 0.7422192277787607\n",
      "Epoch Time (Training + Test) = 26.61 seconds\n",
      "epoch: 24 average loss: 0.589\n",
      "Test Accuracy : 79.2%, Test Loss: 0.5712868288347993\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 26.65 seconds\n",
      "epoch: 25 average loss: 0.570\n",
      "Test Accuracy : 74.3%, Test Loss: 0.7070748787137526\n",
      "Epoch Time (Training + Test) = 26.86 seconds\n",
      "epoch: 26 average loss: 0.560\n",
      "Test Accuracy : 73.8%, Test Loss: 0.7040909106218363\n",
      "Epoch Time (Training + Test) = 26.89 seconds\n",
      "epoch: 27 average loss: 0.543\n",
      "Test Accuracy : 76.8%, Test Loss: 0.6171589542793322\n",
      "Epoch Time (Training + Test) = 26.80 seconds\n",
      "epoch: 28 average loss: 0.532\n",
      "Test Accuracy : 78.3%, Test Loss: 0.5924685933167422\n",
      "Epoch Time (Training + Test) = 27.23 seconds\n",
      "epoch: 29 average loss: 0.526\n",
      "Test Accuracy : 75.1%, Test Loss: 0.6708035250253315\n",
      "Epoch Time (Training + Test) = 26.89 seconds\n",
      "epoch: 30 average loss: 0.513\n",
      "Test Accuracy : 77.4%, Test Loss: 0.6188255554135842\n",
      "Epoch Time (Training + Test) = 26.61 seconds\n",
      "epoch: 31 average loss: 0.500\n",
      "Test Accuracy : 80.3%, Test Loss: 0.533748625200006\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 26.70 seconds\n",
      "epoch: 32 average loss: 0.488\n",
      "Test Accuracy : 79.6%, Test Loss: 0.5555441798288611\n",
      "Epoch Time (Training + Test) = 26.64 seconds\n",
      "epoch: 33 average loss: 0.475\n",
      "Test Accuracy : 79.6%, Test Loss: 0.5376034114934221\n",
      "Epoch Time (Training + Test) = 26.61 seconds\n",
      "epoch: 34 average loss: 0.468\n",
      "Test Accuracy : 76.3%, Test Loss: 0.65305634163603\n",
      "Epoch Time (Training + Test) = 26.67 seconds\n",
      "epoch: 35 average loss: 0.464\n",
      "Test Accuracy : 79.8%, Test Loss: 0.5469256156607519\n",
      "Epoch Time (Training + Test) = 26.58 seconds\n",
      "epoch: 36 average loss: 0.456\n",
      "Test Accuracy : 74.3%, Test Loss: 0.698581325102456\n",
      "Epoch Time (Training + Test) = 26.71 seconds\n",
      "epoch: 37 average loss: 0.442\n",
      "Test Accuracy : 78.1%, Test Loss: 0.6074319640292397\n",
      "Epoch Time (Training + Test) = 26.71 seconds\n",
      "epoch: 38 average loss: 0.442\n",
      "Test Accuracy : 80.5%, Test Loss: 0.5538706975647166\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 26.83 seconds\n",
      "epoch: 39 average loss: 0.426\n",
      "Test Accuracy : 83.0%, Test Loss: 0.46917535647561276\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 26.69 seconds\n",
      "epoch: 40 average loss: 0.423\n",
      "Test Accuracy : 81.4%, Test Loss: 0.4992027950437763\n",
      "Epoch Time (Training + Test) = 26.61 seconds\n",
      "epoch: 41 average loss: 0.412\n",
      "Test Accuracy : 81.2%, Test Loss: 0.5216586235203321\n",
      "Epoch Time (Training + Test) = 26.71 seconds\n",
      "epoch: 42 average loss: 0.405\n",
      "Test Accuracy : 83.3%, Test Loss: 0.46306583134433893\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 26.61 seconds\n",
      "epoch: 43 average loss: 0.397\n",
      "Test Accuracy : 82.2%, Test Loss: 0.49918330243871184\n",
      "Epoch Time (Training + Test) = 27.30 seconds\n",
      "epoch: 44 average loss: 0.393\n",
      "Test Accuracy : 83.7%, Test Loss: 0.4447890493688704\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 26.72 seconds\n",
      "epoch: 45 average loss: 0.390\n",
      "Test Accuracy : 81.4%, Test Loss: 0.5069227650572982\n",
      "Epoch Time (Training + Test) = 26.64 seconds\n",
      "epoch: 46 average loss: 0.378\n",
      "Test Accuracy : 83.6%, Test Loss: 0.4426178619076934\n",
      "Epoch Time (Training + Test) = 26.65 seconds\n",
      "epoch: 47 average loss: 0.375\n",
      "Test Accuracy : 83.8%, Test Loss: 0.459215570665613\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 26.66 seconds\n",
      "epoch: 48 average loss: 0.365\n",
      "Test Accuracy : 84.8%, Test Loss: 0.4242803237483471\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 26.67 seconds\n",
      "epoch: 49 average loss: 0.364\n",
      "Test Accuracy : 83.1%, Test Loss: 0.47985493759565717\n",
      "Epoch Time (Training + Test) = 26.60 seconds\n",
      "epoch: 50 average loss: 0.353\n",
      "Test Accuracy : 84.0%, Test Loss: 0.4373935812824889\n",
      "Epoch Time (Training + Test) = 26.64 seconds\n",
      "epoch: 51 average loss: 0.354\n",
      "Test Accuracy : 79.8%, Test Loss: 0.5399120966844921\n",
      "Epoch Time (Training + Test) = 26.52 seconds\n",
      "epoch: 52 average loss: 0.348\n",
      "Test Accuracy : 82.4%, Test Loss: 0.48128949935677684\n",
      "Epoch Time (Training + Test) = 26.64 seconds\n",
      "epoch: 53 average loss: 0.341\n",
      "Test Accuracy : 84.5%, Test Loss: 0.4321554227720333\n",
      "Epoch Time (Training + Test) = 26.58 seconds\n",
      "epoch: 54 average loss: 0.339\n",
      "Test Accuracy : 82.3%, Test Loss: 0.49199191075337084\n",
      "Epoch Time (Training + Test) = 26.75 seconds\n",
      "epoch: 55 average loss: 0.332\n",
      "Test Accuracy : 85.6%, Test Loss: 0.4039772613139092\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 26.64 seconds\n",
      "epoch: 56 average loss: 0.321\n",
      "Test Accuracy : 81.4%, Test Loss: 0.5230328674557843\n",
      "Epoch Time (Training + Test) = 26.60 seconds\n",
      "epoch: 57 average loss: 0.320\n",
      "Test Accuracy : 82.8%, Test Loss: 0.4674428824005248\n",
      "Epoch Time (Training + Test) = 26.62 seconds\n",
      "epoch: 58 average loss: 0.320\n",
      "Test Accuracy : 81.3%, Test Loss: 0.5101357473980023\n",
      "Epoch Time (Training + Test) = 26.55 seconds\n",
      "epoch: 59 average loss: 0.308\n",
      "Test Accuracy : 81.5%, Test Loss: 0.516648343941079\n",
      "Epoch Time (Training + Test) = 26.73 seconds\n",
      "epoch: 60 average loss: 0.306\n",
      "Test Accuracy : 85.5%, Test Loss: 0.4051660370034508\n",
      "Epoch Time (Training + Test) = 26.56 seconds\n",
      "epoch: 61 average loss: 0.302\n",
      "Test Accuracy : 84.8%, Test Loss: 0.41582433087161824\n",
      "Epoch Time (Training + Test) = 26.29 seconds\n",
      "epoch: 62 average loss: 0.301\n",
      "Test Accuracy : 86.0%, Test Loss: 0.3972506856993784\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 26.58 seconds\n",
      "epoch: 63 average loss: 0.297\n",
      "Test Accuracy : 84.0%, Test Loss: 0.4488426999577993\n",
      "Epoch Time (Training + Test) = 26.89 seconds\n",
      "epoch: 64 average loss: 0.286\n",
      "Test Accuracy : 84.9%, Test Loss: 0.42051714885083935\n",
      "Epoch Time (Training + Test) = 26.89 seconds\n",
      "epoch: 65 average loss: 0.280\n",
      "Test Accuracy : 84.5%, Test Loss: 0.43138838573535787\n",
      "Epoch Time (Training + Test) = 26.76 seconds\n",
      "epoch: 66 average loss: 0.284\n",
      "Test Accuracy : 82.0%, Test Loss: 0.5028724202626869\n",
      "Epoch Time (Training + Test) = 26.28 seconds\n",
      "epoch: 67 average loss: 0.275\n",
      "Test Accuracy : 83.1%, Test Loss: 0.49039676295051093\n",
      "Epoch Time (Training + Test) = 26.66 seconds\n",
      "epoch: 68 average loss: 0.272\n",
      "Test Accuracy : 84.6%, Test Loss: 0.4424038894851751\n",
      "Epoch Time (Training + Test) = 26.76 seconds\n",
      "epoch: 69 average loss: 0.268\n",
      "Test Accuracy : 85.5%, Test Loss: 0.4170500877537305\n",
      "Epoch Time (Training + Test) = 26.33 seconds\n",
      "epoch: 70 average loss: 0.273\n",
      "Test Accuracy : 83.4%, Test Loss: 0.4745943925425976\n",
      "Epoch Time (Training + Test) = 26.45 seconds\n",
      "epoch: 71 average loss: 0.268\n",
      "Test Accuracy : 85.2%, Test Loss: 0.41659988538373877\n",
      "Epoch Time (Training + Test) = 26.52 seconds\n",
      "epoch: 72 average loss: 0.260\n",
      "Test Accuracy : 86.3%, Test Loss: 0.3885545509902737\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 26.72 seconds\n",
      "epoch: 73 average loss: 0.264\n",
      "Test Accuracy : 83.8%, Test Loss: 0.4841408422098884\n",
      "Epoch Time (Training + Test) = 26.62 seconds\n",
      "epoch: 74 average loss: 0.264\n",
      "Test Accuracy : 86.8%, Test Loss: 0.3929730566996562\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 26.36 seconds\n",
      "epoch: 75 average loss: 0.251\n",
      "Test Accuracy : 85.0%, Test Loss: 0.4260130575563334\n",
      "Epoch Time (Training + Test) = 26.10 seconds\n",
      "epoch: 76 average loss: 0.257\n",
      "Test Accuracy : 85.2%, Test Loss: 0.42239664542146876\n",
      "Epoch Time (Training + Test) = 25.98 seconds\n",
      "epoch: 77 average loss: 0.249\n",
      "Test Accuracy : 81.7%, Test Loss: 0.5512699901680403\n",
      "Epoch Time (Training + Test) = 26.15 seconds\n",
      "epoch: 78 average loss: 0.240\n",
      "Test Accuracy : 84.0%, Test Loss: 0.4504659544157831\n",
      "Epoch Time (Training + Test) = 26.06 seconds\n",
      "epoch: 79 average loss: 0.243\n",
      "Test Accuracy : 86.5%, Test Loss: 0.3941673270509213\n",
      "Epoch Time (Training + Test) = 26.04 seconds\n",
      "epoch: 80 average loss: 0.237\n",
      "Test Accuracy : 85.2%, Test Loss: 0.42192422306235833\n",
      "Epoch Time (Training + Test) = 26.08 seconds\n",
      "epoch: 81 average loss: 0.232\n",
      "Test Accuracy : 86.7%, Test Loss: 0.3817845624458941\n",
      "Epoch Time (Training + Test) = 26.62 seconds\n",
      "epoch: 82 average loss: 0.232\n",
      "Test Accuracy : 85.4%, Test Loss: 0.4230975955724716\n",
      "Epoch Time (Training + Test) = 26.24 seconds\n",
      "epoch: 83 average loss: 0.235\n",
      "Test Accuracy : 85.5%, Test Loss: 0.4304519118387488\n",
      "Epoch Time (Training + Test) = 25.98 seconds\n",
      "epoch: 84 average loss: 0.227\n",
      "Test Accuracy : 87.1%, Test Loss: 0.3789778974237321\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 26.17 seconds\n",
      "epoch: 85 average loss: 0.229\n",
      "Test Accuracy : 87.2%, Test Loss: 0.38193297367307205\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 26.03 seconds\n",
      "epoch: 86 average loss: 0.224\n",
      "Test Accuracy : 86.0%, Test Loss: 0.38710849439796013\n",
      "Epoch Time (Training + Test) = 26.20 seconds\n",
      "epoch: 87 average loss: 0.230\n",
      "Test Accuracy : 84.1%, Test Loss: 0.4483189272069478\n",
      "Epoch Time (Training + Test) = 26.06 seconds\n",
      "epoch: 88 average loss: 0.222\n",
      "Test Accuracy : 85.0%, Test Loss: 0.41859302264225634\n",
      "Epoch Time (Training + Test) = 26.10 seconds\n",
      "epoch: 89 average loss: 0.215\n",
      "Test Accuracy : 87.2%, Test Loss: 0.36599083037316044\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 26.15 seconds\n",
      "epoch: 90 average loss: 0.215\n",
      "Test Accuracy : 85.5%, Test Loss: 0.4217191847819316\n",
      "Epoch Time (Training + Test) = 26.08 seconds\n",
      "epoch: 91 average loss: 0.214\n",
      "Test Accuracy : 87.6%, Test Loss: 0.35512080449092237\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 26.15 seconds\n",
      "epoch: 92 average loss: 0.209\n",
      "Test Accuracy : 87.2%, Test Loss: 0.36774170653352256\n",
      "Epoch Time (Training + Test) = 26.08 seconds\n",
      "epoch: 93 average loss: 0.208\n",
      "Test Accuracy : 84.1%, Test Loss: 0.47847110315968716\n",
      "Epoch Time (Training + Test) = 26.09 seconds\n",
      "epoch: 94 average loss: 0.205\n",
      "Test Accuracy : 84.7%, Test Loss: 0.4393704039386556\n",
      "Epoch Time (Training + Test) = 26.10 seconds\n",
      "epoch: 95 average loss: 0.200\n",
      "Test Accuracy : 87.0%, Test Loss: 0.38083930992627446\n",
      "Epoch Time (Training + Test) = 26.05 seconds\n",
      "epoch: 96 average loss: 0.203\n",
      "Test Accuracy : 87.3%, Test Loss: 0.37463250982610485\n",
      "Epoch Time (Training + Test) = 26.04 seconds\n",
      "epoch: 97 average loss: 0.202\n",
      "Test Accuracy : 86.5%, Test Loss: 0.4023712060496777\n",
      "Epoch Time (Training + Test) = 26.00 seconds\n",
      "epoch: 98 average loss: 0.199\n",
      "Test Accuracy : 86.4%, Test Loss: 0.41873563090457194\n",
      "Epoch Time (Training + Test) = 26.14 seconds\n",
      "epoch: 99 average loss: 0.195\n",
      "Test Accuracy : 88.2%, Test Loss: 0.3515350079234642\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 26.14 seconds\n",
      "epoch: 100 average loss: 0.192\n",
      "Test Accuracy : 87.6%, Test Loss: 0.3633125734668744\n",
      "Epoch Time (Training + Test) = 26.11 seconds\n",
      "epoch: 101 average loss: 0.190\n",
      "Test Accuracy : 86.4%, Test Loss: 0.3975109740903106\n",
      "Epoch Time (Training + Test) = 26.12 seconds\n",
      "epoch: 102 average loss: 0.193\n",
      "Test Accuracy : 86.9%, Test Loss: 0.3781470077324517\n",
      "Epoch Time (Training + Test) = 26.06 seconds\n",
      "epoch: 103 average loss: 0.183\n",
      "Test Accuracy : 88.3%, Test Loss: 0.3527042801621594\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 26.17 seconds\n",
      "epoch: 104 average loss: 0.184\n",
      "Test Accuracy : 84.9%, Test Loss: 0.45301345877255067\n",
      "Epoch Time (Training + Test) = 26.04 seconds\n",
      "epoch: 105 average loss: 0.184\n",
      "Test Accuracy : 87.0%, Test Loss: 0.3803913392881049\n",
      "Epoch Time (Training + Test) = 26.05 seconds\n",
      "epoch: 106 average loss: 0.188\n",
      "Test Accuracy : 87.3%, Test Loss: 0.3710571248320085\n",
      "Epoch Time (Training + Test) = 26.05 seconds\n",
      "epoch: 107 average loss: 0.183\n",
      "Test Accuracy : 88.1%, Test Loss: 0.3527635741460172\n",
      "Epoch Time (Training + Test) = 26.12 seconds\n",
      "epoch: 108 average loss: 0.176\n",
      "Test Accuracy : 87.0%, Test Loss: 0.3822734955745407\n",
      "Epoch Time (Training + Test) = 26.07 seconds\n",
      "epoch: 109 average loss: 0.171\n",
      "Test Accuracy : 85.9%, Test Loss: 0.40663551935289477\n",
      "Epoch Time (Training + Test) = 26.20 seconds\n",
      "epoch: 110 average loss: 0.182\n",
      "Test Accuracy : 87.7%, Test Loss: 0.3680960815164107\n",
      "Epoch Time (Training + Test) = 26.05 seconds\n",
      "epoch: 111 average loss: 0.175\n",
      "Test Accuracy : 86.7%, Test Loss: 0.3861387041927893\n",
      "Epoch Time (Training + Test) = 25.97 seconds\n",
      "epoch: 112 average loss: 0.168\n",
      "Test Accuracy : 87.4%, Test Loss: 0.3677670042726058\n",
      "Epoch Time (Training + Test) = 26.11 seconds\n",
      "epoch: 113 average loss: 0.170\n",
      "Test Accuracy : 87.2%, Test Loss: 0.3791629804086082\n",
      "Epoch Time (Training + Test) = 25.73 seconds\n",
      "epoch: 114 average loss: 0.171\n",
      "Test Accuracy : 88.1%, Test Loss: 0.34413767766348924\n",
      "Epoch Time (Training + Test) = 24.73 seconds\n",
      "epoch: 115 average loss: 0.165\n",
      "Test Accuracy : 86.2%, Test Loss: 0.40776501209298266\n",
      "Epoch Time (Training + Test) = 24.76 seconds\n",
      "epoch: 116 average loss: 0.164\n",
      "Test Accuracy : 85.2%, Test Loss: 0.45616346974916094\n",
      "Epoch Time (Training + Test) = 24.78 seconds\n",
      "epoch: 117 average loss: 0.166\n",
      "Test Accuracy : 84.8%, Test Loss: 0.44833902826037586\n",
      "Epoch Time (Training + Test) = 24.70 seconds\n",
      "epoch: 118 average loss: 0.163\n",
      "Test Accuracy : 88.4%, Test Loss: 0.35311054542094844\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 24.81 seconds\n",
      "epoch: 119 average loss: 0.156\n",
      "Test Accuracy : 88.8%, Test Loss: 0.36060255124599117\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 24.81 seconds\n",
      "epoch: 120 average loss: 0.157\n",
      "Test Accuracy : 85.7%, Test Loss: 0.43959591773491874\n",
      "Epoch Time (Training + Test) = 24.68 seconds\n",
      "epoch: 121 average loss: 0.158\n",
      "Test Accuracy : 87.0%, Test Loss: 0.39903778568664683\n",
      "Epoch Time (Training + Test) = 24.72 seconds\n",
      "epoch: 122 average loss: 0.152\n",
      "Test Accuracy : 87.6%, Test Loss: 0.3947911979276923\n",
      "Epoch Time (Training + Test) = 24.76 seconds\n",
      "epoch: 123 average loss: 0.159\n",
      "Test Accuracy : 88.1%, Test Loss: 0.3628507805398748\n",
      "Epoch Time (Training + Test) = 24.72 seconds\n",
      "epoch: 124 average loss: 0.153\n",
      "Test Accuracy : 87.0%, Test Loss: 0.383663428452196\n",
      "Epoch Time (Training + Test) = 24.69 seconds\n",
      "epoch: 125 average loss: 0.150\n",
      "Test Accuracy : 86.1%, Test Loss: 0.4331917930630189\n",
      "Epoch Time (Training + Test) = 24.78 seconds\n",
      "epoch: 126 average loss: 0.144\n",
      "Test Accuracy : 88.2%, Test Loss: 0.36079806994788255\n",
      "Epoch Time (Training + Test) = 24.80 seconds\n",
      "epoch: 127 average loss: 0.147\n",
      "Test Accuracy : 85.9%, Test Loss: 0.47638034292414216\n",
      "Epoch Time (Training + Test) = 24.68 seconds\n",
      "epoch: 128 average loss: 0.149\n",
      "Test Accuracy : 88.6%, Test Loss: 0.34814072973271715\n",
      "Epoch Time (Training + Test) = 24.74 seconds\n",
      "epoch: 129 average loss: 0.143\n",
      "Test Accuracy : 86.9%, Test Loss: 0.3878405286541468\n",
      "Epoch Time (Training + Test) = 24.75 seconds\n",
      "epoch: 130 average loss: 0.145\n",
      "Test Accuracy : 87.6%, Test Loss: 0.3840295117867144\n",
      "Epoch Time (Training + Test) = 24.74 seconds\n",
      "epoch: 131 average loss: 0.145\n",
      "Test Accuracy : 87.7%, Test Loss: 0.3699459312837335\n",
      "Epoch Time (Training + Test) = 24.70 seconds\n",
      "epoch: 132 average loss: 0.140\n",
      "Test Accuracy : 86.6%, Test Loss: 0.4280811114401757\n",
      "Epoch Time (Training + Test) = 25.49 seconds\n",
      "epoch: 133 average loss: 0.145\n",
      "Test Accuracy : 88.5%, Test Loss: 0.350072005315672\n",
      "Epoch Time (Training + Test) = 26.10 seconds\n",
      "epoch: 134 average loss: 0.139\n",
      "Test Accuracy : 89.1%, Test Loss: 0.3463766244770605\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 26.08 seconds\n",
      "epoch: 135 average loss: 0.135\n",
      "Test Accuracy : 87.4%, Test Loss: 0.37384146466096746\n",
      "Epoch Time (Training + Test) = 26.08 seconds\n",
      "epoch: 136 average loss: 0.143\n",
      "Test Accuracy : 87.6%, Test Loss: 0.3842752013779894\n",
      "Epoch Time (Training + Test) = 25.96 seconds\n",
      "epoch: 137 average loss: 0.140\n",
      "Test Accuracy : 85.5%, Test Loss: 0.4809867426564422\n",
      "Epoch Time (Training + Test) = 26.13 seconds\n",
      "epoch: 138 average loss: 0.131\n",
      "Test Accuracy : 87.5%, Test Loss: 0.3726036401489113\n",
      "Epoch Time (Training + Test) = 26.89 seconds\n",
      "epoch: 139 average loss: 0.129\n",
      "Test Accuracy : 87.4%, Test Loss: 0.3791589850111853\n",
      "Epoch Time (Training + Test) = 26.24 seconds\n",
      "epoch: 140 average loss: 0.137\n",
      "Test Accuracy : 88.1%, Test Loss: 0.3724201050457321\n",
      "Epoch Time (Training + Test) = 26.14 seconds\n",
      "epoch: 141 average loss: 0.133\n",
      "Test Accuracy : 89.1%, Test Loss: 0.33607529631898375\n",
      "Epoch Time (Training + Test) = 26.20 seconds\n",
      "epoch: 142 average loss: 0.133\n",
      "Test Accuracy : 87.7%, Test Loss: 0.3834740860929972\n",
      "Epoch Time (Training + Test) = 26.18 seconds\n",
      "epoch: 143 average loss: 0.132\n",
      "Test Accuracy : 87.6%, Test Loss: 0.37053364319608934\n",
      "Epoch Time (Training + Test) = 26.15 seconds\n",
      "epoch: 144 average loss: 0.131\n",
      "Test Accuracy : 86.5%, Test Loss: 0.426979837844832\n",
      "Epoch Time (Training + Test) = 26.20 seconds\n",
      "epoch: 145 average loss: 0.126\n",
      "Test Accuracy : 85.1%, Test Loss: 0.4614777700810493\n",
      "Epoch Time (Training + Test) = 26.22 seconds\n",
      "epoch: 146 average loss: 0.127\n",
      "Test Accuracy : 89.2%, Test Loss: 0.3553539225949517\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 26.20 seconds\n",
      "epoch: 147 average loss: 0.124\n",
      "Test Accuracy : 86.0%, Test Loss: 0.4488211362681623\n",
      "Epoch Time (Training + Test) = 26.21 seconds\n",
      "epoch: 148 average loss: 0.130\n",
      "Test Accuracy : 87.9%, Test Loss: 0.36680555487452426\n",
      "Epoch Time (Training + Test) = 26.18 seconds\n",
      "epoch: 149 average loss: 0.130\n",
      "Test Accuracy : 88.4%, Test Loss: 0.3550078609510313\n",
      "Epoch Time (Training + Test) = 26.17 seconds\n",
      "epoch: 150 average loss: 0.127\n",
      "Test Accuracy : 85.4%, Test Loss: 0.4829172465620162\n",
      "Epoch Time (Training + Test) = 26.11 seconds\n",
      "epoch: 151 average loss: 0.120\n",
      "Test Accuracy : 86.7%, Test Loss: 0.43124705685090414\n",
      "Epoch Time (Training + Test) = 26.27 seconds\n",
      "epoch: 152 average loss: 0.117\n",
      "Test Accuracy : 87.3%, Test Loss: 0.4115414295015456\n",
      "Epoch Time (Training + Test) = 26.19 seconds\n",
      "epoch: 153 average loss: 0.123\n",
      "Test Accuracy : 87.4%, Test Loss: 0.4088173914182035\n",
      "Epoch Time (Training + Test) = 26.35 seconds\n",
      "epoch: 154 average loss: 0.124\n",
      "Test Accuracy : 88.3%, Test Loss: 0.35927050452254994\n",
      "Epoch Time (Training + Test) = 26.65 seconds\n",
      "epoch: 155 average loss: 0.123\n",
      "Test Accuracy : 88.7%, Test Loss: 0.36256046470584746\n",
      "Epoch Time (Training + Test) = 26.01 seconds\n",
      "epoch: 156 average loss: 0.119\n",
      "Test Accuracy : 89.6%, Test Loss: 0.3258620598648168\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 26.11 seconds\n",
      "epoch: 157 average loss: 0.112\n",
      "Test Accuracy : 88.1%, Test Loss: 0.3817116180552712\n",
      "Epoch Time (Training + Test) = 26.06 seconds\n",
      "epoch: 158 average loss: 0.117\n",
      "Test Accuracy : 88.1%, Test Loss: 0.39115344875645414\n",
      "Epoch Time (Training + Test) = 26.01 seconds\n",
      "epoch: 159 average loss: 0.119\n",
      "Test Accuracy : 89.0%, Test Loss: 0.3441370997247817\n",
      "Epoch Time (Training + Test) = 26.01 seconds\n",
      "epoch: 160 average loss: 0.116\n",
      "Test Accuracy : 87.2%, Test Loss: 0.4208102565777453\n",
      "Epoch Time (Training + Test) = 26.02 seconds\n",
      "epoch: 161 average loss: 0.118\n",
      "Test Accuracy : 83.3%, Test Loss: 0.5697360274535191\n",
      "Epoch Time (Training + Test) = 25.37 seconds\n",
      "epoch: 162 average loss: 0.115\n",
      "Test Accuracy : 89.3%, Test Loss: 0.3533820414656325\n",
      "Epoch Time (Training + Test) = 24.34 seconds\n",
      "epoch: 163 average loss: 0.119\n",
      "Test Accuracy : 87.3%, Test Loss: 0.40473579247541064\n",
      "Epoch Time (Training + Test) = 24.43 seconds\n",
      "epoch: 164 average loss: 0.122\n",
      "Test Accuracy : 88.0%, Test Loss: 0.3862376116876361\n",
      "Epoch Time (Training + Test) = 25.28 seconds\n",
      "epoch: 165 average loss: 0.115\n",
      "Test Accuracy : 86.8%, Test Loss: 0.4244541242435763\n",
      "Epoch Time (Training + Test) = 26.26 seconds\n",
      "epoch: 166 average loss: 0.111\n",
      "Test Accuracy : 88.4%, Test Loss: 0.3752276209666382\n",
      "Epoch Time (Training + Test) = 26.22 seconds\n",
      "epoch: 167 average loss: 0.113\n",
      "Test Accuracy : 87.5%, Test Loss: 0.40122754211667216\n",
      "Epoch Time (Training + Test) = 26.08 seconds\n",
      "epoch: 168 average loss: 0.106\n",
      "Test Accuracy : 85.7%, Test Loss: 0.4916028304945064\n",
      "Epoch Time (Training + Test) = 26.07 seconds\n",
      "epoch: 169 average loss: 0.106\n",
      "Test Accuracy : 88.3%, Test Loss: 0.41751469275619413\n",
      "Epoch Time (Training + Test) = 26.14 seconds\n",
      "epoch: 170 average loss: 0.109\n",
      "Test Accuracy : 89.1%, Test Loss: 0.38033111110518253\n",
      "Epoch Time (Training + Test) = 26.05 seconds\n",
      "epoch: 171 average loss: 0.111\n",
      "Test Accuracy : 88.8%, Test Loss: 0.35703592940787726\n",
      "Epoch Time (Training + Test) = 26.05 seconds\n",
      "epoch: 172 average loss: 0.103\n",
      "Test Accuracy : 88.7%, Test Loss: 0.36991685185628603\n",
      "Epoch Time (Training + Test) = 26.14 seconds\n",
      "epoch: 173 average loss: 0.111\n",
      "Test Accuracy : 88.3%, Test Loss: 0.3715255801341956\n",
      "Epoch Time (Training + Test) = 26.02 seconds\n",
      "epoch: 174 average loss: 0.103\n",
      "Test Accuracy : 87.9%, Test Loss: 0.40280398661624406\n",
      "Epoch Time (Training + Test) = 26.05 seconds\n",
      "epoch: 175 average loss: 0.107\n",
      "Test Accuracy : 88.1%, Test Loss: 0.39681534437297644\n",
      "Epoch Time (Training + Test) = 26.01 seconds\n",
      "epoch: 176 average loss: 0.100\n",
      "Test Accuracy : 87.0%, Test Loss: 0.4207426404368274\n",
      "Epoch Time (Training + Test) = 26.14 seconds\n",
      "epoch: 177 average loss: 0.104\n",
      "Test Accuracy : 89.3%, Test Loss: 0.3705829458570556\n",
      "Epoch Time (Training + Test) = 26.22 seconds\n",
      "epoch: 178 average loss: 0.103\n",
      "Test Accuracy : 84.7%, Test Loss: 0.5386537043731424\n",
      "Epoch Time (Training + Test) = 26.02 seconds\n",
      "epoch: 179 average loss: 0.101\n",
      "Test Accuracy : 87.1%, Test Loss: 0.4297804715512674\n",
      "Epoch Time (Training + Test) = 26.10 seconds\n",
      "epoch: 180 average loss: 0.102\n",
      "Test Accuracy : 88.6%, Test Loss: 0.37318258892886247\n",
      "Epoch Time (Training + Test) = 26.62 seconds\n",
      "epoch: 181 average loss: 0.097\n",
      "Test Accuracy : 88.3%, Test Loss: 0.37312651982035816\n",
      "Epoch Time (Training + Test) = 26.38 seconds\n",
      "epoch: 182 average loss: 0.103\n",
      "Test Accuracy : 88.4%, Test Loss: 0.3870419179902801\n",
      "Epoch Time (Training + Test) = 26.07 seconds\n",
      "epoch: 183 average loss: 0.103\n",
      "Test Accuracy : 88.9%, Test Loss: 0.36409138633480553\n",
      "Epoch Time (Training + Test) = 26.05 seconds\n",
      "epoch: 184 average loss: 0.100\n",
      "Test Accuracy : 88.9%, Test Loss: 0.35914928238555976\n",
      "Epoch Time (Training + Test) = 25.97 seconds\n",
      "epoch: 185 average loss: 0.096\n",
      "Test Accuracy : 88.9%, Test Loss: 0.3850921943217893\n",
      "Epoch Time (Training + Test) = 26.13 seconds\n",
      "epoch: 186 average loss: 0.102\n",
      "Test Accuracy : 87.6%, Test Loss: 0.467138441114486\n",
      "Epoch Time (Training + Test) = 26.07 seconds\n",
      "epoch: 187 average loss: 0.100\n",
      "Test Accuracy : 88.8%, Test Loss: 0.3710866955733752\n",
      "Epoch Time (Training + Test) = 26.02 seconds\n",
      "epoch: 188 average loss: 0.104\n",
      "Test Accuracy : 88.2%, Test Loss: 0.3868486520987523\n",
      "Epoch Time (Training + Test) = 26.05 seconds\n",
      "epoch: 189 average loss: 0.100\n",
      "Test Accuracy : 89.6%, Test Loss: 0.33575818206690533\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 26.02 seconds\n",
      "epoch: 190 average loss: 0.095\n",
      "Test Accuracy : 87.2%, Test Loss: 0.4390597294403028\n",
      "Epoch Time (Training + Test) = 26.13 seconds\n",
      "epoch: 191 average loss: 0.095\n",
      "Test Accuracy : 88.0%, Test Loss: 0.4010123844388165\n",
      "Epoch Time (Training + Test) = 26.16 seconds\n",
      "epoch: 192 average loss: 0.095\n",
      "Test Accuracy : 88.8%, Test Loss: 0.37727584029677547\n",
      "Epoch Time (Training + Test) = 26.52 seconds\n",
      "epoch: 193 average loss: 0.101\n",
      "Test Accuracy : 89.5%, Test Loss: 0.3366792882922329\n",
      "Epoch Time (Training + Test) = 26.50 seconds\n",
      "epoch: 194 average loss: 0.095\n",
      "Test Accuracy : 89.6%, Test Loss: 0.3458816590188425\n",
      "Epoch Time (Training + Test) = 26.00 seconds\n",
      "epoch: 195 average loss: 0.093\n",
      "Test Accuracy : 88.9%, Test Loss: 0.3604185489253907\n",
      "Epoch Time (Training + Test) = 26.03 seconds\n",
      "epoch: 196 average loss: 0.095\n",
      "Test Accuracy : 89.9%, Test Loss: 0.3405440363891517\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 26.05 seconds\n",
      "epoch: 197 average loss: 0.092\n",
      "Test Accuracy : 88.7%, Test Loss: 0.36114824369926996\n",
      "Epoch Time (Training + Test) = 26.47 seconds\n",
      "epoch: 198 average loss: 0.093\n",
      "Test Accuracy : 90.2%, Test Loss: 0.3490175558607789\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 25.96 seconds\n",
      "epoch: 199 average loss: 0.094\n",
      "Test Accuracy : 88.3%, Test Loss: 0.4211653591239754\n",
      "Epoch Time (Training + Test) = 25.90 seconds\n",
      "epoch: 200 average loss: 0.089\n",
      "Test Accuracy : 88.3%, Test Loss: 0.3819447103180463\n",
      "Epoch Time (Training + Test) = 25.97 seconds\n",
      "Data Saved to BNN_Cifar10_Backbone_Cifar04.csv\n",
      "Finished Training: \n",
      "Total Time 2.909620 hours\n",
      " Average Time Per Epoch 52.37 seconds\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(BNN,model_name='BNN_Cifar10_Backbone_Cifar04',project_name='UniAdapt_Training_Encoder_Decoder',binarise=True,classes=classes_04)\n",
    "trainer.lr = 1e-3\n",
    "trainer.batch_size = 64\n",
    "trainer.epochs =200\n",
    "trainer.epoch_chkpts = []\n",
    "trainer.start_epoch = 0\n",
    "trainer.set_scheduler(None)\n",
    "trainer.set_optimizer(torch.optim.Adam)\n",
    "trainer.train(train04_loader,test04_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c11b421c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "18ec7f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'SavedModels\\UniAdapt_Training_Encoder_Decoder\\BNN_Cifar10_Backbone_Cifar04_2023-03-27 16-19-04\\BNN_Cifar10_Backbone_Cifar04_best_acc.pth'\n",
    "\n",
    "state = torch.load(path,map_location='cuda:0')\n",
    "BNN = BNN_Resnet_UniAdapt(5,freeze_pre_weights=True)\n",
    "BNN.layer3[-1].do_bntan = True\n",
    "# for key in state.keys():\n",
    "#     if 'decoders' in key or 'encoders' in key:\n",
    "#         del state[key] \n",
    "\n",
    "BNN.load_state_dict(state,strict=False)\n",
    "\n",
    "BNN = BNN.to('cuda:0')\n",
    "BNN.freeze()\n",
    "encoder_net = BNN_UniAdapt_encoders([80,160,320],[40,80,160],1)\n",
    "BNN.add_Encoders(encoder_net=encoder_net)\n",
    "BNN = BNN.to('cuda:0')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4758ffe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor(0.3234, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.2827, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.0883, device='cuda:0', grad_fn=<DivBackward0>)]\n",
      "[tensor(0.1558, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.1588, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.0244, device='cuda:0', grad_fn=<DivBackward0>)]\n",
      "[tensor(0.1486, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.1538, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.0176, device='cuda:0', grad_fn=<DivBackward0>)]\n",
      "[tensor(0.1462, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.1518, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.0159, device='cuda:0', grad_fn=<DivBackward0>)]\n",
      "[tensor(0.1451, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.1508, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.0153, device='cuda:0', grad_fn=<DivBackward0>)]\n",
      "[tensor(0.1446, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.1501, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.0150, device='cuda:0', grad_fn=<DivBackward0>)]\n",
      "[tensor(0.1443, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.1497, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.0146, device='cuda:0', grad_fn=<DivBackward0>)]\n",
      "[tensor(0.1441, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.1495, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.0142, device='cuda:0', grad_fn=<DivBackward0>)]\n",
      "[tensor(0.1440, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.1493, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.0141, device='cuda:0', grad_fn=<DivBackward0>)]\n",
      "[tensor(0.1440, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.1492, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.0137, device='cuda:0', grad_fn=<DivBackward0>)]\n",
      "[tensor(0.1439, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.1490, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.0136, device='cuda:0', grad_fn=<DivBackward0>)]\n",
      "[tensor(0.1438, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.1490, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.0135, device='cuda:0', grad_fn=<DivBackward0>)]\n",
      "[tensor(0.1438, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.1489, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.0135, device='cuda:0', grad_fn=<DivBackward0>)]\n",
      "[tensor(0.1438, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.1488, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.0133, device='cuda:0', grad_fn=<DivBackward0>)]\n",
      "[tensor(0.1438, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.1488, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.0132, device='cuda:0', grad_fn=<DivBackward0>)]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\John Su\\Downloads\\SydneyUni\\2022\\thesis\\Thesis\\Untitled.ipynb Cell 14\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/John%20Su/Downloads/SydneyUni/2022/thesis/Thesis/Untitled.ipynb#X16sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m BNN\u001b[39m.\u001b[39;49mtrain_encoders(\u001b[39m20\u001b[39;49m,trainloader \u001b[39m=\u001b[39;49m train04_loader)\n",
      "File \u001b[1;32mc:\\Users\\John Su\\Downloads\\SydneyUni\\2022\\thesis\\Thesis\\NN_Thesis\\nn_classes.py:291\u001b[0m, in \u001b[0;36mBNN_Resnet_UniAdapt.train_encoders\u001b[1;34m(self, epochs, trainloader, lr)\u001b[0m\n\u001b[0;32m    288\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtanh1(x)\n\u001b[0;32m    289\u001b[0m     f1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer1(x)\n\u001b[1;32m--> 291\u001b[0m     f2 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayer2(f1)\n\u001b[0;32m    292\u001b[0m     f3 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer3(f2)\n\u001b[0;32m    295\u001b[0m decodes \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder_net([f1,f2,f3],train\u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    203\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 204\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    205\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\John Su\\Downloads\\SydneyUni\\2022\\thesis\\Thesis\\NN_Thesis\\models\\resnet_binary.py:68\u001b[0m, in \u001b[0;36mBasicBlock.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     64\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv2(out)\n\u001b[0;32m     67\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdownsample \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m---> 68\u001b[0m     \u001b[39mif\u001b[39;00m residual\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mmax()\u001b[39m>\u001b[39;49m\u001b[39m1\u001b[39;49m:\n\u001b[0;32m     69\u001b[0m         \u001b[39mimport\u001b[39;00m \u001b[39mpdb\u001b[39;00m; pdb\u001b[39m.\u001b[39mset_trace()\n\u001b[0;32m     70\u001b[0m     residual \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdownsample(residual)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "BNN.train_encoders(20,trainloader = train04_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "14200375",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 160])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n1 = uniAdapt_Net([40,80,160],[40,80,160],block = thinBlock2)\n",
    "n1 = n1.cuda()\n",
    "n1.train()\n",
    "# adapt_net = nn.Sequential(n1,nn.AdaptiveMaxPool2d(1),nn.Flatten(),nn.Linear(320,100))\n",
    "adapt_net = nn.Sequential(n1,nn.Flatten())\n",
    "adapt_net.to('cuda:0')\n",
    "\n",
    "y = [torch.rand((4,40,32,32)).cuda(),torch.rand((4,80,16,16)).cuda(),torch.rand((4,160,8,8)).cuda()]\n",
    "adapt_net(y).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f2ae4ce3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "29aa4645",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "BNN.freeze()\n",
    "BNN.head_bn =  nn.BatchNorm1d(320 + 160)\n",
    "BNN.head = nn.Linear(320+160,5)\n",
    "BNN.add_UniAdapter(adapt_net)\n",
    "BNN = BNN.to('cuda:0')\n",
    "BNN.uniAdapt = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "30ef83bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No lr scheduler\n",
      "Optimizer Set to New: {'state': {}, 'param_groups': [{'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': False, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27]}]}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1hjtvtmv) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Current Best Acc</td><td>▁▄▆▇▇▇▇▇████████████████████████████████</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>epoch time (s)</td><td>▁████████████████████████████▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_accuracy</td><td>▁▄▆▇▇▇▇▇████████████████████████████████</td></tr><tr><td>test_loss</td><td>█▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>training_loss</td><td>█▆▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Best Accuracy</td><td>0.9378</td></tr><tr><td>Current Best Acc</td><td>0.9378</td></tr><tr><td>Total Time (hours)</td><td>1.08243</td></tr><tr><td>epoch</td><td>75</td></tr><tr><td>epoch time (s)</td><td>25.00121</td></tr><tr><td>lr</td><td>0.001</td></tr><tr><td>test_accuracy</td><td>0.9296</td></tr><tr><td>test_loss</td><td>0.19573</td></tr><tr><td>training_loss</td><td>0.1377</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">dulcet-paper-15</strong>: <a href=\"https://wandb.ai/johnny_suu/UniAdapt_Training_Encoder_Decoder/runs/1hjtvtmv\" target=\"_blank\">https://wandb.ai/johnny_suu/UniAdapt_Training_Encoder_Decoder/runs/1hjtvtmv</a><br/>Synced 5 W&B file(s), 1 media file(s), 1 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230328_105411-1hjtvtmv\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1hjtvtmv). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b61e0c57053446d08cfc643d709f412e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.14.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\John Su\\Downloads\\SydneyUni\\2022\\thesis\\Thesis\\wandb\\run-20230328_115827-ieugugyc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/johnny_suu/UniAdapt_Training_Encoder_Decoder/runs/ieugugyc\" target=\"_blank\">legendary-darkness-16</a></strong> to <a href=\"https://wandb.ai/johnny_suu/UniAdapt_Training_Encoder_Decoder\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "Project Name: UniAdapt_Training_Encoder_Decoder, Run Name BNN_Cifar10_finetune_encoder \n",
      "\n",
      "\n",
      "Run Start : 2023-03-28 11-58-27\n",
      "start_epoch : 0\n",
      "initial_lr : 0.001\n",
      "batch_size : 64\n",
      "epochs : 75\n",
      "epoch_chkpts : []\n",
      "device : cuda:0\n",
      "criterion : CrossEntropyLoss()\n",
      "optimizer : Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: False\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "seed : None\n",
      "model_architecture : <class 'NN_Thesis.nn_classes.BNN_Resnet_UniAdapt'>\n",
      "binerised_training : True\n",
      "Number of Elements : 4479980\n",
      "Initial accuracy:\n",
      "Test Accuracy : 29.0%, Test Loss: 1.7367503396080584\n",
      "best_acc.pth saved!\n",
      "Test Accuracy : 29.0%, Test Loss: 1.7449769128727008\n",
      "best_acc.pth saved!\n",
      "epoch: 1 average loss: 0.563\n",
      "Test Accuracy : 82.5%, Test Loss: 0.47561505550070654\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 15.55 seconds\n",
      "epoch: 2 average loss: 0.452\n",
      "Test Accuracy : 84.4%, Test Loss: 0.4352752158913431\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 15.52 seconds\n",
      "epoch: 3 average loss: 0.435\n",
      "Test Accuracy : 84.7%, Test Loss: 0.4233401398870009\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 15.48 seconds\n",
      "epoch: 4 average loss: 0.409\n",
      "Test Accuracy : 85.4%, Test Loss: 0.4133481426706797\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 15.79 seconds\n",
      "epoch: 5 average loss: 0.405\n",
      "Test Accuracy : 84.3%, Test Loss: 0.4188890940026392\n",
      "Epoch Time (Training + Test) = 15.66 seconds\n",
      "epoch: 6 average loss: 0.402\n",
      "Test Accuracy : 86.1%, Test Loss: 0.3989151789794994\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 15.71 seconds\n",
      "epoch: 7 average loss: 0.393\n",
      "Test Accuracy : 84.9%, Test Loss: 0.4175939167602153\n",
      "Epoch Time (Training + Test) = 15.82 seconds\n",
      "epoch: 8 average loss: 0.388\n",
      "Test Accuracy : 86.9%, Test Loss: 0.36187395189381855\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 15.57 seconds\n",
      "epoch: 9 average loss: 0.378\n",
      "Test Accuracy : 86.4%, Test Loss: 0.36944367010382156\n",
      "Epoch Time (Training + Test) = 15.63 seconds\n",
      "epoch: 10 average loss: 0.377\n",
      "Test Accuracy : 86.5%, Test Loss: 0.39240528680855713\n",
      "Epoch Time (Training + Test) = 15.63 seconds\n",
      "epoch: 11 average loss: 0.373\n",
      "Test Accuracy : 87.5%, Test Loss: 0.3573043876433674\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 15.77 seconds\n",
      "epoch: 12 average loss: 0.374\n",
      "Test Accuracy : 87.3%, Test Loss: 0.36156003422374966\n",
      "Epoch Time (Training + Test) = 15.55 seconds\n",
      "epoch: 13 average loss: 0.371\n",
      "Test Accuracy : 87.2%, Test Loss: 0.35804886116257195\n",
      "Epoch Time (Training + Test) = 15.49 seconds\n",
      "epoch: 14 average loss: 0.364\n",
      "Test Accuracy : 87.8%, Test Loss: 0.3455289509854739\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 15.38 seconds\n",
      "epoch: 15 average loss: 0.362\n",
      "Test Accuracy : 86.3%, Test Loss: 0.37618464099455484\n",
      "Epoch Time (Training + Test) = 15.32 seconds\n",
      "epoch: 16 average loss: 0.363\n",
      "Test Accuracy : 88.0%, Test Loss: 0.34194493878491317\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 15.42 seconds\n",
      "epoch: 17 average loss: 0.369\n",
      "Test Accuracy : 86.9%, Test Loss: 0.36259036746960654\n",
      "Epoch Time (Training + Test) = 15.36 seconds\n",
      "epoch: 18 average loss: 0.366\n",
      "Test Accuracy : 85.6%, Test Loss: 0.41538096964359283\n",
      "Epoch Time (Training + Test) = 15.31 seconds\n",
      "epoch: 19 average loss: 0.358\n",
      "Test Accuracy : 87.6%, Test Loss: 0.35344332777246645\n",
      "Epoch Time (Training + Test) = 15.41 seconds\n",
      "epoch: 20 average loss: 0.356\n",
      "Test Accuracy : 86.0%, Test Loss: 0.3892482951094833\n",
      "Epoch Time (Training + Test) = 15.71 seconds\n",
      "epoch: 21 average loss: 0.361\n",
      "Test Accuracy : 87.3%, Test Loss: 0.36221086158405374\n",
      "Epoch Time (Training + Test) = 15.61 seconds\n",
      "epoch: 22 average loss: 0.357\n",
      "Test Accuracy : 87.6%, Test Loss: 0.3548135018046898\n",
      "Epoch Time (Training + Test) = 15.50 seconds\n",
      "epoch: 23 average loss: 0.357\n",
      "Test Accuracy : 86.6%, Test Loss: 0.3669748662770549\n",
      "Epoch Time (Training + Test) = 15.35 seconds\n",
      "epoch: 24 average loss: 0.351\n",
      "Test Accuracy : 87.4%, Test Loss: 0.360293256519716\n",
      "Epoch Time (Training + Test) = 15.41 seconds\n",
      "epoch: 25 average loss: 0.355\n",
      "Test Accuracy : 88.2%, Test Loss: 0.33366770363306697\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 15.53 seconds\n",
      "epoch: 26 average loss: 0.350\n",
      "Test Accuracy : 87.6%, Test Loss: 0.3405633442386796\n",
      "Epoch Time (Training + Test) = 15.79 seconds\n",
      "epoch: 27 average loss: 0.347\n",
      "Test Accuracy : 87.3%, Test Loss: 0.3516959257518189\n",
      "Epoch Time (Training + Test) = 15.57 seconds\n",
      "epoch: 28 average loss: 0.347\n",
      "Test Accuracy : 87.0%, Test Loss: 0.36187320117709004\n",
      "Epoch Time (Training + Test) = 15.85 seconds\n",
      "epoch: 29 average loss: 0.349\n",
      "Test Accuracy : 86.4%, Test Loss: 0.385864596389517\n",
      "Epoch Time (Training + Test) = 15.59 seconds\n",
      "epoch: 30 average loss: 0.349\n",
      "Test Accuracy : 88.1%, Test Loss: 0.33555787782880325\n",
      "Epoch Time (Training + Test) = 15.71 seconds\n",
      "epoch: 31 average loss: 0.343\n",
      "Test Accuracy : 86.6%, Test Loss: 0.3609224920031391\n",
      "Epoch Time (Training + Test) = 15.78 seconds\n",
      "epoch: 32 average loss: 0.354\n",
      "Test Accuracy : 87.6%, Test Loss: 0.35218645726578146\n",
      "Epoch Time (Training + Test) = 15.71 seconds\n",
      "epoch: 33 average loss: 0.342\n",
      "Test Accuracy : 88.1%, Test Loss: 0.3428255106452145\n",
      "Epoch Time (Training + Test) = 15.72 seconds\n",
      "epoch: 34 average loss: 0.347\n",
      "Test Accuracy : 85.5%, Test Loss: 0.4122460852318172\n",
      "Epoch Time (Training + Test) = 15.77 seconds\n",
      "epoch: 35 average loss: 0.345\n",
      "Test Accuracy : 85.7%, Test Loss: 0.41072314656987974\n",
      "Epoch Time (Training + Test) = 15.75 seconds\n",
      "epoch: 36 average loss: 0.339\n",
      "Test Accuracy : 86.8%, Test Loss: 0.37118786007543153\n",
      "Epoch Time (Training + Test) = 15.70 seconds\n",
      "epoch: 37 average loss: 0.342\n",
      "Test Accuracy : 87.3%, Test Loss: 0.350981441479695\n",
      "Epoch Time (Training + Test) = 15.66 seconds\n",
      "epoch: 38 average loss: 0.341\n",
      "Test Accuracy : 86.5%, Test Loss: 0.3915873899867263\n",
      "Epoch Time (Training + Test) = 15.58 seconds\n",
      "epoch: 39 average loss: 0.344\n",
      "Test Accuracy : 87.6%, Test Loss: 0.3549415614408783\n",
      "Epoch Time (Training + Test) = 15.43 seconds\n",
      "epoch: 40 average loss: 0.340\n",
      "Test Accuracy : 86.8%, Test Loss: 0.3620192368951025\n",
      "Epoch Time (Training + Test) = 15.62 seconds\n",
      "epoch: 41 average loss: 0.341\n",
      "Test Accuracy : 88.3%, Test Loss: 0.3396066081108926\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 15.38 seconds\n",
      "epoch: 42 average loss: 0.337\n",
      "Test Accuracy : 87.8%, Test Loss: 0.3385701988694034\n",
      "Epoch Time (Training + Test) = 15.27 seconds\n",
      "epoch: 43 average loss: 0.340\n",
      "Test Accuracy : 87.7%, Test Loss: 0.3359102051280722\n",
      "Epoch Time (Training + Test) = 15.32 seconds\n",
      "epoch: 44 average loss: 0.345\n",
      "Test Accuracy : 88.6%, Test Loss: 0.3257351863799216\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 15.48 seconds\n",
      "epoch: 45 average loss: 0.337\n",
      "Test Accuracy : 87.6%, Test Loss: 0.33789813245021844\n",
      "Epoch Time (Training + Test) = 15.47 seconds\n",
      "epoch: 46 average loss: 0.338\n",
      "Test Accuracy : 86.6%, Test Loss: 0.3697126666201821\n",
      "Epoch Time (Training + Test) = 15.67 seconds\n",
      "epoch: 47 average loss: 0.345\n",
      "Test Accuracy : 88.3%, Test Loss: 0.3281116241329833\n",
      "Epoch Time (Training + Test) = 15.76 seconds\n",
      "epoch: 48 average loss: 0.333\n",
      "Test Accuracy : 87.2%, Test Loss: 0.37928584406647503\n",
      "Epoch Time (Training + Test) = 15.67 seconds\n",
      "epoch: 49 average loss: 0.338\n",
      "Test Accuracy : 88.1%, Test Loss: 0.33580919704105283\n",
      "Epoch Time (Training + Test) = 15.51 seconds\n",
      "epoch: 50 average loss: 0.338\n",
      "Test Accuracy : 87.9%, Test Loss: 0.3398325843147085\n",
      "Epoch Time (Training + Test) = 15.32 seconds\n",
      "epoch: 51 average loss: 0.335\n",
      "Test Accuracy : 87.8%, Test Loss: 0.3413086910791035\n",
      "Epoch Time (Training + Test) = 15.26 seconds\n",
      "epoch: 52 average loss: 0.334\n",
      "Test Accuracy : 87.3%, Test Loss: 0.3642884447604795\n",
      "Epoch Time (Training + Test) = 15.26 seconds\n",
      "epoch: 53 average loss: 0.332\n",
      "Test Accuracy : 87.8%, Test Loss: 0.3455367384454872\n",
      "Epoch Time (Training + Test) = 15.32 seconds\n",
      "epoch: 54 average loss: 0.335\n",
      "Test Accuracy : 88.1%, Test Loss: 0.33631629128999346\n",
      "Epoch Time (Training + Test) = 15.32 seconds\n",
      "epoch: 55 average loss: 0.332\n",
      "Test Accuracy : 87.7%, Test Loss: 0.3506806089153773\n",
      "Epoch Time (Training + Test) = 15.40 seconds\n",
      "epoch: 56 average loss: 0.330\n",
      "Test Accuracy : 88.7%, Test Loss: 0.32774208957635903\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 15.60 seconds\n",
      "epoch: 57 average loss: 0.334\n",
      "Test Accuracy : 87.1%, Test Loss: 0.36672476858277864\n",
      "Epoch Time (Training + Test) = 15.67 seconds\n",
      "epoch: 58 average loss: 0.333\n",
      "Test Accuracy : 86.6%, Test Loss: 0.37216941842549967\n",
      "Epoch Time (Training + Test) = 15.70 seconds\n",
      "epoch: 59 average loss: 0.330\n",
      "Test Accuracy : 86.4%, Test Loss: 0.3774733924413029\n",
      "Epoch Time (Training + Test) = 15.72 seconds\n",
      "epoch: 60 average loss: 0.333\n",
      "Test Accuracy : 87.5%, Test Loss: 0.3564380554458763\n",
      "Epoch Time (Training + Test) = 15.46 seconds\n",
      "epoch: 61 average loss: 0.331\n",
      "Test Accuracy : 87.6%, Test Loss: 0.34584346243852304\n",
      "Epoch Time (Training + Test) = 15.51 seconds\n",
      "epoch: 62 average loss: 0.327\n",
      "Test Accuracy : 88.1%, Test Loss: 0.3403699912979633\n",
      "Epoch Time (Training + Test) = 15.71 seconds\n",
      "epoch: 63 average loss: 0.329\n",
      "Test Accuracy : 87.9%, Test Loss: 0.3404252266016188\n",
      "Epoch Time (Training + Test) = 15.65 seconds\n",
      "epoch: 64 average loss: 0.328\n",
      "Test Accuracy : 88.5%, Test Loss: 0.32153938691827316\n",
      "Epoch Time (Training + Test) = 15.50 seconds\n",
      "epoch: 65 average loss: 0.330\n",
      "Test Accuracy : 88.1%, Test Loss: 0.3336382491301887\n",
      "Epoch Time (Training + Test) = 15.38 seconds\n",
      "epoch: 66 average loss: 0.334\n",
      "Test Accuracy : 88.3%, Test Loss: 0.33186915073590945\n",
      "Epoch Time (Training + Test) = 15.50 seconds\n",
      "epoch: 67 average loss: 0.329\n",
      "Test Accuracy : 87.9%, Test Loss: 0.33112771733652185\n",
      "Epoch Time (Training + Test) = 15.65 seconds\n",
      "epoch: 68 average loss: 0.329\n",
      "Test Accuracy : 88.1%, Test Loss: 0.3269150537214702\n",
      "Epoch Time (Training + Test) = 15.77 seconds\n",
      "epoch: 69 average loss: 0.332\n",
      "Test Accuracy : 87.3%, Test Loss: 0.3554901519342314\n",
      "Epoch Time (Training + Test) = 15.50 seconds\n",
      "epoch: 70 average loss: 0.326\n",
      "Test Accuracy : 88.5%, Test Loss: 0.3271042353744748\n",
      "Epoch Time (Training + Test) = 15.44 seconds\n",
      "epoch: 71 average loss: 0.328\n",
      "Test Accuracy : 86.4%, Test Loss: 0.39108596893051\n",
      "Epoch Time (Training + Test) = 15.47 seconds\n",
      "epoch: 72 average loss: 0.328\n",
      "Test Accuracy : 88.2%, Test Loss: 0.33526463523695743\n",
      "Epoch Time (Training + Test) = 15.62 seconds\n",
      "epoch: 73 average loss: 0.329\n",
      "Test Accuracy : 87.7%, Test Loss: 0.3573869566751432\n",
      "Epoch Time (Training + Test) = 15.93 seconds\n",
      "epoch: 74 average loss: 0.325\n",
      "Test Accuracy : 86.5%, Test Loss: 0.39514936902855013\n",
      "Epoch Time (Training + Test) = 15.67 seconds\n",
      "epoch: 75 average loss: 0.331\n",
      "Test Accuracy : 87.9%, Test Loss: 0.3497168053931828\n",
      "Epoch Time (Training + Test) = 15.40 seconds\n",
      "Data Saved to BNN_Cifar10_finetune_encoder.csv\n",
      "Finished Training: \n",
      "Total Time 0.648320 hours\n",
      " Average Time Per Epoch 31.12 seconds\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(BNN,model_name='BNN_Cifar10_finetune_encoder',project_name='UniAdapt_Training_Encoder_Decoder',binarise=True,classes=classes_59)\n",
    "trainer.lr = 1e-3\n",
    "trainer.batch_size = 64\n",
    "trainer.epochs =75\n",
    "trainer.epoch_chkpts = []\n",
    "trainer.start_epoch = 0\n",
    "params = nn.ModuleList([BNN.uniAdaptNet,BNN.head_bn,BNN.head]).parameters()\n",
    "trainer.set_scheduler(None)\n",
    "trainer.set_optimizer(torch.optim.Adam,params)\n",
    "trainer.train(train59_loader,test59_loader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "adb38c36",
   "metadata": {},
   "source": [
    "# Scaling Factor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f5711bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'SavedModels\\UniAdapt_Training_Encoder_Decoder\\BNN_Cifar10_Backbone_Cifar04_2023-03-27 16-19-04\\BNN_Cifar10_Backbone_Cifar04_best_acc.pth'\n",
    "\n",
    "state = torch.load(path,map_location='cuda:0')\n",
    "BNN = BNN_Resnet_UniAdapt(5,freeze_pre_weights=True)\n",
    "BNN.layer3[-1].do_bntan = True\n",
    "# for key in state.keys():\n",
    "#     if 'decoders' in key or 'encoders' in key:\n",
    "#         del state[key] \n",
    "\n",
    "BNN.load_state_dict(state,strict=False)\n",
    "BNN.freeze()\n",
    "\n",
    "BNN = BNN.to('cuda:0')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "639ef933",
   "metadata": {},
   "outputs": [],
   "source": [
    "class placeholder(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return x\n",
    "\n",
    "BNN.encoder_net = placeholder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4ab484f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class scaling_factor(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.scalar = nn.Parameter(data = torch.rand(1))\n",
    "    def forward(self,x):\n",
    "        return self.scalar*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b241aad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 320])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n1 = uniAdapt_Net([80,160,320],[80,160,320],block = thinBlock2)\n",
    "n1 = n1.cuda()\n",
    "n1.train()\n",
    "# adapt_net = nn.Sequential(n1,nn.AdaptiveMaxPool2d(1),nn.Flatten(),nn.Linear(320,100))\n",
    "adapt_net = nn.Sequential(n1,nn.Flatten(),scaling_factor())\n",
    "adapt_net.to('cuda:0')\n",
    "\n",
    "y = [torch.rand((4,80,32,32)).cuda(),torch.rand((4,160,16,16)).cuda(),torch.rand((4,320,8,8)).cuda()]\n",
    "adapt_net(y).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a757460e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "BNN.freeze()\n",
    "BNN.head_bn =  placeholder()\n",
    "BNN.head =  nn.Linear(320 + 320,5)\n",
    "BNN.add_UniAdapter(adapt_net)\n",
    "BNN = BNN.to('cuda:0')\n",
    "BNN.uniAdapt = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68774430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No lr scheduler\n",
      "Optimizer Set to New: {'state': {}, 'param_groups': [{'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': False, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26]}]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjohnny_suu\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.14.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\John Su\\Downloads\\SydneyUni\\2022\\thesis\\Thesis\\wandb\\run-20230328_103154-2m2v4wbu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/johnny_suu/UniAdapt_Training_Encoder_Decoder/runs/2m2v4wbu\" target=\"_blank\">royal-wave-13</a></strong> to <a href=\"https://wandb.ai/johnny_suu/UniAdapt_Training_Encoder_Decoder\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "Project Name: UniAdapt_Training_Encoder_Decoder, Run Name BNN_Cifar10_finetune_no_Encoder_scaling_factor_no_bn \n",
      "\n",
      "\n",
      "Run Start : 2023-03-28 10-31-52\n",
      "start_epoch : 0\n",
      "initial_lr : 0.001\n",
      "batch_size : 64\n",
      "epochs : 75\n",
      "epoch_chkpts : []\n",
      "device : cuda:0\n",
      "criterion : CrossEntropyLoss()\n",
      "optimizer : Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: False\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "seed : None\n",
      "model_architecture : <class 'NN_Thesis.nn_classes.BNN_Resnet_UniAdapt'>\n",
      "binerised_training : True\n",
      "Number of Elements : 4349541\n",
      "Initial accuracy:\n",
      "Test Accuracy : 8.1%, Test Loss: 1.8555544267225144\n",
      "best_acc.pth saved!\n",
      "Test Accuracy : 8.7%, Test Loss: 1.8640522745591175\n",
      "best_acc.pth saved!\n",
      "epoch: 1 average loss: 0.595\n",
      "Test Accuracy : 83.0%, Test Loss: 0.46201721060125134\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 17.08 seconds\n",
      "epoch: 2 average loss: 0.451\n",
      "Test Accuracy : 85.8%, Test Loss: 0.3981338382899007\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 17.07 seconds\n",
      "epoch: 3 average loss: 0.409\n",
      "Test Accuracy : 86.1%, Test Loss: 0.3742979372603984\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 16.81 seconds\n",
      "epoch: 4 average loss: 0.388\n",
      "Test Accuracy : 87.1%, Test Loss: 0.3669791751647297\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 16.88 seconds\n",
      "epoch: 5 average loss: 0.376\n",
      "Test Accuracy : 87.1%, Test Loss: 0.35415815307369714\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 16.79 seconds\n",
      "epoch: 6 average loss: 0.362\n",
      "Test Accuracy : 87.2%, Test Loss: 0.34538995691492586\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 16.41 seconds\n",
      "epoch: 7 average loss: 0.353\n",
      "Test Accuracy : 87.7%, Test Loss: 0.3390366501068767\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 16.32 seconds\n",
      "epoch: 8 average loss: 0.355\n",
      "Test Accuracy : 87.5%, Test Loss: 0.3426564697977863\n",
      "Epoch Time (Training + Test) = 16.29 seconds\n",
      "epoch: 9 average loss: 0.343\n",
      "Test Accuracy : 88.0%, Test Loss: 0.3285469073283521\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 16.36 seconds\n",
      "epoch: 10 average loss: 0.338\n",
      "Test Accuracy : 88.7%, Test Loss: 0.3265889239839361\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 16.32 seconds\n",
      "epoch: 11 average loss: 0.337\n",
      "Test Accuracy : 88.9%, Test Loss: 0.3199808514570888\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 16.38 seconds\n",
      "epoch: 12 average loss: 0.327\n",
      "Test Accuracy : 88.5%, Test Loss: 0.32720511482109\n",
      "Epoch Time (Training + Test) = 16.31 seconds\n",
      "epoch: 13 average loss: 0.326\n",
      "Test Accuracy : 88.6%, Test Loss: 0.31808576591407195\n",
      "Epoch Time (Training + Test) = 16.32 seconds\n",
      "epoch: 14 average loss: 0.326\n",
      "Test Accuracy : 88.4%, Test Loss: 0.33018609928556636\n",
      "Epoch Time (Training + Test) = 16.30 seconds\n",
      "epoch: 15 average loss: 0.325\n",
      "Test Accuracy : 88.8%, Test Loss: 0.3088998060814942\n",
      "Epoch Time (Training + Test) = 16.31 seconds\n",
      "epoch: 16 average loss: 0.326\n",
      "Test Accuracy : 89.5%, Test Loss: 0.30157678480012506\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 16.30 seconds\n",
      "epoch: 17 average loss: 0.316\n",
      "Test Accuracy : 89.4%, Test Loss: 0.3018502159209191\n",
      "Epoch Time (Training + Test) = 16.29 seconds\n",
      "epoch: 18 average loss: 0.315\n",
      "Test Accuracy : 89.6%, Test Loss: 0.2984999077795427\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 16.35 seconds\n",
      "epoch: 19 average loss: 0.313\n",
      "Test Accuracy : 89.4%, Test Loss: 0.3066846826408483\n",
      "Epoch Time (Training + Test) = 16.41 seconds\n",
      "epoch: 20 average loss: 0.311\n",
      "Test Accuracy : 89.4%, Test Loss: 0.30242460192758824\n",
      "Epoch Time (Training + Test) = 16.57 seconds\n",
      "epoch: 21 average loss: 0.315\n",
      "Test Accuracy : 89.1%, Test Loss: 0.29516197231751456\n",
      "Epoch Time (Training + Test) = 16.33 seconds\n",
      "epoch: 22 average loss: 0.313\n",
      "Test Accuracy : 89.5%, Test Loss: 0.29505410413198835\n",
      "Epoch Time (Training + Test) = 16.31 seconds\n",
      "epoch: 23 average loss: 0.308\n",
      "Test Accuracy : 89.5%, Test Loss: 0.29903130940621414\n",
      "Epoch Time (Training + Test) = 16.32 seconds\n",
      "epoch: 24 average loss: 0.307\n",
      "Test Accuracy : 89.4%, Test Loss: 0.29640256613492966\n",
      "Epoch Time (Training + Test) = 16.43 seconds\n",
      "epoch: 25 average loss: 0.306\n",
      "Test Accuracy : 89.6%, Test Loss: 0.297884016474591\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 16.38 seconds\n",
      "epoch: 26 average loss: 0.304\n",
      "Test Accuracy : 89.4%, Test Loss: 0.3003114794817152\n",
      "Epoch Time (Training + Test) = 16.31 seconds\n",
      "epoch: 27 average loss: 0.306\n",
      "Test Accuracy : 88.4%, Test Loss: 0.316595036112055\n",
      "Epoch Time (Training + Test) = 16.32 seconds\n",
      "epoch: 28 average loss: 0.301\n",
      "Test Accuracy : 89.5%, Test Loss: 0.30159522979697095\n",
      "Epoch Time (Training + Test) = 16.32 seconds\n",
      "epoch: 29 average loss: 0.302\n",
      "Test Accuracy : 89.9%, Test Loss: 0.28118036196956153\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 16.33 seconds\n",
      "epoch: 30 average loss: 0.305\n",
      "Test Accuracy : 89.8%, Test Loss: 0.28849312521611586\n",
      "Epoch Time (Training + Test) = 16.31 seconds\n",
      "epoch: 31 average loss: 0.299\n",
      "Test Accuracy : 89.3%, Test Loss: 0.2981520445097851\n",
      "Epoch Time (Training + Test) = 16.35 seconds\n",
      "epoch: 32 average loss: 0.295\n",
      "Test Accuracy : 89.5%, Test Loss: 0.2957939780965636\n",
      "Epoch Time (Training + Test) = 16.37 seconds\n",
      "epoch: 33 average loss: 0.289\n",
      "Test Accuracy : 89.7%, Test Loss: 0.2876064254890514\n",
      "Epoch Time (Training + Test) = 16.31 seconds\n",
      "epoch: 34 average loss: 0.298\n",
      "Test Accuracy : 89.8%, Test Loss: 0.29130446835409235\n",
      "Epoch Time (Training + Test) = 16.29 seconds\n",
      "epoch: 35 average loss: 0.298\n",
      "Test Accuracy : 89.8%, Test Loss: 0.2889075121736225\n",
      "Epoch Time (Training + Test) = 16.28 seconds\n",
      "epoch: 36 average loss: 0.299\n",
      "Test Accuracy : 89.6%, Test Loss: 0.28427606264624417\n",
      "Epoch Time (Training + Test) = 16.32 seconds\n",
      "epoch: 37 average loss: 0.291\n",
      "Test Accuracy : 89.5%, Test Loss: 0.28376469185835196\n",
      "Epoch Time (Training + Test) = 16.30 seconds\n",
      "epoch: 38 average loss: 0.286\n",
      "Test Accuracy : 89.6%, Test Loss: 0.29458014601016347\n",
      "Epoch Time (Training + Test) = 16.36 seconds\n",
      "epoch: 39 average loss: 0.295\n",
      "Test Accuracy : 90.1%, Test Loss: 0.27685176222761976\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 16.34 seconds\n",
      "epoch: 40 average loss: 0.291\n",
      "Test Accuracy : 89.9%, Test Loss: 0.28233686919453777\n",
      "Epoch Time (Training + Test) = 16.31 seconds\n",
      "epoch: 41 average loss: 0.290\n",
      "Test Accuracy : 90.3%, Test Loss: 0.27488122937045517\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 16.32 seconds\n",
      "epoch: 42 average loss: 0.287\n",
      "Test Accuracy : 90.2%, Test Loss: 0.2843316650088829\n",
      "Epoch Time (Training + Test) = 16.30 seconds\n",
      "epoch: 43 average loss: 0.288\n",
      "Test Accuracy : 90.3%, Test Loss: 0.28178697117144547\n",
      "Epoch Time (Training + Test) = 16.31 seconds\n",
      "epoch: 44 average loss: 0.289\n",
      "Test Accuracy : 88.6%, Test Loss: 0.31124407562273965\n",
      "Epoch Time (Training + Test) = 16.34 seconds\n",
      "epoch: 45 average loss: 0.288\n",
      "Test Accuracy : 90.0%, Test Loss: 0.28078174393011046\n",
      "Epoch Time (Training + Test) = 16.36 seconds\n",
      "epoch: 46 average loss: 0.286\n",
      "Test Accuracy : 89.9%, Test Loss: 0.27844603576614885\n",
      "Epoch Time (Training + Test) = 16.30 seconds\n",
      "epoch: 47 average loss: 0.285\n",
      "Test Accuracy : 90.1%, Test Loss: 0.2838860383328003\n",
      "Epoch Time (Training + Test) = 16.39 seconds\n",
      "epoch: 48 average loss: 0.284\n",
      "Test Accuracy : 90.3%, Test Loss: 0.2850851030855239\n",
      "Epoch Time (Training + Test) = 16.38 seconds\n",
      "epoch: 49 average loss: 0.286\n",
      "Test Accuracy : 89.8%, Test Loss: 0.290640744038775\n",
      "Epoch Time (Training + Test) = 16.34 seconds\n",
      "epoch: 50 average loss: 0.287\n",
      "Test Accuracy : 90.1%, Test Loss: 0.2865468539957759\n",
      "Epoch Time (Training + Test) = 16.34 seconds\n",
      "epoch: 51 average loss: 0.276\n",
      "Test Accuracy : 90.3%, Test Loss: 0.277735748057124\n",
      "Epoch Time (Training + Test) = 16.36 seconds\n",
      "epoch: 52 average loss: 0.279\n",
      "Test Accuracy : 89.7%, Test Loss: 0.2819451701980603\n",
      "Epoch Time (Training + Test) = 16.37 seconds\n",
      "epoch: 53 average loss: 0.278\n",
      "Test Accuracy : 89.7%, Test Loss: 0.29164212357394304\n",
      "Epoch Time (Training + Test) = 16.33 seconds\n",
      "epoch: 54 average loss: 0.285\n",
      "Test Accuracy : 89.2%, Test Loss: 0.2930985306259952\n",
      "Epoch Time (Training + Test) = 16.35 seconds\n",
      "epoch: 55 average loss: 0.280\n",
      "Test Accuracy : 90.2%, Test Loss: 0.27688337315486955\n",
      "Epoch Time (Training + Test) = 16.34 seconds\n",
      "epoch: 56 average loss: 0.278\n",
      "Test Accuracy : 90.2%, Test Loss: 0.2704270438680166\n",
      "Epoch Time (Training + Test) = 16.32 seconds\n",
      "epoch: 57 average loss: 0.278\n",
      "Test Accuracy : 89.3%, Test Loss: 0.2900537923921513\n",
      "Epoch Time (Training + Test) = 16.33 seconds\n",
      "epoch: 58 average loss: 0.275\n",
      "Test Accuracy : 89.9%, Test Loss: 0.280548594892025\n",
      "Epoch Time (Training + Test) = 16.35 seconds\n",
      "epoch: 59 average loss: 0.278\n",
      "Test Accuracy : 89.1%, Test Loss: 0.31115826313631445\n",
      "Epoch Time (Training + Test) = 16.35 seconds\n",
      "epoch: 60 average loss: 0.279\n",
      "Test Accuracy : 90.0%, Test Loss: 0.2829168906694726\n",
      "Epoch Time (Training + Test) = 16.34 seconds\n",
      "epoch: 61 average loss: 0.277\n",
      "Test Accuracy : 90.4%, Test Loss: 0.2839980099020125\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 16.47 seconds\n",
      "epoch: 62 average loss: 0.278\n",
      "Test Accuracy : 90.3%, Test Loss: 0.27457353727349754\n",
      "Epoch Time (Training + Test) = 16.32 seconds\n",
      "epoch: 63 average loss: 0.273\n",
      "Test Accuracy : 90.5%, Test Loss: 0.28147720913343793\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 16.38 seconds\n",
      "epoch: 64 average loss: 0.276\n",
      "Test Accuracy : 89.7%, Test Loss: 0.2832205420053458\n",
      "Epoch Time (Training + Test) = 16.34 seconds\n",
      "epoch: 65 average loss: 0.270\n",
      "Test Accuracy : 90.1%, Test Loss: 0.28847464966245845\n",
      "Epoch Time (Training + Test) = 16.34 seconds\n",
      "epoch: 66 average loss: 0.276\n",
      "Test Accuracy : 90.5%, Test Loss: 0.2700304792651647\n",
      "Epoch Time (Training + Test) = 16.34 seconds\n",
      "epoch: 67 average loss: 0.275\n",
      "Test Accuracy : 90.9%, Test Loss: 0.26460052427800396\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 16.36 seconds\n",
      "epoch: 68 average loss: 0.275\n",
      "Test Accuracy : 90.1%, Test Loss: 0.2911430313428746\n",
      "Epoch Time (Training + Test) = 16.34 seconds\n",
      "epoch: 69 average loss: 0.274\n",
      "Test Accuracy : 89.9%, Test Loss: 0.292652349777614\n",
      "Epoch Time (Training + Test) = 16.30 seconds\n",
      "epoch: 70 average loss: 0.275\n",
      "Test Accuracy : 89.5%, Test Loss: 0.2951537373511097\n",
      "Epoch Time (Training + Test) = 16.32 seconds\n",
      "epoch: 71 average loss: 0.272\n",
      "Test Accuracy : 89.4%, Test Loss: 0.3056219913348367\n",
      "Epoch Time (Training + Test) = 16.34 seconds\n",
      "epoch: 72 average loss: 0.273\n",
      "Test Accuracy : 90.5%, Test Loss: 0.2746723937460139\n",
      "Epoch Time (Training + Test) = 16.85 seconds\n",
      "epoch: 73 average loss: 0.274\n",
      "Test Accuracy : 89.8%, Test Loss: 0.28851644232680523\n",
      "Epoch Time (Training + Test) = 17.07 seconds\n",
      "epoch: 74 average loss: 0.274\n",
      "Test Accuracy : 90.4%, Test Loss: 0.26196893596950965\n",
      "Epoch Time (Training + Test) = 17.11 seconds\n",
      "epoch: 75 average loss: 0.276\n",
      "Test Accuracy : 90.1%, Test Loss: 0.2775015230231647\n",
      "Epoch Time (Training + Test) = 16.75 seconds\n",
      "Data Saved to BNN_Cifar10_finetune_no_Encoder_scaling_factor_no_bn.csv\n",
      "Finished Training: \n",
      "Total Time 0.683843 hours\n",
      " Average Time Per Epoch 32.82 seconds\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(BNN,model_name='BNN_Cifar10_finetune_no_Encoder_scaling_factor_no_bn',project_name='UniAdapt_Training_Encoder_Decoder',binarise=True,classes=classes_59)\n",
    "trainer.lr = 1e-3\n",
    "trainer.batch_size = 64\n",
    "trainer.epochs =75\n",
    "trainer.epoch_chkpts = []\n",
    "trainer.start_epoch = 0\n",
    "params = nn.ModuleList([BNN.uniAdaptNet,BNN.head_bn,BNN.head]).parameters()\n",
    "trainer.set_scheduler(None)\n",
    "trainer.set_optimizer(torch.optim.Adam,params,lr = 1e-3)\n",
    "trainer.train(train59_loader,test59_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "13b5e0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'SavedModels\\UniAdapt_Training_Encoder_Decoder\\BNN_Cifar10_Backbone_Cifar04_2023-03-27 16-19-04\\BNN_Cifar10_Backbone_Cifar04_best_acc.pth'\n",
    "\n",
    "state = torch.load(path,map_location='cuda:0')\n",
    "BNN = BNN_Resnet_UniAdapt(5,freeze_pre_weights=False)\n",
    "BNN.layer3[-1].do_bntan = True\n",
    "# for key in state.keys():\n",
    "#     if 'decoders' in key or 'encoders' in key:\n",
    "#         del state[key] \n",
    "\n",
    "BNN.load_state_dict(state,strict=False)\n",
    "BNN = BNN.to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "da55f067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No lr scheduler\n",
      "Optimizer Set to New: {'state': {}, 'param_groups': [{'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': False, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54]}]}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:26nqmmde) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a9b2bfcd6a042c283d6ef050c3362dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.006 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.120309…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Current Best Acc</td><td>▁</td></tr><tr><td>epoch</td><td>▁</td></tr><tr><td>epoch time (s)</td><td>▁</td></tr><tr><td>lr</td><td>▁</td></tr><tr><td>test_accuracy</td><td>▁</td></tr><tr><td>test_loss</td><td>▁</td></tr><tr><td>training_loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Current Best Acc</td><td>0.0414</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>epoch time (s)</td><td>11.87827</td></tr><tr><td>lr</td><td>0.001</td></tr><tr><td>test_accuracy</td><td>0.0414</td></tr><tr><td>test_loss</td><td>8.09316</td></tr><tr><td>training_loss</td><td>8.02099</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">autumn-disco-14</strong>: <a href=\"https://wandb.ai/johnny_suu/UniAdapt_Training_Encoder_Decoder/runs/26nqmmde\" target=\"_blank\">https://wandb.ai/johnny_suu/UniAdapt_Training_Encoder_Decoder/runs/26nqmmde</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230328_105320-26nqmmde\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:26nqmmde). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00ae6c325f574579bd5ebfd1d533dc6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016916666666899498, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.14.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\John Su\\Downloads\\SydneyUni\\2022\\thesis\\Thesis\\wandb\\run-20230328_105411-1hjtvtmv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/johnny_suu/UniAdapt_Training_Encoder_Decoder/runs/1hjtvtmv\" target=\"_blank\">dulcet-paper-15</a></strong> to <a href=\"https://wandb.ai/johnny_suu/UniAdapt_Training_Encoder_Decoder\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "Project Name: UniAdapt_Training_Encoder_Decoder, Run Name BNN_Cifar10_Regular_Finetune \n",
      "\n",
      "\n",
      "Run Start : 2023-03-28 10-54-11\n",
      "start_epoch : 0\n",
      "initial_lr : 0.001\n",
      "batch_size : 64\n",
      "epochs : 75\n",
      "epoch_chkpts : []\n",
      "device : cuda:0\n",
      "criterion : CrossEntropyLoss()\n",
      "optimizer : Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: False\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "seed : None\n",
      "model_architecture : <class 'NN_Thesis.nn_classes.BNN_Resnet_UniAdapt'>\n",
      "binerised_training : True\n",
      "Number of Elements : 4339780\n",
      "Initial accuracy:\n",
      "Test Accuracy : 4.2%, Test Loss: 8.030213656023031\n",
      "best_acc.pth saved!\n",
      "Test Accuracy : 4.1%, Test Loss: 8.093156693856928\n",
      "epoch: 1 average loss: 5.301\n",
      "Test Accuracy : 37.6%, Test Loss: 2.2666201531132564\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 26.58 seconds\n",
      "epoch: 2 average loss: 1.599\n",
      "Test Accuracy : 57.8%, Test Loss: 1.2085598880731607\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 26.47 seconds\n",
      "epoch: 3 average loss: 1.095\n",
      "Test Accuracy : 67.6%, Test Loss: 0.9145914519889445\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 26.84 seconds\n",
      "epoch: 4 average loss: 0.872\n",
      "Test Accuracy : 73.6%, Test Loss: 0.7482009760186642\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 26.40 seconds\n",
      "epoch: 5 average loss: 0.752\n",
      "Test Accuracy : 76.3%, Test Loss: 0.6681623025031029\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 26.27 seconds\n",
      "epoch: 6 average loss: 0.667\n",
      "Test Accuracy : 78.9%, Test Loss: 0.5818518271929101\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 26.43 seconds\n",
      "epoch: 7 average loss: 0.602\n",
      "Test Accuracy : 80.8%, Test Loss: 0.5304281172118609\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 26.32 seconds\n",
      "epoch: 8 average loss: 0.567\n",
      "Test Accuracy : 82.4%, Test Loss: 0.49586836144893986\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 26.28 seconds\n",
      "epoch: 9 average loss: 0.523\n",
      "Test Accuracy : 83.1%, Test Loss: 0.4734016256996348\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 26.29 seconds\n",
      "epoch: 10 average loss: 0.490\n",
      "Test Accuracy : 83.5%, Test Loss: 0.4440678241132181\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 26.38 seconds\n",
      "epoch: 11 average loss: 0.466\n",
      "Test Accuracy : 84.4%, Test Loss: 0.4307081282516069\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 26.31 seconds\n",
      "epoch: 12 average loss: 0.439\n",
      "Test Accuracy : 86.0%, Test Loss: 0.39146591949311993\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 26.23 seconds\n",
      "epoch: 13 average loss: 0.422\n",
      "Test Accuracy : 86.0%, Test Loss: 0.3802458736338193\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 26.33 seconds\n",
      "epoch: 14 average loss: 0.398\n",
      "Test Accuracy : 87.0%, Test Loss: 0.3642171325185631\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 26.25 seconds\n",
      "epoch: 15 average loss: 0.391\n",
      "Test Accuracy : 87.9%, Test Loss: 0.34266670727277104\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 26.32 seconds\n",
      "epoch: 16 average loss: 0.372\n",
      "Test Accuracy : 88.1%, Test Loss: 0.33009467943559717\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 26.30 seconds\n",
      "epoch: 17 average loss: 0.365\n",
      "Test Accuracy : 88.5%, Test Loss: 0.3193479821651797\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 26.26 seconds\n",
      "epoch: 18 average loss: 0.354\n",
      "Test Accuracy : 88.9%, Test Loss: 0.3116555283718471\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 26.46 seconds\n",
      "epoch: 19 average loss: 0.342\n",
      "Test Accuracy : 89.4%, Test Loss: 0.30237444122380847\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 26.39 seconds\n",
      "epoch: 20 average loss: 0.331\n",
      "Test Accuracy : 89.0%, Test Loss: 0.29427227019509183\n",
      "Epoch Time (Training + Test) = 26.52 seconds\n",
      "epoch: 21 average loss: 0.321\n",
      "Test Accuracy : 89.2%, Test Loss: 0.29792484942870806\n",
      "Epoch Time (Training + Test) = 26.59 seconds\n",
      "epoch: 22 average loss: 0.312\n",
      "Test Accuracy : 89.7%, Test Loss: 0.28683404633893245\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 26.60 seconds\n",
      "epoch: 23 average loss: 0.307\n",
      "Test Accuracy : 88.6%, Test Loss: 0.30826731686350667\n",
      "Epoch Time (Training + Test) = 26.57 seconds\n",
      "epoch: 24 average loss: 0.304\n",
      "Test Accuracy : 90.0%, Test Loss: 0.28880659651152696\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 26.53 seconds\n",
      "epoch: 25 average loss: 0.294\n",
      "Test Accuracy : 89.9%, Test Loss: 0.2730900803698769\n",
      "Epoch Time (Training + Test) = 26.44 seconds\n",
      "epoch: 26 average loss: 0.284\n",
      "Test Accuracy : 89.7%, Test Loss: 0.28271482601950443\n",
      "Epoch Time (Training + Test) = 26.41 seconds\n",
      "epoch: 27 average loss: 0.277\n",
      "Test Accuracy : 90.5%, Test Loss: 0.2603364382174951\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 26.42 seconds\n",
      "epoch: 28 average loss: 0.269\n",
      "Test Accuracy : 90.3%, Test Loss: 0.26502657313890093\n",
      "Epoch Time (Training + Test) = 26.38 seconds\n",
      "epoch: 29 average loss: 0.266\n",
      "Test Accuracy : 90.3%, Test Loss: 0.2726120172610766\n",
      "Epoch Time (Training + Test) = 26.43 seconds\n",
      "epoch: 30 average loss: 0.262\n",
      "Test Accuracy : 91.1%, Test Loss: 0.24997513694099233\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 26.41 seconds\n",
      "epoch: 31 average loss: 0.253\n",
      "Test Accuracy : 91.2%, Test Loss: 0.24651127761300606\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 26.48 seconds\n",
      "epoch: 32 average loss: 0.254\n",
      "Test Accuracy : 91.5%, Test Loss: 0.24548102189091187\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 26.34 seconds\n",
      "epoch: 33 average loss: 0.252\n",
      "Test Accuracy : 91.5%, Test Loss: 0.24150497334290155\n",
      "Epoch Time (Training + Test) = 26.45 seconds\n",
      "epoch: 34 average loss: 0.246\n",
      "Test Accuracy : 91.1%, Test Loss: 0.24556639262392552\n",
      "Epoch Time (Training + Test) = 26.46 seconds\n",
      "epoch: 35 average loss: 0.240\n",
      "Test Accuracy : 91.5%, Test Loss: 0.23231605619569368\n",
      "Epoch Time (Training + Test) = 26.39 seconds\n",
      "epoch: 36 average loss: 0.239\n",
      "Test Accuracy : 90.4%, Test Loss: 0.2644016823813885\n",
      "Epoch Time (Training + Test) = 26.45 seconds\n",
      "epoch: 37 average loss: 0.233\n",
      "Test Accuracy : 91.3%, Test Loss: 0.23531395583590375\n",
      "Epoch Time (Training + Test) = 26.35 seconds\n",
      "epoch: 38 average loss: 0.225\n",
      "Test Accuracy : 91.4%, Test Loss: 0.23848663581700263\n",
      "Epoch Time (Training + Test) = 26.41 seconds\n",
      "epoch: 39 average loss: 0.222\n",
      "Test Accuracy : 91.8%, Test Loss: 0.2362682924051828\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 26.44 seconds\n",
      "epoch: 40 average loss: 0.222\n",
      "Test Accuracy : 92.2%, Test Loss: 0.22284844659174544\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 26.39 seconds\n",
      "epoch: 41 average loss: 0.213\n",
      "Test Accuracy : 91.8%, Test Loss: 0.2279953981124902\n",
      "Epoch Time (Training + Test) = 26.48 seconds\n",
      "epoch: 42 average loss: 0.210\n",
      "Test Accuracy : 91.9%, Test Loss: 0.2285713230884528\n",
      "Epoch Time (Training + Test) = 26.39 seconds\n",
      "epoch: 43 average loss: 0.211\n",
      "Test Accuracy : 91.7%, Test Loss: 0.22440497001892404\n",
      "Epoch Time (Training + Test) = 26.38 seconds\n",
      "epoch: 44 average loss: 0.209\n",
      "Test Accuracy : 92.1%, Test Loss: 0.21807857489661325\n",
      "Epoch Time (Training + Test) = 26.43 seconds\n",
      "epoch: 45 average loss: 0.206\n",
      "Test Accuracy : 92.1%, Test Loss: 0.21844479267167138\n",
      "Epoch Time (Training + Test) = 26.43 seconds\n",
      "epoch: 46 average loss: 0.204\n",
      "Test Accuracy : 92.3%, Test Loss: 0.21777999174745777\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 26.42 seconds\n",
      "epoch: 47 average loss: 0.199\n",
      "Test Accuracy : 92.0%, Test Loss: 0.2219446165180659\n",
      "Epoch Time (Training + Test) = 26.46 seconds\n",
      "epoch: 48 average loss: 0.197\n",
      "Test Accuracy : 92.6%, Test Loss: 0.2041749417121652\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 26.43 seconds\n",
      "epoch: 49 average loss: 0.195\n",
      "Test Accuracy : 92.6%, Test Loss: 0.20929338721723495\n",
      "Epoch Time (Training + Test) = 26.37 seconds\n",
      "epoch: 50 average loss: 0.196\n",
      "Test Accuracy : 91.8%, Test Loss: 0.23349346130897727\n",
      "Epoch Time (Training + Test) = 26.37 seconds\n",
      "epoch: 51 average loss: 0.185\n",
      "Test Accuracy : 92.5%, Test Loss: 0.21400729661123663\n",
      "Epoch Time (Training + Test) = 26.41 seconds\n",
      "epoch: 52 average loss: 0.195\n",
      "Test Accuracy : 92.7%, Test Loss: 0.21278831768262235\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 26.38 seconds\n",
      "epoch: 53 average loss: 0.189\n",
      "Test Accuracy : 92.6%, Test Loss: 0.21218436277365382\n",
      "Epoch Time (Training + Test) = 25.79 seconds\n",
      "epoch: 54 average loss: 0.185\n",
      "Test Accuracy : 92.6%, Test Loss: 0.21207268814308733\n",
      "Epoch Time (Training + Test) = 24.98 seconds\n",
      "epoch: 55 average loss: 0.180\n",
      "Test Accuracy : 93.1%, Test Loss: 0.19520693470405626\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 25.04 seconds\n",
      "epoch: 56 average loss: 0.181\n",
      "Test Accuracy : 92.5%, Test Loss: 0.21779224564191663\n",
      "Epoch Time (Training + Test) = 24.84 seconds\n",
      "epoch: 57 average loss: 0.176\n",
      "Test Accuracy : 92.6%, Test Loss: 0.2061309671100182\n",
      "Epoch Time (Training + Test) = 24.95 seconds\n",
      "epoch: 58 average loss: 0.174\n",
      "Test Accuracy : 92.6%, Test Loss: 0.21040470577493497\n",
      "Epoch Time (Training + Test) = 24.92 seconds\n",
      "epoch: 59 average loss: 0.168\n",
      "Test Accuracy : 92.9%, Test Loss: 0.198575699065305\n",
      "Epoch Time (Training + Test) = 24.96 seconds\n",
      "epoch: 60 average loss: 0.174\n",
      "Test Accuracy : 92.4%, Test Loss: 0.21291674824455117\n",
      "Epoch Time (Training + Test) = 24.84 seconds\n",
      "epoch: 61 average loss: 0.170\n",
      "Test Accuracy : 92.6%, Test Loss: 0.20563949852050106\n",
      "Epoch Time (Training + Test) = 24.88 seconds\n",
      "epoch: 62 average loss: 0.167\n",
      "Test Accuracy : 93.5%, Test Loss: 0.1913696343102787\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 24.94 seconds\n",
      "epoch: 63 average loss: 0.165\n",
      "Test Accuracy : 93.0%, Test Loss: 0.1944831321133843\n",
      "Epoch Time (Training + Test) = 24.96 seconds\n",
      "epoch: 64 average loss: 0.163\n",
      "Test Accuracy : 93.3%, Test Loss: 0.19161800474305696\n",
      "Epoch Time (Training + Test) = 24.95 seconds\n",
      "epoch: 65 average loss: 0.163\n",
      "Test Accuracy : 93.8%, Test Loss: 0.18483383873406845\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 24.88 seconds\n",
      "epoch: 66 average loss: 0.152\n",
      "Test Accuracy : 93.6%, Test Loss: 0.18647580376908748\n",
      "Epoch Time (Training + Test) = 24.97 seconds\n",
      "epoch: 67 average loss: 0.160\n",
      "Test Accuracy : 93.1%, Test Loss: 0.20329284611382062\n",
      "Epoch Time (Training + Test) = 24.90 seconds\n",
      "epoch: 68 average loss: 0.155\n",
      "Test Accuracy : 92.8%, Test Loss: 0.20226059373043762\n",
      "Epoch Time (Training + Test) = 24.71 seconds\n",
      "epoch: 69 average loss: 0.150\n",
      "Test Accuracy : 93.0%, Test Loss: 0.20138002847191655\n",
      "Epoch Time (Training + Test) = 25.17 seconds\n",
      "epoch: 70 average loss: 0.150\n",
      "Test Accuracy : 93.4%, Test Loss: 0.1863910381599695\n",
      "Epoch Time (Training + Test) = 25.16 seconds\n",
      "epoch: 71 average loss: 0.145\n",
      "Test Accuracy : 93.3%, Test Loss: 0.20847572112762475\n",
      "Epoch Time (Training + Test) = 25.26 seconds\n",
      "epoch: 72 average loss: 0.147\n",
      "Test Accuracy : 93.6%, Test Loss: 0.19420441105678865\n",
      "Epoch Time (Training + Test) = 24.90 seconds\n",
      "epoch: 73 average loss: 0.145\n",
      "Test Accuracy : 93.3%, Test Loss: 0.1905710734898531\n",
      "Epoch Time (Training + Test) = 24.92 seconds\n",
      "epoch: 74 average loss: 0.139\n",
      "Test Accuracy : 92.6%, Test Loss: 0.2147400023156329\n",
      "Epoch Time (Training + Test) = 24.94 seconds\n",
      "epoch: 75 average loss: 0.138\n",
      "Test Accuracy : 93.0%, Test Loss: 0.19573327111481112\n",
      "Epoch Time (Training + Test) = 25.00 seconds\n",
      "Data Saved to BNN_Cifar10_Regular_Finetune.csv\n",
      "Finished Training: \n",
      "Total Time 1.082432 hours\n",
      " Average Time Per Epoch 51.96 seconds\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(BNN,model_name='BNN_Cifar10_Regular_Finetune',project_name='UniAdapt_Training_Encoder_Decoder',binarise=True,classes=classes_59)\n",
    "trainer.lr = 1e-3\n",
    "trainer.batch_size = 64\n",
    "trainer.epochs =75\n",
    "trainer.epoch_chkpts = []\n",
    "trainer.start_epoch = 0\n",
    "trainer.set_scheduler(None)\n",
    "trainer.set_optimizer(torch.optim.Adam,lr = 1e-3)\n",
    "trainer.train(train59_loader,test59_loader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ff54641a",
   "metadata": {},
   "source": [
    "# Cifar 80 Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e825fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_transforms(dataset,train= True):\n",
    "    def normalize_channels(data):\n",
    "    #We have a nxCxWxH array\n",
    "        d = data\n",
    "        d = torch.flatten(data,2,-1).to(dtype = torch.float32)/255\n",
    "        mean= torch.mean(d,dim = [0,2])\n",
    "        std = torch.std(d,dim = [0,2])\n",
    "        return mean,std\n",
    "\n",
    "    mean,std = normalize_channels(dataset.data)\n",
    "    if train:\n",
    "        transform = transforms.Compose([\n",
    "            # transforms.ToTensor(),\n",
    "            transforms.RandomCrop(32, padding=4),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.Normalize(mean, std),\n",
    "        ])\n",
    "    else:\n",
    "        transform = transforms.Compose([\n",
    "        transforms.Normalize(mean, std),\n",
    "        ])\n",
    "\n",
    "    dataset.transform = transform\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6a366b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_channels(data):\n",
    "    #We have a nxCxWxH array\n",
    "    d = data\n",
    "    d = torch.flatten(data,2,-1)\n",
    "    mean= torch.mean(d,dim = [0,2])\n",
    "    std = torch.std(d,dim = [0,2])\n",
    "    return mean,std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c0ee87d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.5034, 0.4852, 0.4427]), tensor([0.2684, 0.2562, 0.2766]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train80_path = r'data\\cifar-100\\cifar-80\\train'\n",
    "test80_path =r'data\\cifar-100\\cifar-80\\train'\n",
    "cifar80_path = r'data\\cifar-100\\cifar-80'\n",
    "\n",
    "\n",
    "train_80 = cifar_100_split(cifar80_path)\n",
    "train_80=apply_transforms(train_80)\n",
    "\n",
    "test_80 = cifar_100_split(cifar80_path,train = False)\n",
    "test_80=apply_transforms(test_80,train = False)\n",
    "normalize_channels(train_80.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f89f8d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean,std = normalize_channels(train_80.data)\n",
    "transform_train = transforms.Compose([#resises the image so it can be perfect for our model.\n",
    "                                      transforms.RandomCrop(32, padding=4),\n",
    "                                      transforms.RandomHorizontalFlip(), # FLips the image w.r.t horizontal axis\n",
    "                                      transforms.RandomRotation(10),     #Rotates the image to a specified angel\n",
    "                                      transforms.RandomAffine(0, shear=10, scale=(0.8,1.2)), #Performs actions like zooms, change shear angles.\n",
    "                                      transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2), # Set the color params\n",
    "                                      transforms.Normalize(mean,std) #Normalize all the images\n",
    "                               ])\n",
    "transform_train = transforms.Compose([#resises the image so it can be perfect for our model.\n",
    "                                      transforms.RandomCrop(32, padding=4),\n",
    "                                      transforms.RandomRotation(15),\n",
    "                                      transforms.RandomHorizontalFlip(), # FLips the image w.r.t horizontal axis\n",
    "                                      transforms.Normalize(mean,std) #Normalize all the images\n",
    "                               ])\n",
    "\n",
    "\n",
    "\n",
    "transform_test = transforms.Compose([  #resises the image so it can be perfect for our model.\n",
    "                                      transforms.Normalize(mean,std) #Normalize all the images\n",
    "                               ])\n",
    "\n",
    "train_80.transform = transform_train\n",
    "test_80.transform = transform_test\n",
    "train80_DL = DataLoader(train_80,batch_size=64,shuffle= True,num_workers=8)\n",
    "test80_DL = DataLoader(test_80,batch_size=64,shuffle= False,num_workers= 8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ddd78ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "BNN = net = BNN_Resnet_Cifar100_UniAdapt(80,block=BasicBlock, layers=[2, 2, 2, 2],inflate = 1)\n",
    "\n",
    "BNN.layer3[-1].do_bntan = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "039d6c7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BNN_Resnet_Cifar100_UniAdapt(\n",
       "  (conv1): BinarizeConv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (bn3): BatchNorm1d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
       "  (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): BinarizeConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
       "      (conv2): BinarizeConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): BinarizeConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
       "      (conv2): BinarizeConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): BinarizeConv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
       "      (conv2): BinarizeConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): BinarizeConv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): BinarizeConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
       "      (conv2): BinarizeConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): BinarizeConv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
       "      (conv2): BinarizeConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): BinarizeConv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): BinarizeConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
       "      (conv2): BinarizeConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): BinarizeConv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
       "      (conv2): BinarizeConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): BinarizeConv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): BinarizeConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
       "      (conv2): BinarizeConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "  (fc): BinarizeLinear(in_features=512, out_features=80, bias=True)\n",
       "  (adapter_dict): ModuleDict()\n",
       "  (head_bn): BatchNorm1d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (head): Linear(in_features=480, out_features=80, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    if isinstance(m, BinarizeConv2d) or isinstance(m,BinarizeLinear) :\n",
    "        torch.nn.init.xavier_normal_(m.weight,gain = torch.nn.init.calculate_gain('tanh'))\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.fill_(0.001)\n",
    "\n",
    "\n",
    "BNN.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "498e15e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No lr scheduler\n",
      "Optimizer Set to New: {'state': {}, 'param_groups': [{'lr': 0.0001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': False, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69]}]}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3flwrekn) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1201739f52834bf885cb1d3ca603c625",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Current Best Acc</td><td>▁▄▆▆▇▇▇▇████████████████████████████████</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>epoch time (s)</td><td>█▆▃▃▃▃▃▃▃▃▃▃▃▃▂▃▃▁▃▃▃▃▃▂▃▃▃▂▂▃▂▂▂▃▃▃▃▃▃▃</td></tr><tr><td>lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_accuracy</td><td>▁▄▆▆▇▇▇▇█▇██████████████████████████████</td></tr><tr><td>test_loss</td><td>█▅▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁</td></tr><tr><td>training_loss</td><td>█▆▄▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Best Accuracy</td><td>0.59688</td></tr><tr><td>Current Best Acc</td><td>0.59688</td></tr><tr><td>Total Time (hours)</td><td>10.89517</td></tr><tr><td>epoch</td><td>400</td></tr><tr><td>epoch time (s)</td><td>48.80525</td></tr><tr><td>lr</td><td>0.001</td></tr><tr><td>test_accuracy</td><td>0.58475</td></tr><tr><td>test_loss</td><td>1.66106</td></tr><tr><td>training_loss</td><td>1.01137</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">fast-moon-14</strong>: <a href=\"https://wandb.ai/johnny_suu/UniAdapt_Cifar100_Resnet18_Large/runs/3flwrekn\" target=\"_blank\">https://wandb.ai/johnny_suu/UniAdapt_Cifar100_Resnet18_Large/runs/3flwrekn</a><br/>Synced 5 W&B file(s), 1 media file(s), 1 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230423_230525-3flwrekn\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3flwrekn). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "712296af0dbf47fc98aa9c6e2f283760",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016916666666899498, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\John Su\\Downloads\\SydneyUni\\2022\\thesis\\Thesis\\wandb\\run-20230424_115036-16rxvejw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/johnny_suu/UniAdapt_Cifar100_Resnet18_Large/runs/16rxvejw\" target=\"_blank\">youthful-wood-15</a></strong> to <a href=\"https://wandb.ai/johnny_suu/UniAdapt_Cifar100_Resnet18_Large\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "Project Name: UniAdapt_Cifar100_Resnet18_Large, Run Name BNN_Cifar80_Inflate1_ExtraSkip_BiReal_LowerLR \n",
      "\n",
      "\n",
      "Run Start : 2023-04-24 11-50-36\n",
      "start_epoch : 0\n",
      "initial_lr : 0.0001\n",
      "batch_size : 64\n",
      "epochs : 100\n",
      "epoch_chkpts : []\n",
      "device : cuda:0\n",
      "criterion : CrossEntropyLoss()\n",
      "optimizer : Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: False\n",
      "    lr: 0.0001\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "seed : None\n",
      "model_architecture : <class 'NN_Thesis.nn_classes.BNN_Resnet_Cifar100_UniAdapt'>\n",
      "binerised_training : True\n",
      "Number of Elements : 11250496\n",
      "Initial accuracy:\n",
      "Test Accuracy : 73.5%, Test Loss: 0.8783768689155579\n",
      "best_acc.pth saved!\n",
      "Test Accuracy : 58.5%, Test Loss: 1.6610630445480348\n",
      "epoch: 1 average loss: 0.885\n",
      "Test Accuracy : 60.1%, Test Loss: 1.5560923228263854\n",
      "Epoch Time (Training + Test) = 54.03 seconds\n",
      "epoch: 2 average loss: 0.862\n",
      "Test Accuracy : 60.8%, Test Loss: 1.5393893947601318\n",
      "Epoch Time (Training + Test) = 54.10 seconds\n",
      "epoch: 3 average loss: 0.852\n",
      "Test Accuracy : 60.5%, Test Loss: 1.5564617319107055\n",
      "Epoch Time (Training + Test) = 52.50 seconds\n",
      "epoch: 4 average loss: 0.840\n",
      "Test Accuracy : 59.7%, Test Loss: 1.5738851957321167\n",
      "Epoch Time (Training + Test) = 50.84 seconds\n",
      "epoch: 5 average loss: 0.845\n",
      "Test Accuracy : 60.3%, Test Loss: 1.5714176483154296\n",
      "Epoch Time (Training + Test) = 50.60 seconds\n",
      "epoch: 6 average loss: 0.822\n",
      "Test Accuracy : 60.7%, Test Loss: 1.5443002996444701\n",
      "Epoch Time (Training + Test) = 50.78 seconds\n",
      "epoch: 7 average loss: 0.830\n",
      "Test Accuracy : 60.2%, Test Loss: 1.5743978729248047\n",
      "Epoch Time (Training + Test) = 50.02 seconds\n",
      "epoch: 8 average loss: 0.827\n",
      "Test Accuracy : 60.4%, Test Loss: 1.57191153383255\n",
      "Epoch Time (Training + Test) = 50.03 seconds\n",
      "epoch: 9 average loss: 0.826\n",
      "Test Accuracy : 60.0%, Test Loss: 1.5936911582946778\n",
      "Epoch Time (Training + Test) = 50.46 seconds\n",
      "epoch: 10 average loss: 0.824\n",
      "Test Accuracy : 60.5%, Test Loss: 1.5672594585418702\n",
      "Epoch Time (Training + Test) = 50.62 seconds\n",
      "epoch: 11 average loss: 0.818\n",
      "Test Accuracy : 60.5%, Test Loss: 1.586466889858246\n",
      "Epoch Time (Training + Test) = 50.19 seconds\n",
      "epoch: 12 average loss: 0.813\n",
      "Test Accuracy : 61.3%, Test Loss: 1.5705450949668884\n",
      "Epoch Time (Training + Test) = 50.83 seconds\n",
      "epoch: 13 average loss: 0.820\n",
      "Test Accuracy : 60.3%, Test Loss: 1.580743272304535\n",
      "Epoch Time (Training + Test) = 50.04 seconds\n",
      "epoch: 14 average loss: 0.818\n",
      "Test Accuracy : 61.3%, Test Loss: 1.5646845359802246\n",
      "Epoch Time (Training + Test) = 49.99 seconds\n",
      "epoch: 15 average loss: 0.809\n",
      "Test Accuracy : 60.2%, Test Loss: 1.5634807939529418\n",
      "Epoch Time (Training + Test) = 50.17 seconds\n",
      "epoch: 16 average loss: 0.813\n",
      "Test Accuracy : 60.1%, Test Loss: 1.5798910264968873\n",
      "Epoch Time (Training + Test) = 50.66 seconds\n",
      "epoch: 17 average loss: 0.814\n",
      "Test Accuracy : 60.7%, Test Loss: 1.5812900390625\n",
      "Epoch Time (Training + Test) = 50.52 seconds\n",
      "epoch: 18 average loss: 0.809\n",
      "Test Accuracy : 60.2%, Test Loss: 1.5848824601173401\n",
      "Epoch Time (Training + Test) = 50.08 seconds\n",
      "epoch: 19 average loss: 0.807\n",
      "Test Accuracy : 60.9%, Test Loss: 1.572100977897644\n",
      "Epoch Time (Training + Test) = 50.06 seconds\n",
      "epoch: 20 average loss: 0.817\n",
      "Test Accuracy : 61.2%, Test Loss: 1.537172486782074\n",
      "Epoch Time (Training + Test) = 50.22 seconds\n",
      "epoch: 21 average loss: 0.808\n",
      "Test Accuracy : 60.6%, Test Loss: 1.571864149093628\n",
      "Epoch Time (Training + Test) = 49.25 seconds\n",
      "epoch: 22 average loss: 0.809\n",
      "Test Accuracy : 60.3%, Test Loss: 1.594676959514618\n",
      "Epoch Time (Training + Test) = 49.27 seconds\n",
      "epoch: 23 average loss: 0.802\n",
      "Test Accuracy : 60.5%, Test Loss: 1.5755235877037048\n",
      "Epoch Time (Training + Test) = 49.27 seconds\n",
      "epoch: 24 average loss: 0.799\n",
      "Test Accuracy : 60.6%, Test Loss: 1.607275242805481\n",
      "Epoch Time (Training + Test) = 49.29 seconds\n",
      "epoch: 25 average loss: 0.815\n",
      "Test Accuracy : 60.4%, Test Loss: 1.5677735195159912\n",
      "Epoch Time (Training + Test) = 49.22 seconds\n",
      "epoch: 26 average loss: 0.797\n",
      "Test Accuracy : 60.8%, Test Loss: 1.578572308063507\n",
      "Epoch Time (Training + Test) = 49.32 seconds\n",
      "epoch: 27 average loss: 0.799\n",
      "Test Accuracy : 59.7%, Test Loss: 1.6350694856643677\n",
      "Epoch Time (Training + Test) = 49.13 seconds\n",
      "epoch: 28 average loss: 0.799\n",
      "Test Accuracy : 60.0%, Test Loss: 1.6117268872261048\n",
      "Epoch Time (Training + Test) = 49.26 seconds\n",
      "epoch: 29 average loss: 0.805\n",
      "Test Accuracy : 60.2%, Test Loss: 1.5958739924430847\n",
      "Epoch Time (Training + Test) = 49.13 seconds\n",
      "epoch: 30 average loss: 0.802\n",
      "Test Accuracy : 60.7%, Test Loss: 1.579713743209839\n",
      "Epoch Time (Training + Test) = 49.34 seconds\n",
      "epoch: 31 average loss: 0.793\n",
      "Test Accuracy : 60.3%, Test Loss: 1.5944228205680848\n",
      "Epoch Time (Training + Test) = 49.26 seconds\n",
      "epoch: 32 average loss: 0.790\n",
      "Test Accuracy : 60.8%, Test Loss: 1.5608560581207276\n",
      "Epoch Time (Training + Test) = 49.40 seconds\n",
      "epoch: 33 average loss: 0.802\n",
      "Test Accuracy : 61.1%, Test Loss: 1.563824215888977\n",
      "Epoch Time (Training + Test) = 49.37 seconds\n",
      "epoch: 34 average loss: 0.796\n",
      "Test Accuracy : 61.5%, Test Loss: 1.5533162398338318\n",
      "Epoch Time (Training + Test) = 49.14 seconds\n",
      "epoch: 35 average loss: 0.793\n",
      "Test Accuracy : 60.9%, Test Loss: 1.5688215928077698\n",
      "Epoch Time (Training + Test) = 49.24 seconds\n",
      "epoch: 36 average loss: 0.796\n",
      "Test Accuracy : 61.0%, Test Loss: 1.5854191331863403\n",
      "Epoch Time (Training + Test) = 49.26 seconds\n",
      "epoch: 37 average loss: 0.795\n",
      "Test Accuracy : 60.2%, Test Loss: 1.571346462249756\n",
      "Epoch Time (Training + Test) = 49.08 seconds\n",
      "epoch: 38 average loss: 0.796\n",
      "Test Accuracy : 60.8%, Test Loss: 1.5785288581848145\n",
      "Epoch Time (Training + Test) = 49.29 seconds\n",
      "epoch: 39 average loss: 0.800\n",
      "Test Accuracy : 60.9%, Test Loss: 1.5578534393310546\n",
      "Epoch Time (Training + Test) = 49.15 seconds\n",
      "epoch: 40 average loss: 0.780\n",
      "Test Accuracy : 60.3%, Test Loss: 1.5911821064949037\n",
      "Epoch Time (Training + Test) = 49.39 seconds\n",
      "epoch: 41 average loss: 0.790\n",
      "Test Accuracy : 60.2%, Test Loss: 1.5927630767822265\n",
      "Epoch Time (Training + Test) = 49.22 seconds\n",
      "epoch: 42 average loss: 0.790\n",
      "Test Accuracy : 60.6%, Test Loss: 1.5915927801132201\n",
      "Epoch Time (Training + Test) = 49.30 seconds\n",
      "epoch: 43 average loss: 0.792\n",
      "Test Accuracy : 60.8%, Test Loss: 1.586645158290863\n",
      "Epoch Time (Training + Test) = 49.05 seconds\n",
      "epoch: 44 average loss: 0.790\n",
      "Test Accuracy : 60.0%, Test Loss: 1.5941248064041138\n",
      "Epoch Time (Training + Test) = 49.31 seconds\n",
      "epoch: 45 average loss: 0.791\n",
      "Test Accuracy : 61.1%, Test Loss: 1.5745379500389098\n",
      "Epoch Time (Training + Test) = 49.08 seconds\n",
      "epoch: 46 average loss: 0.785\n",
      "Test Accuracy : 60.3%, Test Loss: 1.5853167700767516\n",
      "Epoch Time (Training + Test) = 49.44 seconds\n",
      "epoch: 47 average loss: 0.792\n",
      "Test Accuracy : 60.4%, Test Loss: 1.5852919130325318\n",
      "Epoch Time (Training + Test) = 49.13 seconds\n",
      "epoch: 48 average loss: 0.799\n",
      "Test Accuracy : 60.3%, Test Loss: 1.592490400314331\n",
      "Epoch Time (Training + Test) = 49.13 seconds\n",
      "epoch: 49 average loss: 0.787\n",
      "Test Accuracy : 60.2%, Test Loss: 1.6253726825714112\n",
      "Epoch Time (Training + Test) = 49.12 seconds\n",
      "epoch: 50 average loss: 0.802\n",
      "Test Accuracy : 60.2%, Test Loss: 1.6045839061737062\n",
      "Epoch Time (Training + Test) = 49.40 seconds\n",
      "epoch: 51 average loss: 0.785\n",
      "Test Accuracy : 60.4%, Test Loss: 1.5883451581001282\n",
      "Epoch Time (Training + Test) = 49.12 seconds\n",
      "epoch: 52 average loss: 0.790\n",
      "Test Accuracy : 60.6%, Test Loss: 1.5717955985069274\n",
      "Epoch Time (Training + Test) = 49.23 seconds\n",
      "epoch: 53 average loss: 0.773\n",
      "Test Accuracy : 60.5%, Test Loss: 1.5867684655189513\n",
      "Epoch Time (Training + Test) = 49.29 seconds\n",
      "epoch: 54 average loss: 0.790\n",
      "Test Accuracy : 61.0%, Test Loss: 1.5921819138526916\n",
      "Epoch Time (Training + Test) = 49.15 seconds\n",
      "epoch: 55 average loss: 0.779\n",
      "Test Accuracy : 60.6%, Test Loss: 1.5733231744766236\n",
      "Epoch Time (Training + Test) = 49.04 seconds\n",
      "epoch: 56 average loss: 0.769\n",
      "Test Accuracy : 60.5%, Test Loss: 1.6095203638076783\n",
      "Epoch Time (Training + Test) = 49.22 seconds\n",
      "epoch: 57 average loss: 0.771\n",
      "Test Accuracy : 60.8%, Test Loss: 1.5915536098480225\n",
      "Epoch Time (Training + Test) = 49.13 seconds\n",
      "epoch: 58 average loss: 0.785\n",
      "Test Accuracy : 61.0%, Test Loss: 1.581150249004364\n",
      "Epoch Time (Training + Test) = 49.13 seconds\n",
      "epoch: 59 average loss: 0.774\n",
      "Test Accuracy : 60.2%, Test Loss: 1.6056685881614685\n",
      "Epoch Time (Training + Test) = 49.09 seconds\n",
      "epoch: 60 average loss: 0.778\n",
      "Test Accuracy : 60.8%, Test Loss: 1.6145301399230958\n",
      "Epoch Time (Training + Test) = 49.30 seconds\n",
      "epoch: 61 average loss: 0.779\n",
      "Test Accuracy : 60.9%, Test Loss: 1.6191513662338257\n",
      "Epoch Time (Training + Test) = 49.02 seconds\n",
      "epoch: 62 average loss: 0.770\n",
      "Test Accuracy : 60.6%, Test Loss: 1.6032474703788757\n",
      "Epoch Time (Training + Test) = 49.20 seconds\n",
      "epoch: 63 average loss: 0.781\n",
      "Test Accuracy : 60.6%, Test Loss: 1.6001065864562989\n",
      "Epoch Time (Training + Test) = 49.00 seconds\n",
      "epoch: 64 average loss: 0.773\n",
      "Test Accuracy : 60.8%, Test Loss: 1.6087247705459595\n",
      "Epoch Time (Training + Test) = 49.23 seconds\n",
      "epoch: 65 average loss: 0.781\n",
      "Test Accuracy : 60.0%, Test Loss: 1.6253377327919005\n",
      "Epoch Time (Training + Test) = 49.16 seconds\n",
      "epoch: 66 average loss: 0.780\n",
      "Test Accuracy : 60.3%, Test Loss: 1.5783252263069152\n",
      "Epoch Time (Training + Test) = 49.21 seconds\n",
      "epoch: 67 average loss: 0.780\n",
      "Test Accuracy : 60.5%, Test Loss: 1.5903957042694092\n",
      "Epoch Time (Training + Test) = 49.12 seconds\n",
      "epoch: 68 average loss: 0.777\n",
      "Test Accuracy : 60.3%, Test Loss: 1.6019003148078919\n",
      "Epoch Time (Training + Test) = 49.30 seconds\n",
      "epoch: 69 average loss: 0.764\n",
      "Test Accuracy : 60.1%, Test Loss: 1.6178126559257506\n",
      "Epoch Time (Training + Test) = 49.29 seconds\n",
      "epoch: 70 average loss: 0.774\n",
      "Test Accuracy : 61.2%, Test Loss: 1.5897614302635192\n",
      "Epoch Time (Training + Test) = 49.36 seconds\n",
      "epoch: 71 average loss: 0.781\n",
      "Test Accuracy : 60.5%, Test Loss: 1.6104523158073425\n",
      "Epoch Time (Training + Test) = 49.04 seconds\n",
      "epoch: 72 average loss: 0.772\n",
      "Test Accuracy : 60.2%, Test Loss: 1.6262938094139099\n",
      "Epoch Time (Training + Test) = 49.14 seconds\n",
      "epoch: 73 average loss: 0.782\n",
      "Test Accuracy : 60.4%, Test Loss: 1.6084987897872924\n",
      "Epoch Time (Training + Test) = 49.03 seconds\n",
      "epoch: 74 average loss: 0.777\n",
      "Test Accuracy : 60.4%, Test Loss: 1.593103898525238\n",
      "Epoch Time (Training + Test) = 49.30 seconds\n",
      "epoch: 75 average loss: 0.779\n",
      "Test Accuracy : 60.1%, Test Loss: 1.6142709522247314\n",
      "Epoch Time (Training + Test) = 49.07 seconds\n",
      "epoch: 76 average loss: 0.781\n",
      "Test Accuracy : 60.2%, Test Loss: 1.6000865755081177\n",
      "Epoch Time (Training + Test) = 49.08 seconds\n",
      "epoch: 77 average loss: 0.775\n",
      "Test Accuracy : 59.7%, Test Loss: 1.626708948135376\n",
      "Epoch Time (Training + Test) = 49.09 seconds\n",
      "epoch: 78 average loss: 0.778\n",
      "Test Accuracy : 60.7%, Test Loss: 1.5917486476898193\n",
      "Epoch Time (Training + Test) = 49.28 seconds\n",
      "epoch: 79 average loss: 0.769\n",
      "Test Accuracy : 60.5%, Test Loss: 1.6123123497962952\n",
      "Epoch Time (Training + Test) = 49.15 seconds\n",
      "epoch: 80 average loss: 0.776\n",
      "Test Accuracy : 60.6%, Test Loss: 1.5862858638763428\n",
      "Epoch Time (Training + Test) = 49.22 seconds\n",
      "epoch: 81 average loss: 0.767\n",
      "Test Accuracy : 59.9%, Test Loss: 1.6061089429855346\n",
      "Epoch Time (Training + Test) = 49.13 seconds\n",
      "epoch: 82 average loss: 0.777\n",
      "Test Accuracy : 60.9%, Test Loss: 1.578280529975891\n",
      "Epoch Time (Training + Test) = 49.09 seconds\n",
      "epoch: 83 average loss: 0.781\n",
      "Test Accuracy : 60.9%, Test Loss: 1.6213157753944396\n",
      "Epoch Time (Training + Test) = 49.05 seconds\n",
      "epoch: 84 average loss: 0.770\n",
      "Test Accuracy : 60.2%, Test Loss: 1.593435920238495\n",
      "Epoch Time (Training + Test) = 49.09 seconds\n",
      "epoch: 85 average loss: 0.776\n",
      "Test Accuracy : 61.1%, Test Loss: 1.572303740978241\n",
      "Epoch Time (Training + Test) = 49.16 seconds\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(BNN,model_name='BNN_Cifar80_Inflate1_ExtraSkip_BiReal_LowerLR',project_name='UniAdapt_Cifar100_Resnet18_Large',binarise=True,classes=train_80.classes)\n",
    "trainer.lr = 1e-4\n",
    "trainer.batch_size = 64\n",
    "trainer.epochs =100\n",
    "trainer.epoch_chkpts = []\n",
    "trainer.start_epoch = 0\n",
    "trainer.set_scheduler(None)\n",
    "trainer.set_optimizer(torch.optim.Adam,lr = trainer.lr)\n",
    "trainer.train(train80_DL,test80_DL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3fb9371c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Baseline = resnet18 = resnet(num_classes = 80 , depth = 18, dataset = 'cifar10')\n",
    "Baseline.layer3[-1].do_bntan = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "b81f3b3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No lr scheduler\n",
      "Optimizer Set to New: {'state': {}, 'param_groups': [{'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': False, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46]}]}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2qi1jy96) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c489aed82bac4f48a0a62e9a2fb38bec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Current Best Acc</td><td>▁▂▃▄▄▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇██████████████████</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>epoch time (s)</td><td>▁▆▅▆▆▅▅▇▅▄▅▅▅▅▅▅▅▅▃▃▅▄█▇▇▆▆▆▆▅▇▇▇▇▇▄▄▄▄▅</td></tr><tr><td>lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_accuracy</td><td>▁▂▃▄▄▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇█▇▇▇▇███▇██████████</td></tr><tr><td>test_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>training_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Current Best Acc</td><td>0.57675</td></tr><tr><td>epoch</td><td>102</td></tr><tr><td>epoch time (s)</td><td>86.81471</td></tr><tr><td>lr</td><td>0.001</td></tr><tr><td>test_accuracy</td><td>0.55175</td></tr><tr><td>test_loss</td><td>1.69485</td></tr><tr><td>training_loss</td><td>1.47903</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">cerulean-serenity-8</strong>: <a href=\"https://wandb.ai/johnny_suu/UniAdapt_Cifar100/runs/2qi1jy96\" target=\"_blank\">https://wandb.ai/johnny_suu/UniAdapt_Cifar100/runs/2qi1jy96</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230417_194809-2qi1jy96\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2qi1jy96). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e190a3c5fca4a149b5cc584d288128a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01693333333338766, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.14.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\John Su\\Downloads\\SydneyUni\\2022\\thesis\\Thesis\\wandb\\run-20230418_103330-2egbrje7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/johnny_suu/UniAdapt_Cifar100/runs/2egbrje7\" target=\"_blank\">classic-wildflower-9</a></strong> to <a href=\"https://wandb.ai/johnny_suu/UniAdapt_Cifar100\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "Project Name: UniAdapt_Cifar100, Run Name ResNet18_Baseline \n",
      "\n",
      "\n",
      "Run Start : 2023-04-18 10-33-30\n",
      "start_epoch : 0\n",
      "initial_lr : 0.001\n",
      "batch_size : 64\n",
      "epochs : 400\n",
      "epoch_chkpts : []\n",
      "device : cuda:0\n",
      "criterion : CrossEntropyLoss()\n",
      "optimizer : Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: False\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "seed : None\n",
      "model_architecture : <class 'NN_Thesis.models.resnet.ResNet_cifar10'>\n",
      "binerised_training : False\n",
      "Number of Elements : 179808\n",
      "Initial accuracy:\n",
      "Test Accuracy : 1.2%, Test Loss: 5.019302528381347\n",
      "best_acc.pth saved!\n",
      "Test Accuracy : 1.2%, Test Loss: 4.969655086517334\n",
      "epoch: 1 average loss: 3.739\n",
      "Test Accuracy : 15.3%, Test Loss: 3.4218055591583254\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 22.35 seconds\n",
      "epoch: 2 average loss: 3.263\n",
      "Test Accuracy : 22.6%, Test Loss: 3.035095485687256\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 22.24 seconds\n",
      "epoch: 3 average loss: 2.994\n",
      "Test Accuracy : 25.0%, Test Loss: 2.9336289138793945\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 21.94 seconds\n",
      "epoch: 4 average loss: 2.802\n",
      "Test Accuracy : 28.5%, Test Loss: 2.7795604553222657\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 22.06 seconds\n",
      "epoch: 5 average loss: 2.638\n",
      "Test Accuracy : 34.2%, Test Loss: 2.4856223421096804\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 21.95 seconds\n",
      "epoch: 6 average loss: 2.514\n",
      "Test Accuracy : 39.6%, Test Loss: 2.247082194328308\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 22.37 seconds\n",
      "epoch: 7 average loss: 2.405\n",
      "Test Accuracy : 37.9%, Test Loss: 2.326650734901428\n",
      "Epoch Time (Training + Test) = 21.94 seconds\n",
      "epoch: 8 average loss: 2.310\n",
      "Test Accuracy : 40.6%, Test Loss: 2.2002665758132935\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 21.93 seconds\n",
      "epoch: 9 average loss: 2.234\n",
      "Test Accuracy : 43.1%, Test Loss: 2.086431544303894\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 21.88 seconds\n",
      "epoch: 10 average loss: 2.171\n",
      "Test Accuracy : 44.6%, Test Loss: 2.0495690393447874\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 22.09 seconds\n",
      "epoch: 11 average loss: 2.117\n",
      "Test Accuracy : 45.3%, Test Loss: 2.0307387838363646\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 22.12 seconds\n",
      "epoch: 12 average loss: 2.068\n",
      "Test Accuracy : 46.1%, Test Loss: 1.991472186088562\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 22.27 seconds\n",
      "epoch: 13 average loss: 2.023\n",
      "Test Accuracy : 48.9%, Test Loss: 1.8620638971328736\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 23.22 seconds\n",
      "epoch: 14 average loss: 1.987\n",
      "Test Accuracy : 46.4%, Test Loss: 1.979398901939392\n",
      "Epoch Time (Training + Test) = 22.66 seconds\n",
      "epoch: 15 average loss: 1.944\n",
      "Test Accuracy : 49.0%, Test Loss: 1.8596664152145386\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 23.69 seconds\n",
      "epoch: 16 average loss: 1.900\n",
      "Test Accuracy : 48.3%, Test Loss: 1.8911650161743163\n",
      "Epoch Time (Training + Test) = 22.71 seconds\n",
      "epoch: 17 average loss: 1.885\n",
      "Test Accuracy : 49.9%, Test Loss: 1.8343624572753907\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 22.81 seconds\n",
      "epoch: 18 average loss: 1.852\n",
      "Test Accuracy : 50.7%, Test Loss: 1.7957543382644654\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 23.02 seconds\n",
      "epoch: 19 average loss: 1.826\n",
      "Test Accuracy : 51.2%, Test Loss: 1.770575161933899\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 23.28 seconds\n",
      "epoch: 20 average loss: 1.794\n",
      "Test Accuracy : 50.9%, Test Loss: 1.7761838598251343\n",
      "Epoch Time (Training + Test) = 25.19 seconds\n",
      "epoch: 21 average loss: 1.777\n",
      "Test Accuracy : 52.2%, Test Loss: 1.7534415931701661\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 23.41 seconds\n",
      "epoch: 22 average loss: 1.753\n",
      "Test Accuracy : 50.7%, Test Loss: 1.855070161819458\n",
      "Epoch Time (Training + Test) = 23.70 seconds\n",
      "epoch: 23 average loss: 1.734\n",
      "Test Accuracy : 53.8%, Test Loss: 1.6901520109176635\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 22.56 seconds\n",
      "epoch: 24 average loss: 1.716\n",
      "Test Accuracy : 53.1%, Test Loss: 1.7262151880264283\n",
      "Epoch Time (Training + Test) = 23.45 seconds\n",
      "epoch: 25 average loss: 1.701\n",
      "Test Accuracy : 53.8%, Test Loss: 1.6799159774780272\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 22.81 seconds\n",
      "epoch: 26 average loss: 1.678\n",
      "Test Accuracy : 54.8%, Test Loss: 1.6509230260849\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 22.96 seconds\n",
      "epoch: 27 average loss: 1.660\n",
      "Test Accuracy : 55.4%, Test Loss: 1.6162789630889893\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 23.14 seconds\n",
      "epoch: 28 average loss: 1.653\n",
      "Test Accuracy : 53.6%, Test Loss: 1.7034250411987304\n",
      "Epoch Time (Training + Test) = 24.03 seconds\n",
      "epoch: 29 average loss: 1.626\n",
      "Test Accuracy : 56.4%, Test Loss: 1.5771172747612\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 22.63 seconds\n",
      "epoch: 30 average loss: 1.614\n",
      "Test Accuracy : 54.6%, Test Loss: 1.653509449005127\n",
      "Epoch Time (Training + Test) = 22.55 seconds\n",
      "epoch: 31 average loss: 1.600\n",
      "Test Accuracy : 56.1%, Test Loss: 1.5690311517715454\n",
      "Epoch Time (Training + Test) = 22.57 seconds\n",
      "epoch: 32 average loss: 1.589\n",
      "Test Accuracy : 56.3%, Test Loss: 1.5870779848098755\n",
      "Epoch Time (Training + Test) = 23.58 seconds\n",
      "epoch: 33 average loss: 1.574\n",
      "Test Accuracy : 55.3%, Test Loss: 1.6338730921745301\n",
      "Epoch Time (Training + Test) = 22.83 seconds\n",
      "epoch: 34 average loss: 1.568\n",
      "Test Accuracy : 57.0%, Test Loss: 1.5576082944869996\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 22.52 seconds\n",
      "epoch: 35 average loss: 1.552\n",
      "Test Accuracy : 55.8%, Test Loss: 1.5920700435638429\n",
      "Epoch Time (Training + Test) = 22.62 seconds\n",
      "epoch: 36 average loss: 1.549\n",
      "Test Accuracy : 57.2%, Test Loss: 1.5758561816215515\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 22.57 seconds\n",
      "epoch: 37 average loss: 1.527\n",
      "Test Accuracy : 56.4%, Test Loss: 1.5661070737838745\n",
      "Epoch Time (Training + Test) = 22.75 seconds\n",
      "epoch: 38 average loss: 1.508\n",
      "Test Accuracy : 57.1%, Test Loss: 1.5605539026260375\n",
      "Epoch Time (Training + Test) = 22.68 seconds\n",
      "epoch: 39 average loss: 1.509\n",
      "Test Accuracy : 57.6%, Test Loss: 1.549460597038269\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 22.42 seconds\n",
      "epoch: 40 average loss: 1.501\n",
      "Test Accuracy : 57.9%, Test Loss: 1.573231589794159\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 22.09 seconds\n",
      "epoch: 41 average loss: 1.495\n",
      "Test Accuracy : 57.9%, Test Loss: 1.532056725502014\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 22.14 seconds\n",
      "epoch: 42 average loss: 1.475\n",
      "Test Accuracy : 56.8%, Test Loss: 1.6030595202445983\n",
      "Epoch Time (Training + Test) = 22.45 seconds\n",
      "epoch: 43 average loss: 1.472\n",
      "Test Accuracy : 57.5%, Test Loss: 1.582547025680542\n",
      "Epoch Time (Training + Test) = 22.44 seconds\n",
      "epoch: 44 average loss: 1.459\n",
      "Test Accuracy : 57.8%, Test Loss: 1.5562729501724244\n",
      "Epoch Time (Training + Test) = 21.93 seconds\n",
      "epoch: 45 average loss: 1.451\n",
      "Test Accuracy : 59.2%, Test Loss: 1.4965921726226807\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 21.83 seconds\n",
      "epoch: 46 average loss: 1.438\n",
      "Test Accuracy : 59.2%, Test Loss: 1.486681990623474\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 21.69 seconds\n",
      "epoch: 47 average loss: 1.432\n",
      "Test Accuracy : 59.0%, Test Loss: 1.4782082724571228\n",
      "Epoch Time (Training + Test) = 21.77 seconds\n",
      "epoch: 48 average loss: 1.422\n",
      "Test Accuracy : 59.2%, Test Loss: 1.4811223373413085\n",
      "Epoch Time (Training + Test) = 22.10 seconds\n",
      "epoch: 49 average loss: 1.422\n",
      "Test Accuracy : 58.9%, Test Loss: 1.523016637802124\n",
      "Epoch Time (Training + Test) = 21.72 seconds\n",
      "epoch: 50 average loss: 1.408\n",
      "Test Accuracy : 59.7%, Test Loss: 1.4934995226860046\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 22.15 seconds\n",
      "epoch: 51 average loss: 1.408\n",
      "Test Accuracy : 59.5%, Test Loss: 1.4663204803466796\n",
      "Epoch Time (Training + Test) = 22.43 seconds\n",
      "epoch: 52 average loss: 1.397\n",
      "Test Accuracy : 58.9%, Test Loss: 1.5227613687515258\n",
      "Epoch Time (Training + Test) = 22.14 seconds\n",
      "epoch: 53 average loss: 1.387\n",
      "Test Accuracy : 59.9%, Test Loss: 1.4557620906829833\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 22.04 seconds\n",
      "epoch: 54 average loss: 1.382\n",
      "Test Accuracy : 59.0%, Test Loss: 1.5041850366592406\n",
      "Epoch Time (Training + Test) = 22.44 seconds\n",
      "epoch: 55 average loss: 1.384\n",
      "Test Accuracy : 58.7%, Test Loss: 1.504987533569336\n",
      "Epoch Time (Training + Test) = 22.12 seconds\n",
      "epoch: 56 average loss: 1.369\n",
      "Test Accuracy : 60.3%, Test Loss: 1.4686305203437806\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 22.18 seconds\n",
      "epoch: 57 average loss: 1.367\n",
      "Test Accuracy : 58.4%, Test Loss: 1.5158385276794433\n",
      "Epoch Time (Training + Test) = 22.07 seconds\n",
      "epoch: 58 average loss: 1.359\n",
      "Test Accuracy : 61.0%, Test Loss: 1.4338080921173095\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 22.18 seconds\n",
      "epoch: 59 average loss: 1.340\n",
      "Test Accuracy : 61.0%, Test Loss: 1.4415659294128418\n",
      "Epoch Time (Training + Test) = 22.14 seconds\n",
      "epoch: 60 average loss: 1.346\n",
      "Test Accuracy : 60.4%, Test Loss: 1.4654477124214171\n",
      "Epoch Time (Training + Test) = 22.20 seconds\n",
      "epoch: 61 average loss: 1.337\n",
      "Test Accuracy : 60.6%, Test Loss: 1.4562761459350586\n",
      "Epoch Time (Training + Test) = 22.11 seconds\n",
      "epoch: 62 average loss: 1.333\n",
      "Test Accuracy : 61.2%, Test Loss: 1.4112887401580811\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 22.08 seconds\n",
      "epoch: 63 average loss: 1.322\n",
      "Test Accuracy : 60.6%, Test Loss: 1.4451932711601256\n",
      "Epoch Time (Training + Test) = 22.20 seconds\n",
      "epoch: 64 average loss: 1.327\n",
      "Test Accuracy : 61.1%, Test Loss: 1.4448520150184632\n",
      "Epoch Time (Training + Test) = 22.16 seconds\n",
      "epoch: 65 average loss: 1.317\n",
      "Test Accuracy : 60.6%, Test Loss: 1.4388849582672119\n",
      "Epoch Time (Training + Test) = 21.90 seconds\n",
      "epoch: 66 average loss: 1.315\n",
      "Test Accuracy : 59.9%, Test Loss: 1.4704788279533387\n",
      "Epoch Time (Training + Test) = 21.64 seconds\n",
      "epoch: 67 average loss: 1.309\n",
      "Test Accuracy : 59.5%, Test Loss: 1.4849910364151\n",
      "Epoch Time (Training + Test) = 21.37 seconds\n",
      "epoch: 68 average loss: 1.304\n",
      "Test Accuracy : 60.8%, Test Loss: 1.4482700119018554\n",
      "Epoch Time (Training + Test) = 21.38 seconds\n",
      "epoch: 69 average loss: 1.298\n",
      "Test Accuracy : 59.8%, Test Loss: 1.4779336194992065\n",
      "Epoch Time (Training + Test) = 21.25 seconds\n",
      "epoch: 70 average loss: 1.291\n",
      "Test Accuracy : 60.6%, Test Loss: 1.4659702477455139\n",
      "Epoch Time (Training + Test) = 21.40 seconds\n",
      "epoch: 71 average loss: 1.288\n",
      "Test Accuracy : 62.1%, Test Loss: 1.3836772985458374\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 21.52 seconds\n",
      "epoch: 72 average loss: 1.281\n",
      "Test Accuracy : 61.2%, Test Loss: 1.4395114831924438\n",
      "Epoch Time (Training + Test) = 21.59 seconds\n",
      "epoch: 73 average loss: 1.280\n",
      "Test Accuracy : 61.4%, Test Loss: 1.433722656726837\n",
      "Epoch Time (Training + Test) = 21.53 seconds\n",
      "epoch: 74 average loss: 1.278\n",
      "Test Accuracy : 61.3%, Test Loss: 1.4044194951057434\n",
      "Epoch Time (Training + Test) = 21.52 seconds\n",
      "epoch: 75 average loss: 1.273\n",
      "Test Accuracy : 62.0%, Test Loss: 1.4482199358940124\n",
      "Epoch Time (Training + Test) = 21.44 seconds\n",
      "epoch: 76 average loss: 1.263\n",
      "Test Accuracy : 61.6%, Test Loss: 1.434287197113037\n",
      "Epoch Time (Training + Test) = 21.78 seconds\n",
      "epoch: 77 average loss: 1.269\n",
      "Test Accuracy : 60.7%, Test Loss: 1.449427351474762\n",
      "Epoch Time (Training + Test) = 21.37 seconds\n",
      "epoch: 78 average loss: 1.257\n",
      "Test Accuracy : 60.8%, Test Loss: 1.46575337600708\n",
      "Epoch Time (Training + Test) = 21.53 seconds\n",
      "epoch: 79 average loss: 1.258\n",
      "Test Accuracy : 61.7%, Test Loss: 1.404571005344391\n",
      "Epoch Time (Training + Test) = 21.37 seconds\n",
      "epoch: 80 average loss: 1.249\n",
      "Test Accuracy : 62.1%, Test Loss: 1.389431833267212\n",
      "Epoch Time (Training + Test) = 21.49 seconds\n",
      "epoch: 81 average loss: 1.252\n",
      "Test Accuracy : 61.7%, Test Loss: 1.4058726449012757\n",
      "Epoch Time (Training + Test) = 21.45 seconds\n",
      "epoch: 82 average loss: 1.236\n",
      "Test Accuracy : 61.3%, Test Loss: 1.4368772764205933\n",
      "Epoch Time (Training + Test) = 21.45 seconds\n",
      "epoch: 83 average loss: 1.240\n",
      "Test Accuracy : 62.8%, Test Loss: 1.3826669993400573\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 21.41 seconds\n",
      "epoch: 84 average loss: 1.236\n",
      "Test Accuracy : 61.8%, Test Loss: 1.4291914601325988\n",
      "Epoch Time (Training + Test) = 21.33 seconds\n",
      "epoch: 85 average loss: 1.233\n",
      "Test Accuracy : 61.2%, Test Loss: 1.4388522706031799\n",
      "Epoch Time (Training + Test) = 21.41 seconds\n",
      "epoch: 86 average loss: 1.220\n",
      "Test Accuracy : 61.4%, Test Loss: 1.426828462600708\n",
      "Epoch Time (Training + Test) = 21.49 seconds\n",
      "epoch: 87 average loss: 1.227\n",
      "Test Accuracy : 62.6%, Test Loss: 1.3874393854141236\n",
      "Epoch Time (Training + Test) = 21.36 seconds\n",
      "epoch: 88 average loss: 1.227\n",
      "Test Accuracy : 60.7%, Test Loss: 1.4786728715896607\n",
      "Epoch Time (Training + Test) = 21.49 seconds\n",
      "epoch: 89 average loss: 1.215\n",
      "Test Accuracy : 62.4%, Test Loss: 1.4237942929267884\n",
      "Epoch Time (Training + Test) = 21.22 seconds\n",
      "epoch: 90 average loss: 1.215\n",
      "Test Accuracy : 62.8%, Test Loss: 1.3582456588745118\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 21.61 seconds\n",
      "epoch: 91 average loss: 1.216\n",
      "Test Accuracy : 62.6%, Test Loss: 1.3871353616714477\n",
      "Epoch Time (Training + Test) = 21.39 seconds\n",
      "epoch: 92 average loss: 1.208\n",
      "Test Accuracy : 61.5%, Test Loss: 1.415442952156067\n",
      "Epoch Time (Training + Test) = 21.46 seconds\n",
      "epoch: 93 average loss: 1.207\n",
      "Test Accuracy : 62.3%, Test Loss: 1.422484335899353\n",
      "Epoch Time (Training + Test) = 21.62 seconds\n",
      "epoch: 94 average loss: 1.201\n",
      "Test Accuracy : 62.8%, Test Loss: 1.3790692324638367\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 21.87 seconds\n",
      "epoch: 95 average loss: 1.192\n",
      "Test Accuracy : 62.6%, Test Loss: 1.3786967358589173\n",
      "Epoch Time (Training + Test) = 21.40 seconds\n",
      "epoch: 96 average loss: 1.203\n",
      "Test Accuracy : 62.6%, Test Loss: 1.3902379965782166\n",
      "Epoch Time (Training + Test) = 21.45 seconds\n",
      "epoch: 97 average loss: 1.195\n",
      "Test Accuracy : 62.5%, Test Loss: 1.367906490802765\n",
      "Epoch Time (Training + Test) = 21.30 seconds\n",
      "epoch: 98 average loss: 1.197\n",
      "Test Accuracy : 62.0%, Test Loss: 1.4327783875465394\n",
      "Epoch Time (Training + Test) = 21.40 seconds\n",
      "epoch: 99 average loss: 1.185\n",
      "Test Accuracy : 62.1%, Test Loss: 1.4043380346298218\n",
      "Epoch Time (Training + Test) = 21.39 seconds\n",
      "epoch: 100 average loss: 1.188\n",
      "Test Accuracy : 62.6%, Test Loss: 1.3875339941978455\n",
      "Epoch Time (Training + Test) = 21.48 seconds\n",
      "epoch: 101 average loss: 1.187\n",
      "Test Accuracy : 63.1%, Test Loss: 1.3824761123657228\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 21.39 seconds\n",
      "epoch: 102 average loss: 1.175\n",
      "Test Accuracy : 63.6%, Test Loss: 1.3631898007392884\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 21.46 seconds\n",
      "epoch: 103 average loss: 1.177\n",
      "Test Accuracy : 63.0%, Test Loss: 1.3794190459251403\n",
      "Epoch Time (Training + Test) = 21.47 seconds\n",
      "epoch: 104 average loss: 1.174\n",
      "Test Accuracy : 62.6%, Test Loss: 1.419096827507019\n",
      "Epoch Time (Training + Test) = 21.38 seconds\n",
      "epoch: 105 average loss: 1.173\n",
      "Test Accuracy : 62.8%, Test Loss: 1.4087562599182128\n",
      "Epoch Time (Training + Test) = 21.49 seconds\n",
      "epoch: 106 average loss: 1.167\n",
      "Test Accuracy : 63.0%, Test Loss: 1.373102873802185\n",
      "Epoch Time (Training + Test) = 22.01 seconds\n",
      "epoch: 107 average loss: 1.163\n",
      "Test Accuracy : 62.4%, Test Loss: 1.3882431893348695\n",
      "Epoch Time (Training + Test) = 21.47 seconds\n",
      "epoch: 108 average loss: 1.162\n",
      "Test Accuracy : 62.8%, Test Loss: 1.382801579475403\n",
      "Epoch Time (Training + Test) = 21.43 seconds\n",
      "epoch: 109 average loss: 1.162\n",
      "Test Accuracy : 63.8%, Test Loss: 1.3662225217819215\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 21.45 seconds\n",
      "epoch: 110 average loss: 1.154\n",
      "Test Accuracy : 62.5%, Test Loss: 1.4356679883003234\n",
      "Epoch Time (Training + Test) = 21.53 seconds\n",
      "epoch: 111 average loss: 1.154\n",
      "Test Accuracy : 63.7%, Test Loss: 1.3742905473709106\n",
      "Epoch Time (Training + Test) = 21.37 seconds\n",
      "epoch: 112 average loss: 1.152\n",
      "Test Accuracy : 62.7%, Test Loss: 1.4121871280670166\n",
      "Epoch Time (Training + Test) = 21.46 seconds\n",
      "epoch: 113 average loss: 1.147\n",
      "Test Accuracy : 62.9%, Test Loss: 1.414913577079773\n",
      "Epoch Time (Training + Test) = 21.35 seconds\n",
      "epoch: 114 average loss: 1.152\n",
      "Test Accuracy : 62.7%, Test Loss: 1.413048126220703\n",
      "Epoch Time (Training + Test) = 21.32 seconds\n",
      "epoch: 115 average loss: 1.140\n",
      "Test Accuracy : 63.2%, Test Loss: 1.35409614944458\n",
      "Epoch Time (Training + Test) = 21.40 seconds\n",
      "epoch: 116 average loss: 1.141\n",
      "Test Accuracy : 62.8%, Test Loss: 1.3567015256881714\n",
      "Epoch Time (Training + Test) = 21.47 seconds\n",
      "epoch: 117 average loss: 1.138\n",
      "Test Accuracy : 62.9%, Test Loss: 1.4096310448646545\n",
      "Epoch Time (Training + Test) = 21.54 seconds\n",
      "epoch: 118 average loss: 1.131\n",
      "Test Accuracy : 63.3%, Test Loss: 1.378090712070465\n",
      "Epoch Time (Training + Test) = 21.47 seconds\n",
      "epoch: 119 average loss: 1.144\n",
      "Test Accuracy : 62.7%, Test Loss: 1.3905242409706116\n",
      "Epoch Time (Training + Test) = 21.39 seconds\n",
      "epoch: 120 average loss: 1.141\n",
      "Test Accuracy : 62.3%, Test Loss: 1.391565402030945\n",
      "Epoch Time (Training + Test) = 21.37 seconds\n",
      "epoch: 121 average loss: 1.132\n",
      "Test Accuracy : 62.9%, Test Loss: 1.3948401794433594\n",
      "Epoch Time (Training + Test) = 21.35 seconds\n",
      "epoch: 122 average loss: 1.125\n",
      "Test Accuracy : 62.5%, Test Loss: 1.3848097763061524\n",
      "Epoch Time (Training + Test) = 21.60 seconds\n",
      "epoch: 123 average loss: 1.123\n",
      "Test Accuracy : 64.4%, Test Loss: 1.3456562728881836\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 21.40 seconds\n",
      "epoch: 124 average loss: 1.122\n",
      "Test Accuracy : 63.6%, Test Loss: 1.3466418919563294\n",
      "Epoch Time (Training + Test) = 21.46 seconds\n",
      "epoch: 125 average loss: 1.122\n",
      "Test Accuracy : 64.0%, Test Loss: 1.3662375059127807\n",
      "Epoch Time (Training + Test) = 21.38 seconds\n",
      "epoch: 126 average loss: 1.117\n",
      "Test Accuracy : 62.9%, Test Loss: 1.3862657623291015\n",
      "Epoch Time (Training + Test) = 21.51 seconds\n",
      "epoch: 127 average loss: 1.127\n",
      "Test Accuracy : 63.3%, Test Loss: 1.382666588306427\n",
      "Epoch Time (Training + Test) = 21.34 seconds\n",
      "epoch: 128 average loss: 1.121\n",
      "Test Accuracy : 61.9%, Test Loss: 1.4243303341865539\n",
      "Epoch Time (Training + Test) = 21.37 seconds\n",
      "epoch: 129 average loss: 1.112\n",
      "Test Accuracy : 62.9%, Test Loss: 1.3836330947875977\n",
      "Epoch Time (Training + Test) = 21.46 seconds\n",
      "epoch: 130 average loss: 1.112\n",
      "Test Accuracy : 63.8%, Test Loss: 1.3871923313140868\n",
      "Epoch Time (Training + Test) = 21.53 seconds\n",
      "epoch: 131 average loss: 1.105\n",
      "Test Accuracy : 63.6%, Test Loss: 1.3617273054122925\n",
      "Epoch Time (Training + Test) = 21.34 seconds\n",
      "epoch: 132 average loss: 1.099\n",
      "Test Accuracy : 64.0%, Test Loss: 1.3570063939094543\n",
      "Epoch Time (Training + Test) = 21.44 seconds\n",
      "epoch: 133 average loss: 1.103\n",
      "Test Accuracy : 64.3%, Test Loss: 1.353771629333496\n",
      "Epoch Time (Training + Test) = 21.43 seconds\n",
      "epoch: 134 average loss: 1.100\n",
      "Test Accuracy : 62.9%, Test Loss: 1.4055795950889587\n",
      "Epoch Time (Training + Test) = 21.81 seconds\n",
      "epoch: 135 average loss: 1.090\n",
      "Test Accuracy : 63.2%, Test Loss: 1.3482950911521911\n",
      "Epoch Time (Training + Test) = 21.53 seconds\n",
      "epoch: 136 average loss: 1.095\n",
      "Test Accuracy : 63.7%, Test Loss: 1.36826607131958\n",
      "Epoch Time (Training + Test) = 21.52 seconds\n",
      "epoch: 137 average loss: 1.098\n",
      "Test Accuracy : 62.7%, Test Loss: 1.4027219581604005\n",
      "Epoch Time (Training + Test) = 21.37 seconds\n",
      "epoch: 138 average loss: 1.088\n",
      "Test Accuracy : 64.0%, Test Loss: 1.3606194710731507\n",
      "Epoch Time (Training + Test) = 21.70 seconds\n",
      "epoch: 139 average loss: 1.095\n",
      "Test Accuracy : 64.6%, Test Loss: 1.320365571975708\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 21.76 seconds\n",
      "epoch: 140 average loss: 1.087\n",
      "Test Accuracy : 64.4%, Test Loss: 1.347766019821167\n",
      "Epoch Time (Training + Test) = 21.61 seconds\n",
      "epoch: 141 average loss: 1.088\n",
      "Test Accuracy : 62.8%, Test Loss: 1.4117756953239442\n",
      "Epoch Time (Training + Test) = 21.42 seconds\n",
      "epoch: 142 average loss: 1.089\n",
      "Test Accuracy : 64.0%, Test Loss: 1.3397903199195862\n",
      "Epoch Time (Training + Test) = 21.48 seconds\n",
      "epoch: 143 average loss: 1.087\n",
      "Test Accuracy : 63.8%, Test Loss: 1.3746022577285766\n",
      "Epoch Time (Training + Test) = 21.31 seconds\n",
      "epoch: 144 average loss: 1.085\n",
      "Test Accuracy : 63.3%, Test Loss: 1.3991185603141785\n",
      "Epoch Time (Training + Test) = 21.80 seconds\n",
      "epoch: 145 average loss: 1.075\n",
      "Test Accuracy : 63.6%, Test Loss: 1.3870113925933838\n",
      "Epoch Time (Training + Test) = 21.54 seconds\n",
      "epoch: 146 average loss: 1.076\n",
      "Test Accuracy : 62.4%, Test Loss: 1.405907609462738\n",
      "Epoch Time (Training + Test) = 21.46 seconds\n",
      "epoch: 147 average loss: 1.078\n",
      "Test Accuracy : 63.7%, Test Loss: 1.3505392622947694\n",
      "Epoch Time (Training + Test) = 21.30 seconds\n",
      "epoch: 148 average loss: 1.073\n",
      "Test Accuracy : 63.8%, Test Loss: 1.3527886600494385\n",
      "Epoch Time (Training + Test) = 21.57 seconds\n",
      "epoch: 149 average loss: 1.075\n",
      "Test Accuracy : 63.4%, Test Loss: 1.3976688413619995\n",
      "Epoch Time (Training + Test) = 21.28 seconds\n",
      "epoch: 150 average loss: 1.060\n",
      "Test Accuracy : 64.0%, Test Loss: 1.3505162825584411\n",
      "Epoch Time (Training + Test) = 21.58 seconds\n",
      "epoch: 151 average loss: 1.074\n",
      "Test Accuracy : 62.5%, Test Loss: 1.4142265367507934\n",
      "Epoch Time (Training + Test) = 21.50 seconds\n",
      "epoch: 152 average loss: 1.068\n",
      "Test Accuracy : 63.3%, Test Loss: 1.3685817799568176\n",
      "Epoch Time (Training + Test) = 21.31 seconds\n",
      "epoch: 153 average loss: 1.069\n",
      "Test Accuracy : 64.1%, Test Loss: 1.3692813487052917\n",
      "Epoch Time (Training + Test) = 21.33 seconds\n",
      "epoch: 154 average loss: 1.058\n",
      "Test Accuracy : 64.1%, Test Loss: 1.3543364429473876\n",
      "Epoch Time (Training + Test) = 21.40 seconds\n",
      "epoch: 155 average loss: 1.056\n",
      "Test Accuracy : 64.1%, Test Loss: 1.3648674902915954\n",
      "Epoch Time (Training + Test) = 21.49 seconds\n",
      "epoch: 156 average loss: 1.060\n",
      "Test Accuracy : 64.6%, Test Loss: 1.3413634309768676\n",
      "Epoch Time (Training + Test) = 21.38 seconds\n",
      "epoch: 157 average loss: 1.062\n",
      "Test Accuracy : 64.0%, Test Loss: 1.3736098046302796\n",
      "Epoch Time (Training + Test) = 21.38 seconds\n",
      "epoch: 158 average loss: 1.058\n",
      "Test Accuracy : 63.9%, Test Loss: 1.3486007585525512\n",
      "Epoch Time (Training + Test) = 21.43 seconds\n",
      "epoch: 159 average loss: 1.052\n",
      "Test Accuracy : 64.9%, Test Loss: 1.335396143913269\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 21.30 seconds\n",
      "epoch: 160 average loss: 1.053\n",
      "Test Accuracy : 63.7%, Test Loss: 1.38394757604599\n",
      "Epoch Time (Training + Test) = 21.34 seconds\n",
      "epoch: 161 average loss: 1.058\n",
      "Test Accuracy : 64.7%, Test Loss: 1.3483237533569337\n",
      "Epoch Time (Training + Test) = 21.36 seconds\n",
      "epoch: 162 average loss: 1.056\n",
      "Test Accuracy : 64.3%, Test Loss: 1.3405311212539672\n",
      "Epoch Time (Training + Test) = 21.36 seconds\n",
      "epoch: 163 average loss: 1.054\n",
      "Test Accuracy : 64.2%, Test Loss: 1.3648782958984376\n",
      "Epoch Time (Training + Test) = 21.37 seconds\n",
      "epoch: 164 average loss: 1.048\n",
      "Test Accuracy : 64.4%, Test Loss: 1.337336622238159\n",
      "Epoch Time (Training + Test) = 21.57 seconds\n",
      "epoch: 165 average loss: 1.053\n",
      "Test Accuracy : 64.7%, Test Loss: 1.3318620533943177\n",
      "Epoch Time (Training + Test) = 21.43 seconds\n",
      "epoch: 166 average loss: 1.049\n",
      "Test Accuracy : 63.2%, Test Loss: 1.4005141487121582\n",
      "Epoch Time (Training + Test) = 21.36 seconds\n",
      "epoch: 167 average loss: 1.042\n",
      "Test Accuracy : 63.6%, Test Loss: 1.381163565158844\n",
      "Epoch Time (Training + Test) = 21.49 seconds\n",
      "epoch: 168 average loss: 1.044\n",
      "Test Accuracy : 63.8%, Test Loss: 1.3813019289970399\n",
      "Epoch Time (Training + Test) = 21.37 seconds\n",
      "epoch: 169 average loss: 1.037\n",
      "Test Accuracy : 64.3%, Test Loss: 1.3539838709831238\n",
      "Epoch Time (Training + Test) = 21.50 seconds\n",
      "epoch: 170 average loss: 1.035\n",
      "Test Accuracy : 63.5%, Test Loss: 1.3912870535850526\n",
      "Epoch Time (Training + Test) = 21.29 seconds\n",
      "epoch: 171 average loss: 1.045\n",
      "Test Accuracy : 64.7%, Test Loss: 1.3336268434524536\n",
      "Epoch Time (Training + Test) = 21.41 seconds\n",
      "epoch: 172 average loss: 1.037\n",
      "Test Accuracy : 64.2%, Test Loss: 1.3578998665809632\n",
      "Epoch Time (Training + Test) = 21.52 seconds\n",
      "epoch: 173 average loss: 1.049\n",
      "Test Accuracy : 64.4%, Test Loss: 1.3550557646751404\n",
      "Epoch Time (Training + Test) = 21.33 seconds\n",
      "epoch: 174 average loss: 1.029\n",
      "Test Accuracy : 64.4%, Test Loss: 1.3443432254791259\n",
      "Epoch Time (Training + Test) = 21.30 seconds\n",
      "epoch: 175 average loss: 1.036\n",
      "Test Accuracy : 64.4%, Test Loss: 1.3712073955535888\n",
      "Epoch Time (Training + Test) = 21.33 seconds\n",
      "epoch: 176 average loss: 1.028\n",
      "Test Accuracy : 64.5%, Test Loss: 1.342410460472107\n",
      "Epoch Time (Training + Test) = 21.40 seconds\n",
      "epoch: 177 average loss: 1.041\n",
      "Test Accuracy : 64.0%, Test Loss: 1.3706076769828797\n",
      "Epoch Time (Training + Test) = 21.48 seconds\n",
      "epoch: 178 average loss: 1.027\n",
      "Test Accuracy : 64.5%, Test Loss: 1.360232918739319\n",
      "Epoch Time (Training + Test) = 21.81 seconds\n",
      "epoch: 179 average loss: 1.032\n",
      "Test Accuracy : 64.3%, Test Loss: 1.341426071166992\n",
      "Epoch Time (Training + Test) = 21.44 seconds\n",
      "epoch: 180 average loss: 1.024\n",
      "Test Accuracy : 64.2%, Test Loss: 1.3533521003723143\n",
      "Epoch Time (Training + Test) = 21.40 seconds\n",
      "epoch: 181 average loss: 1.020\n",
      "Test Accuracy : 64.9%, Test Loss: 1.3353077259063721\n",
      "Epoch Time (Training + Test) = 21.44 seconds\n",
      "epoch: 182 average loss: 1.023\n",
      "Test Accuracy : 64.2%, Test Loss: 1.3505414118766785\n",
      "Epoch Time (Training + Test) = 21.33 seconds\n",
      "epoch: 183 average loss: 1.028\n",
      "Test Accuracy : 63.3%, Test Loss: 1.4248130979537963\n",
      "Epoch Time (Training + Test) = 21.34 seconds\n",
      "epoch: 184 average loss: 1.016\n",
      "Test Accuracy : 64.7%, Test Loss: 1.3810693488121033\n",
      "Epoch Time (Training + Test) = 21.31 seconds\n",
      "epoch: 185 average loss: 1.018\n",
      "Test Accuracy : 64.7%, Test Loss: 1.3709618253707885\n",
      "Epoch Time (Training + Test) = 21.35 seconds\n",
      "epoch: 186 average loss: 1.029\n",
      "Test Accuracy : 64.0%, Test Loss: 1.3611334328651428\n",
      "Epoch Time (Training + Test) = 21.32 seconds\n",
      "epoch: 187 average loss: 1.026\n",
      "Test Accuracy : 64.8%, Test Loss: 1.3440736808776856\n",
      "Epoch Time (Training + Test) = 21.31 seconds\n",
      "epoch: 188 average loss: 1.024\n",
      "Test Accuracy : 64.6%, Test Loss: 1.370473520755768\n",
      "Epoch Time (Training + Test) = 21.37 seconds\n",
      "epoch: 189 average loss: 1.011\n",
      "Test Accuracy : 64.2%, Test Loss: 1.3667644276618958\n",
      "Epoch Time (Training + Test) = 21.37 seconds\n",
      "epoch: 190 average loss: 1.009\n",
      "Test Accuracy : 64.4%, Test Loss: 1.3452469463348389\n",
      "Epoch Time (Training + Test) = 21.54 seconds\n",
      "epoch: 191 average loss: 1.010\n",
      "Test Accuracy : 64.7%, Test Loss: 1.339055884361267\n",
      "Epoch Time (Training + Test) = 21.43 seconds\n",
      "epoch: 192 average loss: 1.017\n",
      "Test Accuracy : 63.8%, Test Loss: 1.375925910949707\n",
      "Epoch Time (Training + Test) = 21.45 seconds\n",
      "epoch: 193 average loss: 1.017\n",
      "Test Accuracy : 65.2%, Test Loss: 1.3137407126426697\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 21.50 seconds\n",
      "epoch: 194 average loss: 1.007\n",
      "Test Accuracy : 63.1%, Test Loss: 1.440794668674469\n",
      "Epoch Time (Training + Test) = 21.53 seconds\n",
      "epoch: 195 average loss: 0.998\n",
      "Test Accuracy : 63.6%, Test Loss: 1.39733669424057\n",
      "Epoch Time (Training + Test) = 21.39 seconds\n",
      "epoch: 196 average loss: 1.015\n",
      "Test Accuracy : 64.3%, Test Loss: 1.3819082584381104\n",
      "Epoch Time (Training + Test) = 21.46 seconds\n",
      "epoch: 197 average loss: 1.005\n",
      "Test Accuracy : 64.0%, Test Loss: 1.3786847095489503\n",
      "Epoch Time (Training + Test) = 21.45 seconds\n",
      "epoch: 198 average loss: 1.004\n",
      "Test Accuracy : 65.0%, Test Loss: 1.3482119193077087\n",
      "Epoch Time (Training + Test) = 21.46 seconds\n",
      "epoch: 199 average loss: 0.994\n",
      "Test Accuracy : 65.4%, Test Loss: 1.3216997466087341\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 21.52 seconds\n",
      "epoch: 200 average loss: 0.998\n",
      "Test Accuracy : 64.7%, Test Loss: 1.3728767261505126\n",
      "Epoch Time (Training + Test) = 21.48 seconds\n",
      "epoch: 201 average loss: 0.995\n",
      "Test Accuracy : 64.8%, Test Loss: 1.3343982381820678\n",
      "Epoch Time (Training + Test) = 21.44 seconds\n",
      "epoch: 202 average loss: 1.001\n",
      "Test Accuracy : 64.9%, Test Loss: 1.3661728811264038\n",
      "Epoch Time (Training + Test) = 21.42 seconds\n",
      "epoch: 203 average loss: 0.988\n",
      "Test Accuracy : 64.5%, Test Loss: 1.3879831681251527\n",
      "Epoch Time (Training + Test) = 21.56 seconds\n",
      "epoch: 204 average loss: 0.999\n",
      "Test Accuracy : 64.3%, Test Loss: 1.3767898211479186\n",
      "Epoch Time (Training + Test) = 21.36 seconds\n",
      "epoch: 205 average loss: 0.996\n",
      "Test Accuracy : 64.8%, Test Loss: 1.3673935675621032\n",
      "Epoch Time (Training + Test) = 21.36 seconds\n",
      "epoch: 206 average loss: 0.988\n",
      "Test Accuracy : 64.8%, Test Loss: 1.3619415359497071\n",
      "Epoch Time (Training + Test) = 21.60 seconds\n",
      "epoch: 207 average loss: 0.980\n",
      "Test Accuracy : 64.6%, Test Loss: 1.3774740238189698\n",
      "Epoch Time (Training + Test) = 21.55 seconds\n",
      "epoch: 208 average loss: 0.989\n",
      "Test Accuracy : 65.1%, Test Loss: 1.3478905329704285\n",
      "Epoch Time (Training + Test) = 21.39 seconds\n",
      "epoch: 209 average loss: 0.993\n",
      "Test Accuracy : 65.3%, Test Loss: 1.3361324911117554\n",
      "Epoch Time (Training + Test) = 21.52 seconds\n",
      "epoch: 210 average loss: 0.986\n",
      "Test Accuracy : 63.8%, Test Loss: 1.4355825805664062\n",
      "Epoch Time (Training + Test) = 21.54 seconds\n",
      "epoch: 211 average loss: 0.988\n",
      "Test Accuracy : 65.0%, Test Loss: 1.3624265284538268\n",
      "Epoch Time (Training + Test) = 21.38 seconds\n",
      "epoch: 212 average loss: 0.989\n",
      "Test Accuracy : 64.8%, Test Loss: 1.3511081433296204\n",
      "Epoch Time (Training + Test) = 21.45 seconds\n",
      "epoch: 213 average loss: 0.980\n",
      "Test Accuracy : 65.5%, Test Loss: 1.326073070049286\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 21.56 seconds\n",
      "epoch: 214 average loss: 0.982\n",
      "Test Accuracy : 64.4%, Test Loss: 1.3674251980781555\n",
      "Epoch Time (Training + Test) = 21.54 seconds\n",
      "epoch: 215 average loss: 0.983\n",
      "Test Accuracy : 64.5%, Test Loss: 1.3571743206977844\n",
      "Epoch Time (Training + Test) = 21.46 seconds\n",
      "epoch: 216 average loss: 0.982\n",
      "Test Accuracy : 64.7%, Test Loss: 1.377059335231781\n",
      "Epoch Time (Training + Test) = 21.41 seconds\n",
      "epoch: 217 average loss: 0.975\n",
      "Test Accuracy : 64.6%, Test Loss: 1.358124081134796\n",
      "Epoch Time (Training + Test) = 21.46 seconds\n",
      "epoch: 218 average loss: 0.979\n",
      "Test Accuracy : 64.3%, Test Loss: 1.3732971920967103\n",
      "Epoch Time (Training + Test) = 21.41 seconds\n",
      "epoch: 219 average loss: 0.973\n",
      "Test Accuracy : 65.1%, Test Loss: 1.3511374773979188\n",
      "Epoch Time (Training + Test) = 21.47 seconds\n",
      "epoch: 220 average loss: 0.974\n",
      "Test Accuracy : 65.2%, Test Loss: 1.3666710085868836\n",
      "Epoch Time (Training + Test) = 21.40 seconds\n",
      "epoch: 221 average loss: 0.973\n",
      "Test Accuracy : 64.9%, Test Loss: 1.3702193880081177\n",
      "Epoch Time (Training + Test) = 21.34 seconds\n",
      "epoch: 222 average loss: 0.981\n",
      "Test Accuracy : 65.0%, Test Loss: 1.349533058166504\n",
      "Epoch Time (Training + Test) = 21.53 seconds\n",
      "epoch: 223 average loss: 0.974\n",
      "Test Accuracy : 64.8%, Test Loss: 1.3721545581817627\n",
      "Epoch Time (Training + Test) = 21.69 seconds\n",
      "epoch: 224 average loss: 0.970\n",
      "Test Accuracy : 65.0%, Test Loss: 1.35578404378891\n",
      "Epoch Time (Training + Test) = 21.51 seconds\n",
      "epoch: 225 average loss: 0.965\n",
      "Test Accuracy : 64.4%, Test Loss: 1.3848283333778382\n",
      "Epoch Time (Training + Test) = 21.55 seconds\n",
      "epoch: 226 average loss: 0.971\n",
      "Test Accuracy : 65.1%, Test Loss: 1.34462602519989\n",
      "Epoch Time (Training + Test) = 21.36 seconds\n",
      "epoch: 227 average loss: 0.965\n",
      "Test Accuracy : 64.8%, Test Loss: 1.3479194841384887\n",
      "Epoch Time (Training + Test) = 21.90 seconds\n",
      "epoch: 228 average loss: 0.966\n",
      "Test Accuracy : 64.6%, Test Loss: 1.3914457030296326\n",
      "Epoch Time (Training + Test) = 21.54 seconds\n",
      "epoch: 229 average loss: 0.971\n",
      "Test Accuracy : 64.8%, Test Loss: 1.3572893433570863\n",
      "Epoch Time (Training + Test) = 21.47 seconds\n",
      "epoch: 230 average loss: 0.973\n",
      "Test Accuracy : 64.9%, Test Loss: 1.405907120704651\n",
      "Epoch Time (Training + Test) = 21.44 seconds\n",
      "epoch: 231 average loss: 0.963\n",
      "Test Accuracy : 65.2%, Test Loss: 1.3767199840545654\n",
      "Epoch Time (Training + Test) = 21.34 seconds\n",
      "epoch: 232 average loss: 0.956\n",
      "Test Accuracy : 64.6%, Test Loss: 1.3814897289276122\n",
      "Epoch Time (Training + Test) = 21.52 seconds\n",
      "epoch: 233 average loss: 0.959\n",
      "Test Accuracy : 64.6%, Test Loss: 1.3939100885391236\n",
      "Epoch Time (Training + Test) = 21.34 seconds\n",
      "epoch: 234 average loss: 0.968\n",
      "Test Accuracy : 64.4%, Test Loss: 1.3849292364120482\n",
      "Epoch Time (Training + Test) = 21.74 seconds\n",
      "epoch: 235 average loss: 0.948\n",
      "Test Accuracy : 65.4%, Test Loss: 1.3283413963317872\n",
      "Epoch Time (Training + Test) = 21.45 seconds\n",
      "epoch: 236 average loss: 0.972\n",
      "Test Accuracy : 65.5%, Test Loss: 1.345532458782196\n",
      "Epoch Time (Training + Test) = 21.36 seconds\n",
      "epoch: 237 average loss: 0.960\n",
      "Test Accuracy : 64.5%, Test Loss: 1.3939233593940734\n",
      "Epoch Time (Training + Test) = 21.35 seconds\n",
      "epoch: 238 average loss: 0.959\n",
      "Test Accuracy : 64.8%, Test Loss: 1.3805567784309387\n",
      "Epoch Time (Training + Test) = 21.42 seconds\n",
      "epoch: 239 average loss: 0.952\n",
      "Test Accuracy : 65.1%, Test Loss: 1.3714445123672485\n",
      "Epoch Time (Training + Test) = 21.33 seconds\n",
      "epoch: 240 average loss: 0.961\n",
      "Test Accuracy : 64.6%, Test Loss: 1.3639608216285706\n",
      "Epoch Time (Training + Test) = 21.44 seconds\n",
      "epoch: 241 average loss: 0.958\n",
      "Test Accuracy : 65.3%, Test Loss: 1.3545634989738464\n",
      "Epoch Time (Training + Test) = 21.64 seconds\n",
      "epoch: 242 average loss: 0.942\n",
      "Test Accuracy : 65.0%, Test Loss: 1.3667099370956421\n",
      "Epoch Time (Training + Test) = 21.38 seconds\n",
      "epoch: 243 average loss: 0.950\n",
      "Test Accuracy : 64.5%, Test Loss: 1.3951031141281127\n",
      "Epoch Time (Training + Test) = 21.48 seconds\n",
      "epoch: 244 average loss: 0.959\n",
      "Test Accuracy : 65.0%, Test Loss: 1.3670813417434693\n",
      "Epoch Time (Training + Test) = 21.60 seconds\n",
      "epoch: 245 average loss: 0.950\n",
      "Test Accuracy : 64.9%, Test Loss: 1.3851350336074828\n",
      "Epoch Time (Training + Test) = 21.42 seconds\n",
      "epoch: 246 average loss: 0.945\n",
      "Test Accuracy : 65.3%, Test Loss: 1.3642750649452209\n",
      "Epoch Time (Training + Test) = 21.42 seconds\n",
      "epoch: 247 average loss: 0.950\n",
      "Test Accuracy : 65.8%, Test Loss: 1.342699697971344\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 21.52 seconds\n",
      "epoch: 248 average loss: 0.952\n",
      "Test Accuracy : 65.3%, Test Loss: 1.3629701476097107\n",
      "Epoch Time (Training + Test) = 21.57 seconds\n",
      "epoch: 249 average loss: 0.945\n",
      "Test Accuracy : 64.8%, Test Loss: 1.3666230163574218\n",
      "Epoch Time (Training + Test) = 21.48 seconds\n",
      "epoch: 250 average loss: 0.949\n",
      "Test Accuracy : 65.0%, Test Loss: 1.3760154285430908\n",
      "Epoch Time (Training + Test) = 21.52 seconds\n",
      "epoch: 251 average loss: 0.952\n",
      "Test Accuracy : 65.7%, Test Loss: 1.3451645307540894\n",
      "Epoch Time (Training + Test) = 21.54 seconds\n",
      "epoch: 252 average loss: 0.939\n",
      "Test Accuracy : 65.5%, Test Loss: 1.3536384387016296\n",
      "Epoch Time (Training + Test) = 21.54 seconds\n",
      "epoch: 253 average loss: 0.949\n",
      "Test Accuracy : 64.6%, Test Loss: 1.379931728363037\n",
      "Epoch Time (Training + Test) = 21.53 seconds\n",
      "epoch: 254 average loss: 0.944\n",
      "Test Accuracy : 65.0%, Test Loss: 1.358225661277771\n",
      "Epoch Time (Training + Test) = 21.36 seconds\n",
      "epoch: 255 average loss: 0.944\n",
      "Test Accuracy : 65.2%, Test Loss: 1.3797588710784912\n",
      "Epoch Time (Training + Test) = 21.46 seconds\n",
      "epoch: 256 average loss: 0.941\n",
      "Test Accuracy : 64.6%, Test Loss: 1.4009068603515624\n",
      "Epoch Time (Training + Test) = 21.42 seconds\n",
      "epoch: 257 average loss: 0.935\n",
      "Test Accuracy : 65.9%, Test Loss: 1.334114891052246\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 21.49 seconds\n",
      "epoch: 258 average loss: 0.938\n",
      "Test Accuracy : 65.6%, Test Loss: 1.362674810886383\n",
      "Epoch Time (Training + Test) = 21.54 seconds\n",
      "epoch: 259 average loss: 0.940\n",
      "Test Accuracy : 65.9%, Test Loss: 1.3599141430854798\n",
      "Epoch Time (Training + Test) = 21.54 seconds\n",
      "epoch: 260 average loss: 0.943\n",
      "Test Accuracy : 65.1%, Test Loss: 1.3784666938781738\n",
      "Epoch Time (Training + Test) = 21.52 seconds\n",
      "epoch: 261 average loss: 0.929\n",
      "Test Accuracy : 65.4%, Test Loss: 1.3428264303207398\n",
      "Epoch Time (Training + Test) = 21.51 seconds\n",
      "epoch: 262 average loss: 0.939\n",
      "Test Accuracy : 64.8%, Test Loss: 1.4074052276611329\n",
      "Epoch Time (Training + Test) = 21.96 seconds\n",
      "epoch: 263 average loss: 0.934\n",
      "Test Accuracy : 65.0%, Test Loss: 1.3534028749465943\n",
      "Epoch Time (Training + Test) = 21.46 seconds\n",
      "epoch: 264 average loss: 0.937\n",
      "Test Accuracy : 65.5%, Test Loss: 1.342140371799469\n",
      "Epoch Time (Training + Test) = 21.44 seconds\n",
      "epoch: 265 average loss: 0.933\n",
      "Test Accuracy : 66.0%, Test Loss: 1.3571905102729798\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 21.53 seconds\n",
      "epoch: 266 average loss: 0.931\n",
      "Test Accuracy : 64.7%, Test Loss: 1.3657115969657898\n",
      "Epoch Time (Training + Test) = 21.42 seconds\n",
      "epoch: 267 average loss: 0.935\n",
      "Test Accuracy : 64.8%, Test Loss: 1.3901686878204347\n",
      "Epoch Time (Training + Test) = 21.42 seconds\n",
      "epoch: 268 average loss: 0.929\n",
      "Test Accuracy : 65.4%, Test Loss: 1.3811718807220459\n",
      "Epoch Time (Training + Test) = 21.49 seconds\n",
      "epoch: 269 average loss: 0.926\n",
      "Test Accuracy : 65.6%, Test Loss: 1.361781575679779\n",
      "Epoch Time (Training + Test) = 21.50 seconds\n",
      "epoch: 270 average loss: 0.938\n",
      "Test Accuracy : 64.7%, Test Loss: 1.4065489778518676\n",
      "Epoch Time (Training + Test) = 21.37 seconds\n",
      "epoch: 271 average loss: 0.929\n",
      "Test Accuracy : 64.6%, Test Loss: 1.3977279348373413\n",
      "Epoch Time (Training + Test) = 21.46 seconds\n",
      "epoch: 272 average loss: 0.934\n",
      "Test Accuracy : 64.9%, Test Loss: 1.3659533090591431\n",
      "Epoch Time (Training + Test) = 21.44 seconds\n",
      "epoch: 273 average loss: 0.921\n",
      "Test Accuracy : 65.2%, Test Loss: 1.3643213682174682\n",
      "Epoch Time (Training + Test) = 21.45 seconds\n",
      "epoch: 274 average loss: 0.925\n",
      "Test Accuracy : 65.0%, Test Loss: 1.3911639375686646\n",
      "Epoch Time (Training + Test) = 21.35 seconds\n",
      "epoch: 275 average loss: 0.913\n",
      "Test Accuracy : 65.2%, Test Loss: 1.3806156907081604\n",
      "Epoch Time (Training + Test) = 21.81 seconds\n",
      "epoch: 276 average loss: 0.928\n",
      "Test Accuracy : 65.0%, Test Loss: 1.376463119983673\n",
      "Epoch Time (Training + Test) = 21.53 seconds\n",
      "epoch: 277 average loss: 0.927\n",
      "Test Accuracy : 65.5%, Test Loss: 1.3582937693595887\n",
      "Epoch Time (Training + Test) = 21.41 seconds\n",
      "epoch: 278 average loss: 0.928\n",
      "Test Accuracy : 65.4%, Test Loss: 1.3839940218925475\n",
      "Epoch Time (Training + Test) = 21.38 seconds\n",
      "epoch: 279 average loss: 0.923\n",
      "Test Accuracy : 65.2%, Test Loss: 1.3614867286682129\n",
      "Epoch Time (Training + Test) = 21.39 seconds\n",
      "epoch: 280 average loss: 0.929\n",
      "Test Accuracy : 64.6%, Test Loss: 1.4202520742416382\n",
      "Epoch Time (Training + Test) = 21.56 seconds\n",
      "epoch: 281 average loss: 0.920\n",
      "Test Accuracy : 65.0%, Test Loss: 1.3991252541542054\n",
      "Epoch Time (Training + Test) = 21.48 seconds\n",
      "epoch: 282 average loss: 0.921\n",
      "Test Accuracy : 65.0%, Test Loss: 1.3877649040222169\n",
      "Epoch Time (Training + Test) = 21.75 seconds\n",
      "epoch: 283 average loss: 0.925\n",
      "Test Accuracy : 65.8%, Test Loss: 1.3858205308914184\n",
      "Epoch Time (Training + Test) = 21.54 seconds\n",
      "epoch: 284 average loss: 0.920\n",
      "Test Accuracy : 64.5%, Test Loss: 1.3594619750976562\n",
      "Epoch Time (Training + Test) = 21.53 seconds\n",
      "epoch: 285 average loss: 0.918\n",
      "Test Accuracy : 65.2%, Test Loss: 1.3676591634750366\n",
      "Epoch Time (Training + Test) = 21.54 seconds\n",
      "epoch: 286 average loss: 0.918\n",
      "Test Accuracy : 64.6%, Test Loss: 1.389623393535614\n",
      "Epoch Time (Training + Test) = 21.94 seconds\n",
      "epoch: 287 average loss: 0.915\n",
      "Test Accuracy : 65.2%, Test Loss: 1.4071581287384034\n",
      "Epoch Time (Training + Test) = 21.43 seconds\n",
      "epoch: 288 average loss: 0.917\n",
      "Test Accuracy : 65.0%, Test Loss: 1.410421368598938\n",
      "Epoch Time (Training + Test) = 21.43 seconds\n",
      "epoch: 289 average loss: 0.921\n",
      "Test Accuracy : 65.7%, Test Loss: 1.3823375225067138\n",
      "Epoch Time (Training + Test) = 21.51 seconds\n",
      "epoch: 290 average loss: 0.925\n",
      "Test Accuracy : 64.4%, Test Loss: 1.402384937763214\n",
      "Epoch Time (Training + Test) = 21.76 seconds\n",
      "epoch: 291 average loss: 0.918\n",
      "Test Accuracy : 65.4%, Test Loss: 1.3629223070144654\n",
      "Epoch Time (Training + Test) = 21.34 seconds\n",
      "epoch: 292 average loss: 0.915\n",
      "Test Accuracy : 64.5%, Test Loss: 1.4040923523902893\n",
      "Epoch Time (Training + Test) = 21.51 seconds\n",
      "epoch: 293 average loss: 0.908\n",
      "Test Accuracy : 65.6%, Test Loss: 1.3657150931358337\n",
      "Epoch Time (Training + Test) = 21.45 seconds\n",
      "epoch: 294 average loss: 0.911\n",
      "Test Accuracy : 64.6%, Test Loss: 1.3771962466239929\n",
      "Epoch Time (Training + Test) = 21.42 seconds\n",
      "epoch: 295 average loss: 0.910\n",
      "Test Accuracy : 65.0%, Test Loss: 1.3917813663482665\n",
      "Epoch Time (Training + Test) = 21.50 seconds\n",
      "epoch: 296 average loss: 0.911\n",
      "Test Accuracy : 66.0%, Test Loss: 1.3532088375091553\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 21.50 seconds\n",
      "epoch: 297 average loss: 0.902\n",
      "Test Accuracy : 64.1%, Test Loss: 1.3815736684799194\n",
      "Epoch Time (Training + Test) = 21.60 seconds\n",
      "epoch: 298 average loss: 0.906\n",
      "Test Accuracy : 64.6%, Test Loss: 1.3981716084480285\n",
      "Epoch Time (Training + Test) = 21.53 seconds\n",
      "epoch: 299 average loss: 0.897\n",
      "Test Accuracy : 65.4%, Test Loss: 1.3808920216560363\n",
      "Epoch Time (Training + Test) = 21.53 seconds\n",
      "epoch: 300 average loss: 0.914\n",
      "Test Accuracy : 65.2%, Test Loss: 1.3845350141525268\n",
      "Epoch Time (Training + Test) = 21.94 seconds\n",
      "epoch: 301 average loss: 0.909\n",
      "Test Accuracy : 64.9%, Test Loss: 1.4060597395896912\n",
      "Epoch Time (Training + Test) = 21.95 seconds\n",
      "epoch: 302 average loss: 0.904\n",
      "Test Accuracy : 64.6%, Test Loss: 1.395552448272705\n",
      "Epoch Time (Training + Test) = 21.44 seconds\n",
      "epoch: 303 average loss: 0.910\n",
      "Test Accuracy : 64.3%, Test Loss: 1.4283733868598938\n",
      "Epoch Time (Training + Test) = 21.38 seconds\n",
      "epoch: 304 average loss: 0.895\n",
      "Test Accuracy : 65.8%, Test Loss: 1.3667338061332703\n",
      "Epoch Time (Training + Test) = 21.53 seconds\n",
      "epoch: 305 average loss: 0.905\n",
      "Test Accuracy : 65.1%, Test Loss: 1.4069589786529542\n",
      "Epoch Time (Training + Test) = 21.40 seconds\n",
      "epoch: 306 average loss: 0.905\n",
      "Test Accuracy : 65.0%, Test Loss: 1.380477334022522\n",
      "Epoch Time (Training + Test) = 21.99 seconds\n",
      "epoch: 307 average loss: 0.903\n",
      "Test Accuracy : 65.0%, Test Loss: 1.3942512097358704\n",
      "Epoch Time (Training + Test) = 21.35 seconds\n",
      "epoch: 308 average loss: 0.906\n",
      "Test Accuracy : 65.5%, Test Loss: 1.3852986516952515\n",
      "Epoch Time (Training + Test) = 21.72 seconds\n",
      "epoch: 309 average loss: 0.901\n",
      "Test Accuracy : 65.0%, Test Loss: 1.3771440224647522\n",
      "Epoch Time (Training + Test) = 21.38 seconds\n",
      "epoch: 310 average loss: 0.902\n",
      "Test Accuracy : 65.5%, Test Loss: 1.3893417201042175\n",
      "Epoch Time (Training + Test) = 21.58 seconds\n",
      "epoch: 311 average loss: 0.903\n",
      "Test Accuracy : 64.8%, Test Loss: 1.4018571367263795\n",
      "Epoch Time (Training + Test) = 21.36 seconds\n",
      "epoch: 312 average loss: 0.906\n",
      "Test Accuracy : 65.4%, Test Loss: 1.3907554626464844\n",
      "Epoch Time (Training + Test) = 21.41 seconds\n",
      "epoch: 313 average loss: 0.898\n",
      "Test Accuracy : 64.8%, Test Loss: 1.3890741415023804\n",
      "Epoch Time (Training + Test) = 21.35 seconds\n",
      "epoch: 314 average loss: 0.893\n",
      "Test Accuracy : 65.2%, Test Loss: 1.3787397351264954\n",
      "Epoch Time (Training + Test) = 21.56 seconds\n",
      "epoch: 315 average loss: 0.894\n",
      "Test Accuracy : 64.3%, Test Loss: 1.396882731437683\n",
      "Epoch Time (Training + Test) = 21.54 seconds\n",
      "epoch: 316 average loss: 0.892\n",
      "Test Accuracy : 65.6%, Test Loss: 1.37061021900177\n",
      "Epoch Time (Training + Test) = 21.51 seconds\n",
      "epoch: 317 average loss: 0.899\n",
      "Test Accuracy : 64.6%, Test Loss: 1.4143663606643677\n",
      "Epoch Time (Training + Test) = 21.35 seconds\n",
      "epoch: 318 average loss: 0.901\n",
      "Test Accuracy : 65.3%, Test Loss: 1.3874811568260192\n",
      "Epoch Time (Training + Test) = 21.67 seconds\n",
      "epoch: 319 average loss: 0.894\n",
      "Test Accuracy : 65.3%, Test Loss: 1.4058046703338622\n",
      "Epoch Time (Training + Test) = 21.35 seconds\n",
      "epoch: 320 average loss: 0.897\n",
      "Test Accuracy : 65.7%, Test Loss: 1.3711043515205383\n",
      "Epoch Time (Training + Test) = 21.39 seconds\n",
      "epoch: 321 average loss: 0.894\n",
      "Test Accuracy : 65.0%, Test Loss: 1.4026847381591796\n",
      "Epoch Time (Training + Test) = 21.50 seconds\n",
      "epoch: 322 average loss: 0.888\n",
      "Test Accuracy : 65.5%, Test Loss: 1.3843746318817138\n",
      "Epoch Time (Training + Test) = 21.52 seconds\n",
      "epoch: 323 average loss: 0.897\n",
      "Test Accuracy : 65.3%, Test Loss: 1.4084528412818909\n",
      "Epoch Time (Training + Test) = 21.29 seconds\n",
      "epoch: 324 average loss: 0.892\n",
      "Test Accuracy : 64.9%, Test Loss: 1.410108709335327\n",
      "Epoch Time (Training + Test) = 21.41 seconds\n",
      "epoch: 325 average loss: 0.892\n",
      "Test Accuracy : 64.9%, Test Loss: 1.3936011147499086\n",
      "Epoch Time (Training + Test) = 21.43 seconds\n",
      "epoch: 326 average loss: 0.885\n",
      "Test Accuracy : 65.3%, Test Loss: 1.3838782920837402\n",
      "Epoch Time (Training + Test) = 21.42 seconds\n",
      "epoch: 327 average loss: 0.891\n",
      "Test Accuracy : 65.0%, Test Loss: 1.3964200420379638\n",
      "Epoch Time (Training + Test) = 21.35 seconds\n",
      "epoch: 328 average loss: 0.886\n",
      "Test Accuracy : 65.3%, Test Loss: 1.3918168907165527\n",
      "Epoch Time (Training + Test) = 21.51 seconds\n",
      "epoch: 329 average loss: 0.896\n",
      "Test Accuracy : 65.3%, Test Loss: 1.4008003401756286\n",
      "Epoch Time (Training + Test) = 21.50 seconds\n",
      "epoch: 330 average loss: 0.891\n",
      "Test Accuracy : 64.7%, Test Loss: 1.3910997524261475\n",
      "Epoch Time (Training + Test) = 21.36 seconds\n",
      "epoch: 331 average loss: 0.892\n",
      "Test Accuracy : 64.8%, Test Loss: 1.4239240288734436\n",
      "Epoch Time (Training + Test) = 21.92 seconds\n",
      "epoch: 332 average loss: 0.887\n",
      "Test Accuracy : 65.3%, Test Loss: 1.4070055599212647\n",
      "Epoch Time (Training + Test) = 21.48 seconds\n",
      "epoch: 333 average loss: 0.881\n",
      "Test Accuracy : 65.4%, Test Loss: 1.391603271484375\n",
      "Epoch Time (Training + Test) = 21.38 seconds\n",
      "epoch: 334 average loss: 0.893\n",
      "Test Accuracy : 65.5%, Test Loss: 1.3888006544113158\n",
      "Epoch Time (Training + Test) = 21.56 seconds\n",
      "epoch: 335 average loss: 0.885\n",
      "Test Accuracy : 65.0%, Test Loss: 1.4023819084167481\n",
      "Epoch Time (Training + Test) = 21.44 seconds\n",
      "epoch: 336 average loss: 0.882\n",
      "Test Accuracy : 66.1%, Test Loss: 1.3837231087684632\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 21.42 seconds\n",
      "epoch: 337 average loss: 0.884\n",
      "Test Accuracy : 65.6%, Test Loss: 1.3821357169151307\n",
      "Epoch Time (Training + Test) = 21.73 seconds\n",
      "epoch: 338 average loss: 0.889\n",
      "Test Accuracy : 65.5%, Test Loss: 1.3753699774742127\n",
      "Epoch Time (Training + Test) = 21.52 seconds\n",
      "epoch: 339 average loss: 0.885\n",
      "Test Accuracy : 65.8%, Test Loss: 1.4016914510726928\n",
      "Epoch Time (Training + Test) = 21.48 seconds\n",
      "epoch: 340 average loss: 0.878\n",
      "Test Accuracy : 65.5%, Test Loss: 1.4016881589889527\n",
      "Epoch Time (Training + Test) = 21.80 seconds\n",
      "epoch: 341 average loss: 0.887\n",
      "Test Accuracy : 65.3%, Test Loss: 1.4197350115776062\n",
      "Epoch Time (Training + Test) = 21.48 seconds\n",
      "epoch: 342 average loss: 0.888\n",
      "Test Accuracy : 65.5%, Test Loss: 1.370673755645752\n",
      "Epoch Time (Training + Test) = 21.51 seconds\n",
      "epoch: 343 average loss: 0.877\n",
      "Test Accuracy : 65.2%, Test Loss: 1.3996023206710815\n",
      "Epoch Time (Training + Test) = 21.41 seconds\n",
      "epoch: 344 average loss: 0.875\n",
      "Test Accuracy : 65.5%, Test Loss: 1.35597585105896\n",
      "Epoch Time (Training + Test) = 21.52 seconds\n",
      "epoch: 345 average loss: 0.887\n",
      "Test Accuracy : 66.4%, Test Loss: 1.3630136189460755\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 21.58 seconds\n",
      "epoch: 346 average loss: 0.888\n",
      "Test Accuracy : 65.7%, Test Loss: 1.3801631679534911\n",
      "Epoch Time (Training + Test) = 21.68 seconds\n",
      "epoch: 347 average loss: 0.875\n",
      "Test Accuracy : 65.8%, Test Loss: 1.3926949162483215\n",
      "Epoch Time (Training + Test) = 21.46 seconds\n",
      "epoch: 348 average loss: 0.874\n",
      "Test Accuracy : 65.8%, Test Loss: 1.3956709542274475\n",
      "Epoch Time (Training + Test) = 21.38 seconds\n",
      "epoch: 349 average loss: 0.875\n",
      "Test Accuracy : 66.2%, Test Loss: 1.3741696410179138\n",
      "Epoch Time (Training + Test) = 21.62 seconds\n",
      "epoch: 350 average loss: 0.876\n",
      "Test Accuracy : 65.3%, Test Loss: 1.3812493181228638\n",
      "Epoch Time (Training + Test) = 21.80 seconds\n",
      "epoch: 351 average loss: 0.874\n",
      "Test Accuracy : 65.5%, Test Loss: 1.38571427154541\n",
      "Epoch Time (Training + Test) = 21.34 seconds\n",
      "epoch: 352 average loss: 0.879\n",
      "Test Accuracy : 65.8%, Test Loss: 1.3836832957267762\n",
      "Epoch Time (Training + Test) = 21.87 seconds\n",
      "epoch: 353 average loss: 0.873\n",
      "Test Accuracy : 65.4%, Test Loss: 1.3922004222869873\n",
      "Epoch Time (Training + Test) = 21.37 seconds\n",
      "epoch: 354 average loss: 0.871\n",
      "Test Accuracy : 66.1%, Test Loss: 1.36438813829422\n",
      "Epoch Time (Training + Test) = 21.47 seconds\n",
      "epoch: 355 average loss: 0.870\n",
      "Test Accuracy : 65.5%, Test Loss: 1.3795786476135254\n",
      "Epoch Time (Training + Test) = 21.42 seconds\n",
      "epoch: 356 average loss: 0.862\n",
      "Test Accuracy : 65.6%, Test Loss: 1.4082219920158385\n",
      "Epoch Time (Training + Test) = 21.52 seconds\n",
      "epoch: 357 average loss: 0.876\n",
      "Test Accuracy : 65.4%, Test Loss: 1.4022070889472962\n",
      "Epoch Time (Training + Test) = 21.52 seconds\n",
      "epoch: 358 average loss: 0.872\n",
      "Test Accuracy : 64.9%, Test Loss: 1.4327407932281495\n",
      "Epoch Time (Training + Test) = 21.54 seconds\n",
      "epoch: 359 average loss: 0.868\n",
      "Test Accuracy : 65.2%, Test Loss: 1.41577854347229\n",
      "Epoch Time (Training + Test) = 21.55 seconds\n",
      "epoch: 360 average loss: 0.865\n",
      "Test Accuracy : 65.9%, Test Loss: 1.383631268978119\n",
      "Epoch Time (Training + Test) = 21.53 seconds\n",
      "epoch: 361 average loss: 0.864\n",
      "Test Accuracy : 65.5%, Test Loss: 1.402696370124817\n",
      "Epoch Time (Training + Test) = 21.49 seconds\n",
      "epoch: 362 average loss: 0.871\n",
      "Test Accuracy : 65.1%, Test Loss: 1.4429364037513732\n",
      "Epoch Time (Training + Test) = 21.36 seconds\n",
      "epoch: 363 average loss: 0.869\n",
      "Test Accuracy : 66.4%, Test Loss: 1.3821230583190918\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 21.49 seconds\n",
      "epoch: 364 average loss: 0.875\n",
      "Test Accuracy : 65.2%, Test Loss: 1.4265368399620055\n",
      "Epoch Time (Training + Test) = 21.57 seconds\n",
      "epoch: 365 average loss: 0.859\n",
      "Test Accuracy : 64.9%, Test Loss: 1.4098780646324158\n",
      "Epoch Time (Training + Test) = 21.52 seconds\n",
      "epoch: 366 average loss: 0.865\n",
      "Test Accuracy : 65.8%, Test Loss: 1.4127633309364318\n",
      "Epoch Time (Training + Test) = 21.56 seconds\n",
      "epoch: 367 average loss: 0.868\n",
      "Test Accuracy : 65.3%, Test Loss: 1.4177144522666931\n",
      "Epoch Time (Training + Test) = 21.48 seconds\n",
      "epoch: 368 average loss: 0.862\n",
      "Test Accuracy : 65.0%, Test Loss: 1.4350029377937317\n",
      "Epoch Time (Training + Test) = 21.39 seconds\n",
      "epoch: 369 average loss: 0.867\n",
      "Test Accuracy : 65.8%, Test Loss: 1.4170134921073914\n",
      "Epoch Time (Training + Test) = 21.35 seconds\n",
      "epoch: 370 average loss: 0.864\n",
      "Test Accuracy : 66.0%, Test Loss: 1.3832780709266663\n",
      "Epoch Time (Training + Test) = 21.47 seconds\n",
      "epoch: 371 average loss: 0.869\n",
      "Test Accuracy : 66.1%, Test Loss: 1.3821870794296265\n",
      "Epoch Time (Training + Test) = 21.48 seconds\n",
      "epoch: 372 average loss: 0.865\n",
      "Test Accuracy : 65.3%, Test Loss: 1.4066105766296386\n",
      "Epoch Time (Training + Test) = 21.35 seconds\n",
      "epoch: 373 average loss: 0.867\n",
      "Test Accuracy : 65.6%, Test Loss: 1.4046579365730285\n",
      "Epoch Time (Training + Test) = 21.79 seconds\n",
      "epoch: 374 average loss: 0.876\n",
      "Test Accuracy : 65.8%, Test Loss: 1.3950141096115112\n",
      "Epoch Time (Training + Test) = 21.51 seconds\n",
      "epoch: 375 average loss: 0.865\n",
      "Test Accuracy : 65.9%, Test Loss: 1.3887745332717896\n",
      "Epoch Time (Training + Test) = 21.76 seconds\n",
      "epoch: 376 average loss: 0.865\n",
      "Test Accuracy : 65.3%, Test Loss: 1.4175929279327393\n",
      "Epoch Time (Training + Test) = 21.53 seconds\n",
      "epoch: 377 average loss: 0.856\n",
      "Test Accuracy : 65.8%, Test Loss: 1.3866970658302307\n",
      "Epoch Time (Training + Test) = 21.44 seconds\n",
      "epoch: 378 average loss: 0.864\n",
      "Test Accuracy : 65.4%, Test Loss: 1.4061689624786378\n",
      "Epoch Time (Training + Test) = 21.57 seconds\n",
      "epoch: 379 average loss: 0.856\n",
      "Test Accuracy : 65.6%, Test Loss: 1.3800480842590332\n",
      "Epoch Time (Training + Test) = 21.37 seconds\n",
      "epoch: 380 average loss: 0.859\n",
      "Test Accuracy : 65.5%, Test Loss: 1.3884156517982482\n",
      "Epoch Time (Training + Test) = 21.45 seconds\n",
      "epoch: 381 average loss: 0.861\n",
      "Test Accuracy : 66.1%, Test Loss: 1.377318904399872\n",
      "Epoch Time (Training + Test) = 21.36 seconds\n",
      "epoch: 382 average loss: 0.857\n",
      "Test Accuracy : 65.6%, Test Loss: 1.3996780786514282\n",
      "Epoch Time (Training + Test) = 21.33 seconds\n",
      "epoch: 383 average loss: 0.869\n",
      "Test Accuracy : 65.1%, Test Loss: 1.4304875416755676\n",
      "Epoch Time (Training + Test) = 21.23 seconds\n",
      "epoch: 384 average loss: 0.860\n",
      "Test Accuracy : 65.3%, Test Loss: 1.4408140001296996\n",
      "Epoch Time (Training + Test) = 21.44 seconds\n",
      "epoch: 385 average loss: 0.858\n",
      "Test Accuracy : 65.3%, Test Loss: 1.406610713481903\n",
      "Epoch Time (Training + Test) = 21.76 seconds\n",
      "epoch: 386 average loss: 0.856\n",
      "Test Accuracy : 66.2%, Test Loss: 1.3911822109222411\n",
      "Epoch Time (Training + Test) = 21.31 seconds\n",
      "epoch: 387 average loss: 0.859\n",
      "Test Accuracy : 66.7%, Test Loss: 1.3562068252563477\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 21.34 seconds\n",
      "epoch: 388 average loss: 0.862\n",
      "Test Accuracy : 65.4%, Test Loss: 1.3828156390190125\n",
      "Epoch Time (Training + Test) = 21.57 seconds\n",
      "epoch: 389 average loss: 0.849\n",
      "Test Accuracy : 65.6%, Test Loss: 1.4042281665802\n",
      "Epoch Time (Training + Test) = 21.44 seconds\n",
      "epoch: 390 average loss: 0.858\n",
      "Test Accuracy : 66.5%, Test Loss: 1.3642283535003663\n",
      "Epoch Time (Training + Test) = 21.41 seconds\n",
      "epoch: 391 average loss: 0.850\n",
      "Test Accuracy : 65.6%, Test Loss: 1.387199676990509\n",
      "Epoch Time (Training + Test) = 21.35 seconds\n",
      "epoch: 392 average loss: 0.849\n",
      "Test Accuracy : 65.6%, Test Loss: 1.3974819979667663\n",
      "Epoch Time (Training + Test) = 21.51 seconds\n",
      "epoch: 393 average loss: 0.855\n",
      "Test Accuracy : 66.0%, Test Loss: 1.3818907980918884\n",
      "Epoch Time (Training + Test) = 21.47 seconds\n",
      "epoch: 394 average loss: 0.853\n",
      "Test Accuracy : 66.1%, Test Loss: 1.367976655960083\n",
      "Epoch Time (Training + Test) = 21.56 seconds\n",
      "epoch: 395 average loss: 0.849\n",
      "Test Accuracy : 65.3%, Test Loss: 1.4054716920852661\n",
      "Epoch Time (Training + Test) = 21.46 seconds\n",
      "epoch: 396 average loss: 0.847\n",
      "Test Accuracy : 66.1%, Test Loss: 1.3684502444267272\n",
      "Epoch Time (Training + Test) = 21.44 seconds\n",
      "epoch: 397 average loss: 0.848\n",
      "Test Accuracy : 65.8%, Test Loss: 1.394579149723053\n",
      "Epoch Time (Training + Test) = 21.42 seconds\n",
      "epoch: 398 average loss: 0.861\n",
      "Test Accuracy : 65.5%, Test Loss: 1.437452497959137\n",
      "Epoch Time (Training + Test) = 21.38 seconds\n",
      "epoch: 399 average loss: 0.853\n",
      "Test Accuracy : 65.8%, Test Loss: 1.4125633659362793\n",
      "Epoch Time (Training + Test) = 21.32 seconds\n",
      "epoch: 400 average loss: 0.856\n",
      "Test Accuracy : 64.4%, Test Loss: 1.4349976372718811\n",
      "Epoch Time (Training + Test) = 21.60 seconds\n",
      "Data Saved to ResNet18_Baseline.csv\n",
      "Finished Training: \n",
      "Total Time 4.809646 hours\n",
      " Average Time Per Epoch 43.29 seconds\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(Baseline,model_name='ResNet18_Baseline',project_name='UniAdapt_Cifar100',binarise=False,classes=train_80.classes)\n",
    "trainer.lr = 1e-3\n",
    "trainer.batch_size = 64\n",
    "trainer.epochs =400\n",
    "trainer.epoch_chkpts = []\n",
    "trainer.start_epoch = 0\n",
    "trainer.set_scheduler(None)\n",
    "trainer.set_optimizer(torch.optim.Adam)\n",
    "trainer.train(train80_DL,test80_DL)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b19b9b8f",
   "metadata": {},
   "source": [
    "# Cifar80 FineTune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f42ab199",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_transforms(dataset,train= True):\n",
    "    def normalize_channels(data):\n",
    "    #We have a nxCxWxH array\n",
    "        d = data\n",
    "        d = torch.flatten(data,2,-1)\n",
    "        mean= torch.mean(d,dim = [0,2])\n",
    "        std = torch.std(d,dim = [0,2])\n",
    "        return mean,std\n",
    "\n",
    "    mean,std = normalize_channels(dataset.data)\n",
    "    if train:\n",
    "        transform = transforms.Compose([\n",
    "            # transforms.ToTensor(),\n",
    "            transforms.RandomCrop(32, padding=4),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.Normalize(mean, std),\n",
    "        ])\n",
    "    else:\n",
    "        transform = transforms.Compose([\n",
    "        transforms.Normalize(mean, std),\n",
    "        ])\n",
    "\n",
    "    dataset.transform = transform\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7bcb9421",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_channels(data):\n",
    "    #We have a nxCxWxH array\n",
    "        d = data\n",
    "        d = torch.flatten(data,2,-1)\n",
    "        mean= torch.mean(d,dim = [0,2])\n",
    "        std = torch.std(d,dim = [0,2])\n",
    "        return mean,std\n",
    "\n",
    "mean,std = normalize_channels(train_80.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "587ae46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar20_path = r'data\\cifar-100\\cifar-20'\n",
    "\n",
    "\n",
    "train_20 = cifar_100_split(cifar20_path)\n",
    "test_20 = cifar_100_split(cifar20_path,train = False)\n",
    "\n",
    "\n",
    "# transform_train = transforms.Compose([transforms.Resize((32,32)),  #resises the image so it can be perfect for our model.\n",
    "#                                       transforms.RandomCrop(32, padding=4),\n",
    "#                                       transforms.RandomHorizontalFlip(), # FLips the image w.r.t horizontal axis\n",
    "#                                       transforms.RandomRotation(10),     #Rotates the image to a specified angel\n",
    "#                                       transforms.RandomAffine(0, shear=10, scale=(0.8,1.2)), #Performs actions like zooms, change shear angles.\n",
    "#                                       transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2), # Set the color params\n",
    "#                                       transforms.Normalize(mean,std) #Normalize all the images\n",
    "#                                ])\n",
    "\n",
    "\n",
    "\n",
    "transform_train = transforms.Compose([  #resises the image so it can be perfect for our model.\n",
    "                                      transforms.RandomCrop(32, padding=4),\n",
    "                                      transforms.RandomHorizontalFlip(), # FLips the image w.r.t horizontal axis\n",
    "                                      transforms.Normalize(mean,std) #Normalize all the images\n",
    "                               ])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "transform_test = transforms.Compose([  #resises the image so it can be perfect for our model.\n",
    "                                      transforms.Normalize(mean,std) #Normalize all the images\n",
    "                               ])\n",
    "\n",
    "train_20.transform = transform_train\n",
    "test_20.transform = transform_test\n",
    "train20_DL = DataLoader(train_20,batch_size=64,shuffle= True,num_workers=4)\n",
    "test20_DL = DataLoader(test_20,batch_size=64,shuffle= False,num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fc0faa5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normalize_channels(train_20.data),normalize_channels(train_80.data)\n",
    "torch.max(test_20.data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0b9d045a",
   "metadata": {},
   "source": [
    "## Regular Finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2ddb7027",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'SavedModels\\UniAdapt_Cifar100\\BNN_Cifar80_Backbone_2023-04-18 16-54-59\\BNN_Cifar80_Backbone_best_acc.pth'\n",
    "\n",
    "state = torch.load(path,map_location='cuda:0')\n",
    "BNN = BNN_Resnet_UniAdapt(80,freeze_pre_weights=False)\n",
    "BNN.layer3[-1].do_bntan = True\n",
    "# for key in state.keys():\n",
    "#     if 'decoders' in key or 'encoders' in key:\n",
    "#         del state[key] \n",
    "\n",
    "BNN.load_state_dict(state,strict=False)\n",
    "\n",
    "BNN = BNN.to('cuda:0')\n",
    "BNN.freeze()\n",
    "#For Regular FineTune\n",
    "BNN.uniAdapt = False\n",
    "BNN.fc = nn.Linear(320,20)\n",
    "BNN.bn3 = nn.BatchNorm1d(20)\n",
    "BNN.logsoftmax = placeholder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fa895c8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No lr scheduler\n",
      "Optimizer Set to New: {'state': {}, 'param_groups': [{'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': False, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54]}]}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2q8714ok) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Current Best Acc</td><td>▁▇██████████████████████████████████████</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>epoch time (s)</td><td>▃▅█▃▃▄▂▁▁▂▂▁▂▂▂▃▃▄▄▄▃▂▁▂▁▂▂▂▂▂▃▄▃▁▂▂▂▁▂▂</td></tr><tr><td>lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_accuracy</td><td>▁▇██████████████████████████████████████</td></tr><tr><td>test_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>training_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Best Accuracy</td><td>0.6495</td></tr><tr><td>Current Best Acc</td><td>0.6495</td></tr><tr><td>Total Time (hours)</td><td>0.34351</td></tr><tr><td>epoch</td><td>75</td></tr><tr><td>epoch time (s)</td><td>8.16681</td></tr><tr><td>lr</td><td>0.0001</td></tr><tr><td>test_accuracy</td><td>0.643</td></tr><tr><td>test_loss</td><td>1.11661</td></tr><tr><td>training_loss</td><td>1.21797</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">expert-snowflake-9</strong>: <a href=\"https://wandb.ai/johnny_suu/UniAdapt_Cifar100_Finetuning/runs/2q8714ok\" target=\"_blank\">https://wandb.ai/johnny_suu/UniAdapt_Cifar100_Finetuning/runs/2q8714ok</a><br/>Synced 5 W&B file(s), 1 media file(s), 1 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230421_112013-2q8714ok\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2q8714ok). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8008d34217a4bf6a33fb404b826d143",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01693333333435779, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\John Su\\Downloads\\SydneyUni\\2022\\thesis\\Thesis\\wandb\\run-20230421_113048-3ng27rkq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/johnny_suu/UniAdapt_Cifar100_Finetuning/runs/3ng27rkq\" target=\"_blank\">dandy-dust-10</a></strong> to <a href=\"https://wandb.ai/johnny_suu/UniAdapt_Cifar100_Finetuning\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "Project Name: UniAdapt_Cifar100_Finetuning, Run Name BNN_UniAdapt_finetune_head_only_linearFP32 \n",
      "\n",
      "\n",
      "Run Start : 2023-04-21 11-30-48\n",
      "start_epoch : 0\n",
      "initial_lr : 0.0001\n",
      "batch_size : 64\n",
      "epochs : 75\n",
      "epoch_chkpts : []\n",
      "device : cuda:0\n",
      "criterion : CrossEntropyLoss()\n",
      "optimizer : Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: False\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "seed : None\n",
      "model_architecture : <class 'NN_Thesis.nn_classes.BNN_Resnet_UniAdapt'>\n",
      "binerised_training : True\n",
      "Number of Elements : 4380700\n",
      "Initial accuracy:\n",
      "Test Accuracy : 5.5%, Test Loss: 3.0401102160192597\n",
      "best_acc.pth saved!\n",
      "Test Accuracy : 4.5%, Test Loss: 3.0586748495697975\n",
      "epoch: 1 average loss: 1.781\n",
      "Test Accuracy : 65.6%, Test Loss: 1.3648990392684937\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 8.24 seconds\n",
      "epoch: 2 average loss: 1.428\n",
      "Test Accuracy : 66.5%, Test Loss: 1.2618993259966373\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 8.53 seconds\n",
      "epoch: 3 average loss: 1.331\n",
      "Test Accuracy : 67.9%, Test Loss: 1.1926457937806845\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 8.62 seconds\n",
      "epoch: 4 average loss: 1.257\n",
      "Test Accuracy : 68.7%, Test Loss: 1.1472926437854767\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 8.44 seconds\n",
      "epoch: 5 average loss: 1.218\n",
      "Test Accuracy : 69.8%, Test Loss: 1.1031925678253174\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 8.47 seconds\n",
      "epoch: 6 average loss: 1.181\n",
      "Test Accuracy : 70.2%, Test Loss: 1.0663295481353998\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 8.49 seconds\n",
      "epoch: 7 average loss: 1.155\n",
      "Test Accuracy : 68.8%, Test Loss: 1.0555877164006233\n",
      "Epoch Time (Training + Test) = 8.29 seconds\n",
      "epoch: 8 average loss: 1.131\n",
      "Test Accuracy : 69.2%, Test Loss: 1.0609243586659431\n",
      "Epoch Time (Training + Test) = 8.18 seconds\n",
      "epoch: 9 average loss: 1.115\n",
      "Test Accuracy : 69.8%, Test Loss: 1.0164097882807255\n",
      "Epoch Time (Training + Test) = 8.04 seconds\n",
      "epoch: 10 average loss: 1.088\n",
      "Test Accuracy : 69.5%, Test Loss: 1.0211362969130278\n",
      "Epoch Time (Training + Test) = 8.29 seconds\n",
      "epoch: 11 average loss: 1.075\n",
      "Test Accuracy : 69.4%, Test Loss: 1.0134359281510115\n",
      "Epoch Time (Training + Test) = 8.22 seconds\n",
      "epoch: 12 average loss: 1.056\n",
      "Test Accuracy : 68.3%, Test Loss: 1.0029247123748064\n",
      "Epoch Time (Training + Test) = 8.37 seconds\n",
      "epoch: 13 average loss: 1.051\n",
      "Test Accuracy : 70.5%, Test Loss: 1.0057137366384268\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 8.46 seconds\n",
      "epoch: 14 average loss: 1.051\n",
      "Test Accuracy : 69.2%, Test Loss: 0.9999330807477236\n",
      "Epoch Time (Training + Test) = 9.12 seconds\n",
      "epoch: 15 average loss: 1.034\n",
      "Test Accuracy : 69.8%, Test Loss: 0.972868911921978\n",
      "Epoch Time (Training + Test) = 8.32 seconds\n",
      "epoch: 16 average loss: 1.025\n",
      "Test Accuracy : 70.1%, Test Loss: 0.980212926864624\n",
      "Epoch Time (Training + Test) = 7.97 seconds\n",
      "epoch: 17 average loss: 1.016\n",
      "Test Accuracy : 70.0%, Test Loss: 0.9736447054892778\n",
      "Epoch Time (Training + Test) = 8.51 seconds\n",
      "epoch: 18 average loss: 1.017\n",
      "Test Accuracy : 71.2%, Test Loss: 0.9586750231683254\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 8.49 seconds\n",
      "epoch: 19 average loss: 1.004\n",
      "Test Accuracy : 70.8%, Test Loss: 0.9601123947650194\n",
      "Epoch Time (Training + Test) = 8.65 seconds\n",
      "epoch: 20 average loss: 1.008\n",
      "Test Accuracy : 71.2%, Test Loss: 0.9493983574211597\n",
      "Epoch Time (Training + Test) = 8.60 seconds\n",
      "epoch: 21 average loss: 1.005\n",
      "Test Accuracy : 69.8%, Test Loss: 0.9663024321198463\n",
      "Epoch Time (Training + Test) = 8.31 seconds\n",
      "epoch: 22 average loss: 1.005\n",
      "Test Accuracy : 70.2%, Test Loss: 0.9568177070468664\n",
      "Epoch Time (Training + Test) = 8.36 seconds\n",
      "epoch: 23 average loss: 0.987\n",
      "Test Accuracy : 70.7%, Test Loss: 0.9418976530432701\n",
      "Epoch Time (Training + Test) = 8.54 seconds\n",
      "epoch: 24 average loss: 0.983\n",
      "Test Accuracy : 71.1%, Test Loss: 0.9529403131455183\n",
      "Epoch Time (Training + Test) = 8.18 seconds\n",
      "epoch: 25 average loss: 0.983\n",
      "Test Accuracy : 70.3%, Test Loss: 0.9253497235476971\n",
      "Epoch Time (Training + Test) = 8.35 seconds\n",
      "epoch: 26 average loss: 0.984\n",
      "Test Accuracy : 69.0%, Test Loss: 0.9573866277933121\n",
      "Epoch Time (Training + Test) = 8.15 seconds\n",
      "epoch: 27 average loss: 0.967\n",
      "Test Accuracy : 70.8%, Test Loss: 0.9332706741988659\n",
      "Epoch Time (Training + Test) = 8.45 seconds\n",
      "epoch: 28 average loss: 0.979\n",
      "Test Accuracy : 69.5%, Test Loss: 0.9514313172549009\n",
      "Epoch Time (Training + Test) = 8.25 seconds\n",
      "epoch: 29 average loss: 0.963\n",
      "Test Accuracy : 71.8%, Test Loss: 0.9206881243735552\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 8.11 seconds\n",
      "epoch: 30 average loss: 0.964\n",
      "Test Accuracy : 70.3%, Test Loss: 0.9342508260160685\n",
      "Epoch Time (Training + Test) = 8.12 seconds\n",
      "epoch: 31 average loss: 0.974\n",
      "Test Accuracy : 70.1%, Test Loss: 0.9451904259622097\n",
      "Epoch Time (Training + Test) = 8.22 seconds\n",
      "epoch: 32 average loss: 0.954\n",
      "Test Accuracy : 71.4%, Test Loss: 0.9181464798748493\n",
      "Epoch Time (Training + Test) = 8.17 seconds\n",
      "epoch: 33 average loss: 0.962\n",
      "Test Accuracy : 69.8%, Test Loss: 0.939261032268405\n",
      "Epoch Time (Training + Test) = 8.05 seconds\n",
      "epoch: 34 average loss: 0.958\n",
      "Test Accuracy : 70.0%, Test Loss: 0.9358572531491518\n",
      "Epoch Time (Training + Test) = 8.18 seconds\n",
      "epoch: 35 average loss: 0.964\n",
      "Test Accuracy : 70.0%, Test Loss: 0.9520111382007599\n",
      "Epoch Time (Training + Test) = 8.39 seconds\n",
      "epoch: 36 average loss: 0.965\n",
      "Test Accuracy : 69.7%, Test Loss: 0.9393685348331928\n",
      "Epoch Time (Training + Test) = 8.40 seconds\n",
      "epoch: 37 average loss: 0.962\n",
      "Test Accuracy : 71.0%, Test Loss: 0.9177303221076727\n",
      "Epoch Time (Training + Test) = 8.43 seconds\n",
      "epoch: 38 average loss: 0.957\n",
      "Test Accuracy : 70.3%, Test Loss: 0.9252455346286297\n",
      "Epoch Time (Training + Test) = 8.49 seconds\n",
      "epoch: 39 average loss: 0.967\n",
      "Test Accuracy : 70.2%, Test Loss: 0.9427818935364485\n",
      "Epoch Time (Training + Test) = 8.31 seconds\n",
      "epoch: 40 average loss: 0.956\n",
      "Test Accuracy : 71.0%, Test Loss: 0.9341510497033596\n",
      "Epoch Time (Training + Test) = 8.26 seconds\n",
      "epoch: 41 average loss: 0.955\n",
      "Test Accuracy : 69.7%, Test Loss: 0.9279453940689564\n",
      "Epoch Time (Training + Test) = 8.15 seconds\n",
      "epoch: 42 average loss: 0.951\n",
      "Test Accuracy : 69.7%, Test Loss: 0.947081632912159\n",
      "Epoch Time (Training + Test) = 8.33 seconds\n",
      "epoch: 43 average loss: 0.946\n",
      "Test Accuracy : 70.3%, Test Loss: 0.9315853379666805\n",
      "Epoch Time (Training + Test) = 8.79 seconds\n",
      "epoch: 44 average loss: 0.934\n",
      "Test Accuracy : 71.7%, Test Loss: 0.9001462198793888\n",
      "Epoch Time (Training + Test) = 8.30 seconds\n",
      "epoch: 45 average loss: 0.944\n",
      "Test Accuracy : 70.2%, Test Loss: 0.9330122601240873\n",
      "Epoch Time (Training + Test) = 7.79 seconds\n",
      "epoch: 46 average loss: 0.960\n",
      "Test Accuracy : 70.0%, Test Loss: 0.9217263478785753\n",
      "Epoch Time (Training + Test) = 8.00 seconds\n",
      "epoch: 47 average loss: 0.934\n",
      "Test Accuracy : 71.6%, Test Loss: 0.9269493315368891\n",
      "Epoch Time (Training + Test) = 8.07 seconds\n",
      "epoch: 48 average loss: 0.929\n",
      "Test Accuracy : 69.8%, Test Loss: 0.9233188759535551\n",
      "Epoch Time (Training + Test) = 8.23 seconds\n",
      "epoch: 49 average loss: 0.931\n",
      "Test Accuracy : 70.7%, Test Loss: 0.9295008685439825\n",
      "Epoch Time (Training + Test) = 8.07 seconds\n",
      "epoch: 50 average loss: 0.944\n",
      "Test Accuracy : 69.8%, Test Loss: 0.9182390384376049\n",
      "Epoch Time (Training + Test) = 8.06 seconds\n",
      "epoch: 51 average loss: 0.937\n",
      "Test Accuracy : 71.4%, Test Loss: 0.9031726736575365\n",
      "Epoch Time (Training + Test) = 7.82 seconds\n",
      "epoch: 52 average loss: 0.942\n",
      "Test Accuracy : 70.7%, Test Loss: 0.9096962809562683\n",
      "Epoch Time (Training + Test) = 7.79 seconds\n",
      "epoch: 53 average loss: 0.937\n",
      "Test Accuracy : 70.5%, Test Loss: 0.9207360278815031\n",
      "Epoch Time (Training + Test) = 7.87 seconds\n",
      "epoch: 54 average loss: 0.937\n",
      "Test Accuracy : 70.1%, Test Loss: 0.919640026986599\n",
      "Epoch Time (Training + Test) = 7.85 seconds\n",
      "epoch: 55 average loss: 0.941\n",
      "Test Accuracy : 70.3%, Test Loss: 0.921757573261857\n",
      "Epoch Time (Training + Test) = 8.27 seconds\n",
      "epoch: 56 average loss: 0.937\n",
      "Test Accuracy : 70.3%, Test Loss: 0.9292113520205021\n",
      "Epoch Time (Training + Test) = 8.06 seconds\n",
      "epoch: 57 average loss: 0.935\n",
      "Test Accuracy : 70.5%, Test Loss: 0.9434259925037622\n",
      "Epoch Time (Training + Test) = 8.21 seconds\n",
      "epoch: 58 average loss: 0.932\n",
      "Test Accuracy : 70.9%, Test Loss: 0.9170527905225754\n",
      "Epoch Time (Training + Test) = 8.18 seconds\n",
      "epoch: 59 average loss: 0.930\n",
      "Test Accuracy : 71.2%, Test Loss: 0.9146535750478506\n",
      "Epoch Time (Training + Test) = 8.37 seconds\n",
      "epoch: 60 average loss: 0.938\n",
      "Test Accuracy : 70.3%, Test Loss: 0.913553711026907\n",
      "Epoch Time (Training + Test) = 7.93 seconds\n",
      "epoch: 61 average loss: 0.941\n",
      "Test Accuracy : 71.0%, Test Loss: 0.9071129262447357\n",
      "Epoch Time (Training + Test) = 7.85 seconds\n",
      "epoch: 62 average loss: 0.940\n",
      "Test Accuracy : 70.8%, Test Loss: 0.9160786494612694\n",
      "Epoch Time (Training + Test) = 8.18 seconds\n",
      "epoch: 63 average loss: 0.930\n",
      "Test Accuracy : 70.2%, Test Loss: 0.9133850391954184\n",
      "Epoch Time (Training + Test) = 8.72 seconds\n",
      "epoch: 64 average loss: 0.929\n",
      "Test Accuracy : 70.0%, Test Loss: 0.916483748704195\n",
      "Epoch Time (Training + Test) = 8.33 seconds\n",
      "epoch: 65 average loss: 0.926\n",
      "Test Accuracy : 70.8%, Test Loss: 0.9200609475374222\n",
      "Epoch Time (Training + Test) = 8.42 seconds\n",
      "epoch: 66 average loss: 0.922\n",
      "Test Accuracy : 69.6%, Test Loss: 0.9314313288778067\n",
      "Epoch Time (Training + Test) = 8.23 seconds\n",
      "epoch: 67 average loss: 0.926\n",
      "Test Accuracy : 70.7%, Test Loss: 0.9121907763183117\n",
      "Epoch Time (Training + Test) = 8.43 seconds\n",
      "epoch: 68 average loss: 0.922\n",
      "Test Accuracy : 71.2%, Test Loss: 0.9226048141717911\n",
      "Epoch Time (Training + Test) = 8.00 seconds\n",
      "epoch: 69 average loss: 0.932\n",
      "Test Accuracy : 70.3%, Test Loss: 0.9206565991044044\n",
      "Epoch Time (Training + Test) = 8.07 seconds\n",
      "epoch: 70 average loss: 0.931\n",
      "Test Accuracy : 70.1%, Test Loss: 0.9038454052060843\n",
      "Epoch Time (Training + Test) = 7.99 seconds\n",
      "epoch: 71 average loss: 0.928\n",
      "Test Accuracy : 71.8%, Test Loss: 0.9125382956117392\n",
      "Epoch Time (Training + Test) = 7.87 seconds\n",
      "epoch: 72 average loss: 0.930\n",
      "Test Accuracy : 70.0%, Test Loss: 0.930737916380167\n",
      "Epoch Time (Training + Test) = 8.63 seconds\n",
      "epoch: 73 average loss: 0.932\n",
      "Test Accuracy : 70.3%, Test Loss: 0.9144103769212961\n",
      "Epoch Time (Training + Test) = 9.38 seconds\n",
      "epoch: 74 average loss: 0.933\n",
      "Test Accuracy : 70.7%, Test Loss: 0.9227229617536068\n",
      "Epoch Time (Training + Test) = 8.51 seconds\n",
      "epoch: 75 average loss: 0.935\n",
      "Test Accuracy : 69.0%, Test Loss: 0.9332192409783602\n",
      "Epoch Time (Training + Test) = 8.59 seconds\n",
      "Data Saved to BNN_UniAdapt_finetune_head_only_linearFP32.csv\n",
      "Finished Training: \n",
      "Total Time 0.345218 hours\n",
      " Average Time Per Epoch 16.57 seconds\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(BNN,model_name='BNN_UniAdapt_finetune_head_only_linearFP32',project_name='UniAdapt_Cifar100_Finetuning',binarise=True,classes=train_20.classes)\n",
    "trainer.lr = 1e-4\n",
    "trainer.batch_size = 64\n",
    "trainer.epochs =75\n",
    "trainer.epoch_chkpts = []\n",
    "trainer.start_epoch = 0\n",
    "params = nn.ModuleList([BNN.fc,BNN.bn3]).parameters()\n",
    "trainer.set_scheduler(None)\n",
    "trainer.set_optimizer(torch.optim.Adam,lr = 1e-3)\n",
    "trainer.train(train20_DL,test20_DL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "51efc14b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No lr scheduler\n",
      "Optimizer Set to New: {'state': {}, 'param_groups': [{'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': False, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54]}]}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2g31vlmk) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7bb31939f884e0b83c23134cc34d871",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Current Best Acc</td><td>▁▅▇▇▇███████████████████████████████████</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>epoch time (s)</td><td>▁▇▇▇▇▇██▇▇▇█▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▆▇▇▆▇▇▇▇▇</td></tr><tr><td>lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_accuracy</td><td>▁▅▇▇▇███████████████▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>test_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>training_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Best Accuracy</td><td>0.743</td></tr><tr><td>Current Best Acc</td><td>0.743</td></tr><tr><td>Total Time (hours)</td><td>0.43383</td></tr><tr><td>epoch</td><td>75</td></tr><tr><td>epoch time (s)</td><td>10.38404</td></tr><tr><td>lr</td><td>0.0001</td></tr><tr><td>test_accuracy</td><td>0.657</td></tr><tr><td>test_loss</td><td>1.16758</td></tr><tr><td>training_loss</td><td>1.22072</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">generous-music-8</strong>: <a href=\"https://wandb.ai/johnny_suu/UniAdapt_Cifar100_Finetuning/runs/2g31vlmk\" target=\"_blank\">https://wandb.ai/johnny_suu/UniAdapt_Cifar100_Finetuning/runs/2g31vlmk</a><br/>Synced 5 W&B file(s), 1 media file(s), 1 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230421_110442-2g31vlmk\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2g31vlmk). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bacc2829a8b04f02b298d8934a32fd74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\John Su\\Downloads\\SydneyUni\\2022\\thesis\\Thesis\\wandb\\run-20230421_112013-2q8714ok</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/johnny_suu/UniAdapt_Cifar100_Finetuning/runs/2q8714ok\" target=\"_blank\">expert-snowflake-9</a></strong> to <a href=\"https://wandb.ai/johnny_suu/UniAdapt_Cifar100_Finetuning\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "Project Name: UniAdapt_Cifar100_Finetuning, Run Name BNN_UniAdapt_finetune_head_only \n",
      "\n",
      "\n",
      "Run Start : 2023-04-21 11-20-13\n",
      "start_epoch : 0\n",
      "initial_lr : 0.0001\n",
      "batch_size : 64\n",
      "epochs : 75\n",
      "epoch_chkpts : []\n",
      "device : cuda:0\n",
      "criterion : CrossEntropyLoss()\n",
      "optimizer : Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: False\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "seed : None\n",
      "model_architecture : <class 'NN_Thesis.nn_classes.BNN_Resnet_UniAdapt'>\n",
      "binerised_training : True\n",
      "Number of Elements : 4380700\n",
      "Initial accuracy:\n",
      "Test Accuracy : 6.2%, Test Loss: 32.79522096427383\n",
      "best_acc.pth saved!\n",
      "Test Accuracy : 6.5%, Test Loss: 31.999305069446564\n",
      "best_acc.pth saved!\n",
      "epoch: 1 average loss: 2.054\n",
      "Test Accuracy : 59.8%, Test Loss: 1.5446261018514633\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 8.59 seconds\n",
      "epoch: 2 average loss: 1.569\n",
      "Test Accuracy : 62.6%, Test Loss: 1.4327696599066257\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 8.72 seconds\n",
      "epoch: 3 average loss: 1.485\n",
      "Test Accuracy : 62.5%, Test Loss: 1.3569957613945007\n",
      "Epoch Time (Training + Test) = 9.08 seconds\n",
      "epoch: 4 average loss: 1.436\n",
      "Test Accuracy : 62.3%, Test Loss: 1.3314133360981941\n",
      "Epoch Time (Training + Test) = 8.60 seconds\n",
      "epoch: 5 average loss: 1.397\n",
      "Test Accuracy : 62.8%, Test Loss: 1.2911067977547646\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 8.31 seconds\n",
      "epoch: 6 average loss: 1.384\n",
      "Test Accuracy : 63.3%, Test Loss: 1.2666727229952812\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 8.41 seconds\n",
      "epoch: 7 average loss: 1.352\n",
      "Test Accuracy : 62.4%, Test Loss: 1.2611619420349598\n",
      "Epoch Time (Training + Test) = 8.36 seconds\n",
      "epoch: 8 average loss: 1.343\n",
      "Test Accuracy : 64.2%, Test Loss: 1.242590706795454\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 8.28 seconds\n",
      "epoch: 9 average loss: 1.331\n",
      "Test Accuracy : 63.4%, Test Loss: 1.2337743751704693\n",
      "Epoch Time (Training + Test) = 8.46 seconds\n",
      "epoch: 10 average loss: 1.299\n",
      "Test Accuracy : 64.4%, Test Loss: 1.2154787592589855\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 8.12 seconds\n",
      "epoch: 11 average loss: 1.295\n",
      "Test Accuracy : 62.9%, Test Loss: 1.2048283908516169\n",
      "Epoch Time (Training + Test) = 8.16 seconds\n",
      "epoch: 12 average loss: 1.302\n",
      "Test Accuracy : 63.3%, Test Loss: 1.2150535732507706\n",
      "Epoch Time (Training + Test) = 8.16 seconds\n",
      "epoch: 13 average loss: 1.275\n",
      "Test Accuracy : 63.5%, Test Loss: 1.2142939008772373\n",
      "Epoch Time (Training + Test) = 8.03 seconds\n",
      "epoch: 14 average loss: 1.278\n",
      "Test Accuracy : 63.5%, Test Loss: 1.185693895444274\n",
      "Epoch Time (Training + Test) = 8.15 seconds\n",
      "epoch: 15 average loss: 1.282\n",
      "Test Accuracy : 63.1%, Test Loss: 1.185135843232274\n",
      "Epoch Time (Training + Test) = 8.10 seconds\n",
      "epoch: 16 average loss: 1.268\n",
      "Test Accuracy : 63.3%, Test Loss: 1.178067497909069\n",
      "Epoch Time (Training + Test) = 8.24 seconds\n",
      "epoch: 17 average loss: 1.267\n",
      "Test Accuracy : 63.4%, Test Loss: 1.1689764484763145\n",
      "Epoch Time (Training + Test) = 8.15 seconds\n",
      "epoch: 18 average loss: 1.261\n",
      "Test Accuracy : 63.4%, Test Loss: 1.1543807126581669\n",
      "Epoch Time (Training + Test) = 8.02 seconds\n",
      "epoch: 19 average loss: 1.265\n",
      "Test Accuracy : 62.8%, Test Loss: 1.1693324483931065\n",
      "Epoch Time (Training + Test) = 8.17 seconds\n",
      "epoch: 20 average loss: 1.263\n",
      "Test Accuracy : 61.6%, Test Loss: 1.1841828431934118\n",
      "Epoch Time (Training + Test) = 8.26 seconds\n",
      "epoch: 21 average loss: 1.258\n",
      "Test Accuracy : 64.0%, Test Loss: 1.1620134599506855\n",
      "Epoch Time (Training + Test) = 8.09 seconds\n",
      "epoch: 22 average loss: 1.256\n",
      "Test Accuracy : 63.2%, Test Loss: 1.1837623920291662\n",
      "Epoch Time (Training + Test) = 8.20 seconds\n",
      "epoch: 23 average loss: 1.244\n",
      "Test Accuracy : 61.8%, Test Loss: 1.1900746691972017\n",
      "Epoch Time (Training + Test) = 8.17 seconds\n",
      "epoch: 24 average loss: 1.241\n",
      "Test Accuracy : 63.2%, Test Loss: 1.1683454401791096\n",
      "Epoch Time (Training + Test) = 8.25 seconds\n",
      "epoch: 25 average loss: 1.231\n",
      "Test Accuracy : 62.9%, Test Loss: 1.167293306440115\n",
      "Epoch Time (Training + Test) = 8.17 seconds\n",
      "epoch: 26 average loss: 1.243\n",
      "Test Accuracy : 64.3%, Test Loss: 1.1342728585004807\n",
      "Epoch Time (Training + Test) = 8.11 seconds\n",
      "epoch: 27 average loss: 1.236\n",
      "Test Accuracy : 63.8%, Test Loss: 1.1602328922599554\n",
      "Epoch Time (Training + Test) = 8.09 seconds\n",
      "epoch: 28 average loss: 1.243\n",
      "Test Accuracy : 64.0%, Test Loss: 1.147260133177042\n",
      "Epoch Time (Training + Test) = 8.27 seconds\n",
      "epoch: 29 average loss: 1.230\n",
      "Test Accuracy : 63.8%, Test Loss: 1.1362950783222914\n",
      "Epoch Time (Training + Test) = 8.50 seconds\n",
      "epoch: 30 average loss: 1.243\n",
      "Test Accuracy : 63.2%, Test Loss: 1.1414870377629995\n",
      "Epoch Time (Training + Test) = 8.40 seconds\n",
      "epoch: 31 average loss: 1.224\n",
      "Test Accuracy : 63.0%, Test Loss: 1.1530970633029938\n",
      "Epoch Time (Training + Test) = 8.46 seconds\n",
      "epoch: 32 average loss: 1.244\n",
      "Test Accuracy : 62.8%, Test Loss: 1.1455207113176584\n",
      "Epoch Time (Training + Test) = 8.47 seconds\n",
      "epoch: 33 average loss: 1.230\n",
      "Test Accuracy : 63.4%, Test Loss: 1.1420060861855745\n",
      "Epoch Time (Training + Test) = 8.39 seconds\n",
      "epoch: 34 average loss: 1.226\n",
      "Test Accuracy : 64.4%, Test Loss: 1.1377641502767801\n",
      "Epoch Time (Training + Test) = 8.44 seconds\n",
      "epoch: 35 average loss: 1.224\n",
      "Test Accuracy : 64.3%, Test Loss: 1.125816721469164\n",
      "Epoch Time (Training + Test) = 8.44 seconds\n",
      "epoch: 36 average loss: 1.219\n",
      "Test Accuracy : 61.7%, Test Loss: 1.1662336103618145\n",
      "Epoch Time (Training + Test) = 8.50 seconds\n",
      "epoch: 37 average loss: 1.219\n",
      "Test Accuracy : 62.3%, Test Loss: 1.1393633764237165\n",
      "Epoch Time (Training + Test) = 8.15 seconds\n",
      "epoch: 38 average loss: 1.234\n",
      "Test Accuracy : 62.5%, Test Loss: 1.1540571358054876\n",
      "Epoch Time (Training + Test) = 8.28 seconds\n",
      "epoch: 39 average loss: 1.222\n",
      "Test Accuracy : 63.8%, Test Loss: 1.122935140505433\n",
      "Epoch Time (Training + Test) = 8.10 seconds\n",
      "epoch: 40 average loss: 1.220\n",
      "Test Accuracy : 63.1%, Test Loss: 1.1639943942427635\n",
      "Epoch Time (Training + Test) = 8.25 seconds\n",
      "epoch: 41 average loss: 1.219\n",
      "Test Accuracy : 63.6%, Test Loss: 1.1247438117861748\n",
      "Epoch Time (Training + Test) = 8.13 seconds\n",
      "epoch: 42 average loss: 1.229\n",
      "Test Accuracy : 63.7%, Test Loss: 1.1458993777632713\n",
      "Epoch Time (Training + Test) = 8.05 seconds\n",
      "epoch: 43 average loss: 1.231\n",
      "Test Accuracy : 63.8%, Test Loss: 1.1505886726081371\n",
      "Epoch Time (Training + Test) = 8.15 seconds\n",
      "epoch: 44 average loss: 1.231\n",
      "Test Accuracy : 63.7%, Test Loss: 1.1350413039326668\n",
      "Epoch Time (Training + Test) = 8.22 seconds\n",
      "epoch: 45 average loss: 1.219\n",
      "Test Accuracy : 64.8%, Test Loss: 1.1089385338127613\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 8.18 seconds\n",
      "epoch: 46 average loss: 1.233\n",
      "Test Accuracy : 64.0%, Test Loss: 1.1135173216462135\n",
      "Epoch Time (Training + Test) = 8.07 seconds\n",
      "epoch: 47 average loss: 1.222\n",
      "Test Accuracy : 63.6%, Test Loss: 1.1309460923075676\n",
      "Epoch Time (Training + Test) = 8.06 seconds\n",
      "epoch: 48 average loss: 1.233\n",
      "Test Accuracy : 64.2%, Test Loss: 1.1193441655486822\n",
      "Epoch Time (Training + Test) = 8.22 seconds\n",
      "epoch: 49 average loss: 1.218\n",
      "Test Accuracy : 63.7%, Test Loss: 1.122090645134449\n",
      "Epoch Time (Training + Test) = 8.14 seconds\n",
      "epoch: 50 average loss: 1.219\n",
      "Test Accuracy : 63.4%, Test Loss: 1.1361963637173176\n",
      "Epoch Time (Training + Test) = 8.14 seconds\n",
      "epoch: 51 average loss: 1.230\n",
      "Test Accuracy : 65.0%, Test Loss: 1.0977337416261435\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 8.14 seconds\n",
      "epoch: 52 average loss: 1.211\n",
      "Test Accuracy : 63.9%, Test Loss: 1.1046959720551968\n",
      "Epoch Time (Training + Test) = 8.23 seconds\n",
      "epoch: 53 average loss: 1.221\n",
      "Test Accuracy : 63.2%, Test Loss: 1.1531156729906797\n",
      "Epoch Time (Training + Test) = 8.16 seconds\n",
      "epoch: 54 average loss: 1.225\n",
      "Test Accuracy : 63.5%, Test Loss: 1.1190885249525309\n",
      "Epoch Time (Training + Test) = 8.08 seconds\n",
      "epoch: 55 average loss: 1.236\n",
      "Test Accuracy : 62.0%, Test Loss: 1.1389685552567244\n",
      "Epoch Time (Training + Test) = 8.14 seconds\n",
      "epoch: 56 average loss: 1.216\n",
      "Test Accuracy : 63.0%, Test Loss: 1.1326393131166697\n",
      "Epoch Time (Training + Test) = 8.40 seconds\n",
      "epoch: 57 average loss: 1.222\n",
      "Test Accuracy : 64.4%, Test Loss: 1.1223179176449776\n",
      "Epoch Time (Training + Test) = 8.33 seconds\n",
      "epoch: 58 average loss: 1.226\n",
      "Test Accuracy : 64.2%, Test Loss: 1.1161740943789482\n",
      "Epoch Time (Training + Test) = 8.39 seconds\n",
      "epoch: 59 average loss: 1.224\n",
      "Test Accuracy : 62.6%, Test Loss: 1.1473905220627785\n",
      "Epoch Time (Training + Test) = 8.44 seconds\n",
      "epoch: 60 average loss: 1.214\n",
      "Test Accuracy : 61.8%, Test Loss: 1.185084830969572\n",
      "Epoch Time (Training + Test) = 8.40 seconds\n",
      "epoch: 61 average loss: 1.219\n",
      "Test Accuracy : 62.6%, Test Loss: 1.1225665900856256\n",
      "Epoch Time (Training + Test) = 8.28 seconds\n",
      "epoch: 62 average loss: 1.205\n",
      "Test Accuracy : 61.6%, Test Loss: 1.1571108121424913\n",
      "Epoch Time (Training + Test) = 8.17 seconds\n",
      "epoch: 63 average loss: 1.217\n",
      "Test Accuracy : 62.0%, Test Loss: 1.1543085165321827\n",
      "Epoch Time (Training + Test) = 8.10 seconds\n",
      "epoch: 64 average loss: 1.223\n",
      "Test Accuracy : 62.2%, Test Loss: 1.1601655595004559\n",
      "Epoch Time (Training + Test) = 8.17 seconds\n",
      "epoch: 65 average loss: 1.225\n",
      "Test Accuracy : 63.9%, Test Loss: 1.1135606728494167\n",
      "Epoch Time (Training + Test) = 8.11 seconds\n",
      "epoch: 66 average loss: 1.214\n",
      "Test Accuracy : 63.5%, Test Loss: 1.1396916080266237\n",
      "Epoch Time (Training + Test) = 8.08 seconds\n",
      "epoch: 67 average loss: 1.217\n",
      "Test Accuracy : 62.6%, Test Loss: 1.145932225510478\n",
      "Epoch Time (Training + Test) = 8.11 seconds\n",
      "epoch: 68 average loss: 1.206\n",
      "Test Accuracy : 63.8%, Test Loss: 1.1200087033212185\n",
      "Epoch Time (Training + Test) = 8.23 seconds\n",
      "epoch: 69 average loss: 1.221\n",
      "Test Accuracy : 62.3%, Test Loss: 1.1574233118444681\n",
      "Epoch Time (Training + Test) = 8.25 seconds\n",
      "epoch: 70 average loss: 1.213\n",
      "Test Accuracy : 62.9%, Test Loss: 1.1420910693705082\n",
      "Epoch Time (Training + Test) = 8.08 seconds\n",
      "epoch: 71 average loss: 1.208\n",
      "Test Accuracy : 63.1%, Test Loss: 1.154937770217657\n",
      "Epoch Time (Training + Test) = 8.05 seconds\n",
      "epoch: 72 average loss: 1.212\n",
      "Test Accuracy : 61.8%, Test Loss: 1.1342436261475086\n",
      "Epoch Time (Training + Test) = 8.04 seconds\n",
      "epoch: 73 average loss: 1.211\n",
      "Test Accuracy : 63.9%, Test Loss: 1.1388056073337793\n",
      "Epoch Time (Training + Test) = 8.18 seconds\n",
      "epoch: 74 average loss: 1.226\n",
      "Test Accuracy : 63.2%, Test Loss: 1.1248210296034813\n",
      "Epoch Time (Training + Test) = 8.13 seconds\n",
      "epoch: 75 average loss: 1.218\n",
      "Test Accuracy : 64.3%, Test Loss: 1.116605518385768\n",
      "Epoch Time (Training + Test) = 8.17 seconds\n",
      "Data Saved to BNN_UniAdapt_finetune_head_only.csv\n",
      "Finished Training: \n",
      "Total Time 0.343507 hours\n",
      " Average Time Per Epoch 16.49 seconds\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(BNN,model_name='BNN_UniAdapt_finetune_head_only',project_name='UniAdapt_Cifar100_Finetuning',binarise=True,classes=train_20.classes)\n",
    "trainer.lr = 1e-4\n",
    "trainer.batch_size = 64\n",
    "trainer.epochs =75\n",
    "trainer.epoch_chkpts = []\n",
    "trainer.start_epoch = 0\n",
    "params = nn.ModuleList([BNN.fc,BNN.bn3]).parameters()\n",
    "trainer.set_scheduler(None)\n",
    "trainer.set_optimizer(torch.optim.Adam,lr = 1e-3)\n",
    "trainer.train(train20_DL,test20_DL)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4d3fbe10",
   "metadata": {},
   "source": [
    "# Serial Adapter FineTune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "05d93679",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'SavedModels\\UniAdapt_Cifar100\\BNN_Cifar80_Backbone_2023-04-18 16-54-59\\BNN_Cifar80_Backbone_best_acc.pth'\n",
    "\n",
    "state = torch.load(path,map_location='cuda:0')\n",
    "n = BNN_Resnet_UniAdapt(num_classes=80)\n",
    "n.load_state_dict(state)\n",
    "conv_bn = conv_channel_adapter3(80,kernel = 3, padding= 1)\n",
    "\n",
    "n.add_adapter(after='layer1',adapter=conv_bn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1203849c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Finetuned BN Head\n",
    "path = r'SavedModels\\UniAdapt_Cifar100_Finetuning\\BNN_UniAdapt_finetune_head_only_2023-04-21 11-20-13\\BNN_UniAdapt_finetune_head_only_best_acc.pth'\n",
    "\n",
    "state = torch.load(path,map_location='cuda:0')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cc0ed834",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 1.0000e+00, -1.1843e-01,  1.6836e-01,  ..., -7.0375e-01,\n",
       "            6.5388e-01,  3.3744e-01],\n",
       "          [-4.2800e-01,  1.0000e+00,  1.0000e+00,  ..., -5.3126e-01,\n",
       "           -3.5286e-01,  1.0000e+00],\n",
       "          [ 2.7405e-01, -2.9783e-01,  3.5821e-01,  ...,  1.9894e-01,\n",
       "            1.4052e-01,  1.0000e+00],\n",
       "          ...,\n",
       "          [-2.5928e-01, -1.5729e-01, -2.1653e-02,  ...,  2.7419e-01,\n",
       "           -5.4606e-02,  1.0000e+00],\n",
       "          [ 1.7323e-01,  1.0000e+00,  4.1396e-01,  ...,  1.0000e+00,\n",
       "            1.0000e+00,  1.0000e+00],\n",
       "          [ 5.8664e-01,  1.0000e+00,  1.0000e+00,  ...,  7.1569e-01,\n",
       "           -2.1845e-01, -4.4775e-01]],\n",
       "\n",
       "         [[ 4.7147e-01,  4.9231e-01,  1.0000e+00,  ..., -9.9781e-01,\n",
       "            3.6301e-02, -8.0490e-01],\n",
       "          [ 1.0000e+00,  1.0000e+00,  1.5996e-02,  ..., -4.6867e-01,\n",
       "           -1.7648e-01,  4.6351e-01],\n",
       "          [-1.2145e-01,  1.0000e+00, -1.0073e-02,  ...,  6.7911e-01,\n",
       "            1.0000e+00,  1.0000e+00],\n",
       "          ...,\n",
       "          [ 7.7220e-02, -5.6242e-01,  1.0000e+00,  ...,  5.3062e-01,\n",
       "            1.0000e+00,  5.2590e-01],\n",
       "          [-3.6945e-01,  1.0000e+00,  6.2187e-01,  ...,  1.0000e+00,\n",
       "            1.0000e+00,  8.9055e-01],\n",
       "          [ 1.0000e+00,  9.2796e-01,  1.0000e+00,  ..., -2.8017e-02,\n",
       "           -9.3721e-01, -6.3560e-01]],\n",
       "\n",
       "         [[ 7.3964e-01,  1.0000e+00, -5.9814e-01,  ...,  1.0000e+00,\n",
       "            1.0000e+00,  1.8361e-01],\n",
       "          [ 8.4749e-01,  1.0000e+00,  1.9565e-01,  ...,  1.0000e+00,\n",
       "            6.5170e-01,  1.0000e+00],\n",
       "          [ 3.4451e-01,  1.0000e+00,  1.0000e+00,  ...,  1.0000e+00,\n",
       "            1.1782e-01,  6.0910e-01],\n",
       "          ...,\n",
       "          [ 1.9039e-01,  1.0000e+00,  1.0689e-02,  ..., -5.2628e-01,\n",
       "           -7.4439e-01, -3.8461e-01],\n",
       "          [ 1.0000e+00, -6.0664e-01,  1.0000e+00,  ..., -6.1817e-01,\n",
       "            1.0000e+00, -9.1823e-01],\n",
       "          [ 3.9989e-01,  8.6367e-01,  1.0000e+00,  ..., -1.2664e-02,\n",
       "           -8.8682e-01,  9.2044e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-2.4527e-01,  1.0000e+00,  1.0000e+00,  ..., -5.7873e-01,\n",
       "            5.9705e-01,  1.0000e+00],\n",
       "          [ 9.7367e-01,  1.0000e+00,  8.0659e-01,  ..., -9.2262e-01,\n",
       "            9.1416e-01,  1.0000e+00],\n",
       "          [ 1.0000e+00, -5.6058e-01,  6.9360e-01,  ...,  4.5405e-01,\n",
       "            1.0000e+00,  1.0000e+00],\n",
       "          ...,\n",
       "          [ 6.3809e-01, -5.8626e-02,  6.4462e-01,  ...,  1.0000e+00,\n",
       "            1.0000e+00,  6.7555e-01],\n",
       "          [-3.2248e-01, -9.2166e-01,  6.9720e-01,  ...,  6.5626e-01,\n",
       "            7.1252e-02,  4.7911e-01],\n",
       "          [ 1.0000e+00,  4.3092e-01,  6.2202e-01,  ..., -8.5142e-01,\n",
       "            1.0000e+00,  5.1275e-01]],\n",
       "\n",
       "         [[ 6.1122e-01, -2.7216e-01,  8.3991e-01,  ..., -6.0976e-01,\n",
       "            1.0000e+00, -3.3635e-02],\n",
       "          [-7.8636e-01, -3.9054e-01,  3.5486e-02,  ..., -7.9373e-01,\n",
       "            1.0000e+00, -8.2888e-01],\n",
       "          [ 1.0000e+00,  4.9717e-01,  5.2093e-01,  ...,  2.2432e-01,\n",
       "            3.7029e-01,  1.0000e+00],\n",
       "          ...,\n",
       "          [ 2.3014e-01,  8.8948e-02, -7.4162e-01,  ...,  1.0000e+00,\n",
       "            1.0000e+00,  1.0000e+00],\n",
       "          [-5.1745e-02,  1.4159e-01, -8.5646e-01,  ..., -7.9755e-01,\n",
       "            7.4762e-01, -2.8587e-01],\n",
       "          [ 1.0000e+00,  1.0000e+00,  8.0436e-01,  ..., -9.9917e-01,\n",
       "           -9.3815e-01, -4.5772e-01]],\n",
       "\n",
       "         [[-3.6801e-01,  1.0000e+00, -5.0522e-01,  ...,  3.5545e-01,\n",
       "           -3.6666e-01,  1.0000e+00],\n",
       "          [ 1.0000e+00,  7.4987e-01,  1.0000e+00,  ...,  1.0000e+00,\n",
       "            5.8753e-01,  2.4256e-01],\n",
       "          [-1.1699e-02, -5.8368e-01,  2.1061e-01,  ..., -3.5167e-01,\n",
       "            1.0000e+00, -2.4731e-01],\n",
       "          ...,\n",
       "          [ 1.0000e+00,  7.0693e-01,  1.0000e+00,  ...,  2.0113e-01,\n",
       "           -3.5389e-01, -3.1508e-01],\n",
       "          [ 4.2419e-01,  4.1876e-01,  1.2817e-01,  ...,  9.9788e-01,\n",
       "           -4.0309e-01,  1.0000e+00],\n",
       "          [ 2.3681e-01, -9.2572e-01,  8.8293e-01,  ...,  1.0000e+00,\n",
       "            1.0000e+00,  1.0000e+00]]],\n",
       "\n",
       "\n",
       "        [[[ 9.4435e-01, -2.6443e-01,  1.8425e-01,  ...,  9.7713e-01,\n",
       "            1.0000e+00,  1.0000e+00],\n",
       "          [-2.3695e-01, -7.6800e-02,  9.4595e-02,  ..., -1.7314e-01,\n",
       "           -3.8435e-01,  1.0000e+00],\n",
       "          [-3.8134e-01, -2.2450e-01,  1.0000e+00,  ...,  1.0000e+00,\n",
       "            2.6248e-01,  3.4367e-01],\n",
       "          ...,\n",
       "          [ 9.0481e-01,  5.5038e-01, -4.3363e-02,  ..., -6.2820e-01,\n",
       "            1.0000e+00,  6.0512e-01],\n",
       "          [ 9.0139e-01,  1.0000e+00, -6.1683e-01,  ...,  1.7645e-01,\n",
       "           -1.1141e-01,  2.4288e-01],\n",
       "          [ 1.0000e+00, -5.4506e-02,  1.0000e+00,  ...,  1.0000e+00,\n",
       "            8.4992e-01, -2.5646e-01]],\n",
       "\n",
       "         [[ 1.0000e+00,  2.7343e-01, -9.2304e-01,  ...,  2.8417e-01,\n",
       "            1.0000e+00,  8.4418e-01],\n",
       "          [ 1.0000e+00,  4.9481e-01,  2.9354e-02,  ...,  5.8914e-01,\n",
       "           -4.8812e-01,  1.0000e+00],\n",
       "          [-4.9913e-01,  1.0000e+00, -2.0473e-01,  ...,  1.0000e+00,\n",
       "            1.0000e+00,  4.7626e-01],\n",
       "          ...,\n",
       "          [ 1.0000e+00,  9.6757e-01,  3.1574e-02,  ...,  7.1427e-01,\n",
       "            6.7885e-01,  5.7708e-01],\n",
       "          [ 5.7930e-01,  1.0000e+00,  2.7127e-01,  ..., -3.1729e-01,\n",
       "            7.8600e-01,  1.6323e-01],\n",
       "          [ 1.0000e+00,  1.0000e+00,  1.0715e-01,  ...,  6.8517e-01,\n",
       "            1.0000e+00,  3.9046e-01]],\n",
       "\n",
       "         [[ 2.8808e-01,  1.0000e+00, -7.0750e-02,  ..., -6.2370e-01,\n",
       "           -6.7182e-01,  1.0000e+00],\n",
       "          [-6.4989e-01,  5.8577e-01,  1.0000e+00,  ...,  5.0802e-01,\n",
       "            5.0881e-01, -4.7569e-01],\n",
       "          [ 1.0000e+00,  1.2708e-01, -8.3267e-01,  ...,  1.0000e+00,\n",
       "           -8.1768e-01,  1.0000e+00],\n",
       "          ...,\n",
       "          [-4.2929e-01, -3.5393e-01,  6.0077e-01,  ..., -2.5559e-01,\n",
       "            1.0000e+00,  3.6759e-01],\n",
       "          [-9.9024e-01, -8.8507e-01, -7.8635e-01,  ...,  4.1645e-02,\n",
       "           -7.5027e-01,  9.2512e-01],\n",
       "          [-3.7720e-01, -4.3879e-01,  1.0000e+00,  ...,  6.1952e-01,\n",
       "            7.0603e-01,  1.0000e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 8.1548e-01, -7.7063e-01, -6.4956e-01,  ...,  4.9130e-01,\n",
       "            9.9549e-01,  6.6086e-01],\n",
       "          [ 3.6543e-01,  1.0000e+00,  4.4286e-01,  ..., -6.9467e-01,\n",
       "            6.4996e-01, -3.5737e-01],\n",
       "          [ 6.8554e-01,  6.4713e-01, -7.6915e-01,  ...,  1.0000e+00,\n",
       "           -5.9759e-01,  1.0000e+00],\n",
       "          ...,\n",
       "          [-9.9741e-01,  4.3815e-01,  1.0000e+00,  ..., -9.5345e-01,\n",
       "            1.9859e-01,  1.0000e+00],\n",
       "          [-6.7044e-01, -3.4571e-01,  1.0000e+00,  ..., -3.6238e-01,\n",
       "           -7.4435e-01, -4.7482e-01],\n",
       "          [ 9.3886e-01, -8.1384e-01, -5.7642e-01,  ...,  1.0000e+00,\n",
       "           -6.4399e-01, -7.5468e-01]],\n",
       "\n",
       "         [[ 1.0000e+00,  1.0000e+00,  9.4497e-01,  ..., -9.4396e-01,\n",
       "            1.0000e+00,  1.0000e+00],\n",
       "          [-7.0524e-01, -9.2085e-01,  1.0000e+00,  ...,  4.6758e-01,\n",
       "           -3.0725e-01,  1.0000e+00],\n",
       "          [ 6.7598e-01,  6.7457e-01,  1.0000e+00,  ...,  1.0000e+00,\n",
       "            1.0000e+00, -6.2695e-01],\n",
       "          ...,\n",
       "          [-6.7667e-01,  1.0000e+00,  1.0000e+00,  ..., -7.1544e-01,\n",
       "            3.5133e-01,  1.0000e+00],\n",
       "          [-9.5509e-01,  1.0000e+00,  1.0000e+00,  ...,  1.0000e+00,\n",
       "           -2.5423e-01,  5.9298e-01],\n",
       "          [ 1.0000e+00,  1.0000e+00,  1.0000e+00,  ..., -1.0554e-01,\n",
       "            1.0000e+00,  1.0000e+00]],\n",
       "\n",
       "         [[-2.2204e-01,  1.8175e-01, -1.3040e-01,  ...,  1.0000e+00,\n",
       "            5.3136e-01,  1.0840e-02],\n",
       "          [ 1.0000e+00,  1.0000e+00,  6.9812e-01,  ...,  7.8359e-01,\n",
       "            1.0000e+00, -1.2611e-01],\n",
       "          [ 5.0204e-02, -3.7097e-02,  1.0000e+00,  ...,  4.3120e-01,\n",
       "           -3.6570e-01,  1.0000e+00],\n",
       "          ...,\n",
       "          [ 1.0000e+00, -6.1263e-01, -2.4870e-01,  ...,  1.0000e+00,\n",
       "            1.0000e+00,  1.3740e-01],\n",
       "          [ 9.7529e-01, -6.5444e-01,  9.8019e-02,  ..., -1.1989e-01,\n",
       "            9.2248e-01,  1.6065e-01],\n",
       "          [ 5.1259e-01,  6.6083e-02,  7.2364e-01,  ...,  5.2525e-01,\n",
       "           -1.8324e-02,  1.1272e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 2.3869e-01, -1.1226e-01,  1.9853e-01,  ...,  7.3485e-01,\n",
       "            6.1063e-01, -7.1116e-02],\n",
       "          [ 1.0000e+00,  1.0000e+00, -5.5135e-02,  ...,  1.0000e+00,\n",
       "            6.3097e-01,  1.0000e+00],\n",
       "          [ 7.3570e-01,  1.0000e+00,  1.0000e+00,  ...,  3.2600e-01,\n",
       "            1.0000e+00,  1.0000e+00],\n",
       "          ...,\n",
       "          [ 3.9099e-01,  1.0000e+00,  2.9578e-01,  ...,  4.1861e-01,\n",
       "           -2.0283e-02, -4.5608e-02],\n",
       "          [ 1.0000e+00,  2.8712e-01,  1.0000e+00,  ...,  1.0000e+00,\n",
       "            1.0000e+00,  5.0653e-02],\n",
       "          [-5.3780e-01, -3.0612e-01, -1.7388e-01,  ...,  2.3167e-01,\n",
       "            7.2582e-01,  1.0000e+00]],\n",
       "\n",
       "         [[ 5.0310e-01,  8.4661e-01, -8.8363e-02,  ...,  1.0000e+00,\n",
       "           -4.8015e-01, -9.8769e-01],\n",
       "          [ 1.0000e+00,  1.0000e+00, -9.2705e-01,  ...,  1.0000e+00,\n",
       "            1.0000e+00, -8.5445e-01],\n",
       "          [-4.6570e-02,  9.5512e-01, -1.5108e-01,  ...,  9.2394e-01,\n",
       "            5.6063e-01,  1.0000e+00],\n",
       "          ...,\n",
       "          [ 1.0000e+00, -1.1842e-01, -7.4383e-01,  ...,  1.0000e+00,\n",
       "           -3.2279e-01,  1.0000e+00],\n",
       "          [ 7.1791e-01,  4.9624e-01,  1.0000e+00,  ...,  1.0000e+00,\n",
       "            4.1366e-01,  1.0000e+00],\n",
       "          [-5.6924e-01, -8.6464e-01, -6.9639e-01,  ..., -4.6827e-01,\n",
       "            2.0288e-01,  9.8156e-01]],\n",
       "\n",
       "         [[-8.5133e-01,  7.4939e-01, -3.1146e-01,  ...,  7.0600e-01,\n",
       "            1.0000e+00,  1.0000e+00],\n",
       "          [ 4.2395e-01, -9.2583e-01,  1.2772e-01,  ...,  1.0000e+00,\n",
       "            1.0000e+00,  9.3004e-01],\n",
       "          [ 1.0000e+00,  5.7648e-01, -5.8171e-01,  ...,  1.0000e+00,\n",
       "            1.0000e+00,  1.0000e+00],\n",
       "          ...,\n",
       "          [ 1.2814e-01,  1.0000e+00,  1.0000e+00,  ...,  1.0000e+00,\n",
       "            7.0626e-01,  3.1822e-03],\n",
       "          [ 6.9499e-01,  1.0000e+00,  7.3430e-02,  ...,  1.0000e+00,\n",
       "            1.0000e+00, -1.3880e-01],\n",
       "          [ 7.6579e-01, -2.5619e-01,  1.0000e+00,  ...,  4.7731e-01,\n",
       "            1.0000e+00, -1.8146e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 8.3372e-01,  1.0000e+00,  1.0000e+00,  ...,  1.0000e+00,\n",
       "           -6.7931e-01, -2.3617e-01],\n",
       "          [ 1.0000e+00,  4.7427e-01,  1.0000e+00,  ..., -5.2414e-01,\n",
       "           -6.9892e-01,  1.0000e+00],\n",
       "          [ 5.1369e-01,  9.1106e-01,  5.5993e-01,  ..., -9.4112e-02,\n",
       "            1.0000e+00,  7.5008e-01],\n",
       "          ...,\n",
       "          [ 8.6712e-02,  6.9135e-01,  7.9520e-01,  ..., -7.8488e-01,\n",
       "           -5.7089e-01, -2.2514e-01],\n",
       "          [-2.7154e-01, -5.0008e-01,  3.9533e-01,  ..., -8.7727e-01,\n",
       "           -1.8248e-01, -9.5147e-02],\n",
       "          [ 3.9474e-01, -5.3059e-01,  1.0000e+00,  ...,  1.0000e+00,\n",
       "           -2.2121e-01, -2.9878e-01]],\n",
       "\n",
       "         [[ 6.5661e-01,  2.1627e-01,  1.5657e-01,  ...,  5.9847e-01,\n",
       "            1.0000e+00,  1.9850e-01],\n",
       "          [ 5.8322e-01,  1.0000e+00,  1.9293e-01,  ..., -5.1944e-01,\n",
       "           -2.1975e-01, -5.8820e-02],\n",
       "          [ 9.0656e-01,  9.3746e-01,  1.8867e-01,  ...,  2.6280e-01,\n",
       "           -9.8560e-01,  1.0000e+00],\n",
       "          ...,\n",
       "          [ 6.2927e-02, -9.6903e-01,  5.6267e-01,  ...,  1.0000e+00,\n",
       "           -6.0809e-01,  6.9169e-01],\n",
       "          [ 1.0000e+00, -7.9783e-01, -8.1949e-01,  ...,  9.0189e-01,\n",
       "            1.0000e+00, -9.3197e-01],\n",
       "          [ 1.0000e+00,  4.5031e-01,  3.7893e-01,  ...,  1.7260e-01,\n",
       "            7.7469e-01,  8.6029e-01]],\n",
       "\n",
       "         [[ 8.9122e-01,  9.2850e-01,  8.6977e-01,  ...,  9.9939e-01,\n",
       "           -5.9249e-01,  3.3107e-01],\n",
       "          [ 1.0000e+00, -5.0396e-01,  2.9027e-01,  ...,  8.1988e-01,\n",
       "            5.9519e-01,  1.0000e+00],\n",
       "          [ 9.2420e-01, -4.6603e-01,  3.5072e-01,  ..., -4.3578e-03,\n",
       "            1.0000e+00,  3.9336e-01],\n",
       "          ...,\n",
       "          [ 1.0000e+00,  1.0000e+00,  2.7284e-02,  ..., -3.1934e-01,\n",
       "            9.3888e-01, -1.7546e-01],\n",
       "          [-1.8593e-01,  1.0000e+00,  1.0000e+00,  ..., -4.0731e-01,\n",
       "           -1.7046e-01,  1.0000e+00],\n",
       "          [-2.7841e-01,  1.0000e+00, -2.4501e-01,  ..., -2.1450e-01,\n",
       "            4.1841e-01,  3.6705e-04]]]], grad_fn=<HardtanhBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_channel = 80\n",
    "conv_bn = conv_channel_adapter3(input_channel,kernel = 1, padding= 0,groups= input_channel//10)\n",
    "\n",
    "x = torch.rand((3,80,32,32))\n",
    "conv_bn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1cf991e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No lr scheduler\n",
      "Optimizer Set to New: {'state': {}, 'param_groups': [{'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': False, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69]}]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjohnny_suu\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\John Su\\Downloads\\SydneyUni\\2022\\thesis\\Thesis\\wandb\\run-20230502_132221-3ntyn800</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/johnny_suu/Cifar80-20_Serial_Final/runs/3ntyn800\" target=\"_blank\">kind-snow-1</a></strong> to <a href=\"https://wandb.ai/johnny_suu/Cifar80-20_Serial_Final\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "Project Name: Cifar80-20_Serial_Final, Run Name 3_adapter_groups_1 \n",
      "\n",
      "\n",
      "Run Start : 2023-05-02 13-22-18\n",
      "start_epoch : 0\n",
      "initial_lr : 0.0001\n",
      "batch_size : 64\n",
      "epochs : 75\n",
      "epoch_chkpts : []\n",
      "device : cuda:0\n",
      "criterion : CrossEntropyLoss()\n",
      "optimizer : Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: False\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "seed : 123\n",
      "model_architecture : <class 'NN_Thesis.nn_classes.BNN_Resnet_UniAdapt'>\n",
      "binerised_training : True\n",
      "Number of Elements : 4517340\n",
      "Initial accuracy:\n",
      "Test Accuracy : 5.0%, Test Loss: 60.99716193812668\n",
      "best_acc.pth saved!\n",
      "Test Accuracy : 5.0%, Test Loss: 59.2879421710968\n",
      "epoch: 1 average loss: 2.552\n",
      "Test Accuracy : 32.6%, Test Loss: 2.2354499585926533\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.64 seconds\n",
      "epoch: 2 average loss: 2.224\n",
      "Test Accuracy : 34.2%, Test Loss: 2.1164265535771847\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.58 seconds\n",
      "epoch: 3 average loss: 2.092\n",
      "Test Accuracy : 39.7%, Test Loss: 1.9880206137895584\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.49 seconds\n",
      "epoch: 4 average loss: 2.004\n",
      "Test Accuracy : 43.0%, Test Loss: 1.9201951399445534\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.46 seconds\n",
      "epoch: 5 average loss: 1.926\n",
      "Test Accuracy : 44.9%, Test Loss: 1.8448830656707287\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.48 seconds\n",
      "epoch: 6 average loss: 1.876\n",
      "Test Accuracy : 45.4%, Test Loss: 1.796482875943184\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.50 seconds\n",
      "epoch: 7 average loss: 1.830\n",
      "Test Accuracy : 46.4%, Test Loss: 1.7785092629492283\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.48 seconds\n",
      "epoch: 8 average loss: 1.789\n",
      "Test Accuracy : 47.8%, Test Loss: 1.7327037490904331\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.46 seconds\n",
      "epoch: 9 average loss: 1.765\n",
      "Test Accuracy : 47.6%, Test Loss: 1.711891196668148\n",
      "Epoch Time (Training + Test) = 9.43 seconds\n",
      "epoch: 10 average loss: 1.721\n",
      "Test Accuracy : 48.3%, Test Loss: 1.6872325353324413\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.63 seconds\n",
      "epoch: 11 average loss: 1.686\n",
      "Test Accuracy : 48.9%, Test Loss: 1.6390526667237282\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.76 seconds\n",
      "epoch: 12 average loss: 1.666\n",
      "Test Accuracy : 49.1%, Test Loss: 1.666179746389389\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.65 seconds\n",
      "epoch: 13 average loss: 1.659\n",
      "Test Accuracy : 50.5%, Test Loss: 1.5982102751731873\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.46 seconds\n",
      "epoch: 14 average loss: 1.635\n",
      "Test Accuracy : 51.4%, Test Loss: 1.609090369194746\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.51 seconds\n",
      "epoch: 15 average loss: 1.634\n",
      "Test Accuracy : 52.1%, Test Loss: 1.5657270774245262\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.38 seconds\n",
      "epoch: 16 average loss: 1.594\n",
      "Test Accuracy : 52.8%, Test Loss: 1.5458465702831745\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.53 seconds\n",
      "epoch: 17 average loss: 1.579\n",
      "Test Accuracy : 53.6%, Test Loss: 1.5273246765136719\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.37 seconds\n",
      "epoch: 18 average loss: 1.574\n",
      "Test Accuracy : 52.5%, Test Loss: 1.529029194265604\n",
      "Epoch Time (Training + Test) = 9.56 seconds\n",
      "epoch: 19 average loss: 1.554\n",
      "Test Accuracy : 53.5%, Test Loss: 1.4936783090233803\n",
      "Epoch Time (Training + Test) = 9.37 seconds\n",
      "epoch: 20 average loss: 1.541\n",
      "Test Accuracy : 54.2%, Test Loss: 1.5069448351860046\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.47 seconds\n",
      "epoch: 21 average loss: 1.537\n",
      "Test Accuracy : 53.5%, Test Loss: 1.4741958603262901\n",
      "Epoch Time (Training + Test) = 9.44 seconds\n",
      "epoch: 22 average loss: 1.520\n",
      "Test Accuracy : 52.3%, Test Loss: 1.5042679123580456\n",
      "Epoch Time (Training + Test) = 9.42 seconds\n",
      "epoch: 23 average loss: 1.504\n",
      "Test Accuracy : 54.4%, Test Loss: 1.474237248301506\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.44 seconds\n",
      "epoch: 24 average loss: 1.504\n",
      "Test Accuracy : 55.0%, Test Loss: 1.4837159737944603\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.52 seconds\n",
      "epoch: 25 average loss: 1.488\n",
      "Test Accuracy : 54.7%, Test Loss: 1.4468306750059128\n",
      "Epoch Time (Training + Test) = 9.54 seconds\n",
      "epoch: 26 average loss: 1.496\n",
      "Test Accuracy : 54.0%, Test Loss: 1.4743707776069641\n",
      "Epoch Time (Training + Test) = 9.35 seconds\n",
      "epoch: 27 average loss: 1.464\n",
      "Test Accuracy : 54.1%, Test Loss: 1.441464252769947\n",
      "Epoch Time (Training + Test) = 9.48 seconds\n",
      "epoch: 28 average loss: 1.466\n",
      "Test Accuracy : 54.5%, Test Loss: 1.4391449429094791\n",
      "Epoch Time (Training + Test) = 9.45 seconds\n",
      "epoch: 29 average loss: 1.449\n",
      "Test Accuracy : 54.9%, Test Loss: 1.4347925037145615\n",
      "Epoch Time (Training + Test) = 9.50 seconds\n",
      "epoch: 30 average loss: 1.440\n",
      "Test Accuracy : 55.5%, Test Loss: 1.4184863455593586\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.47 seconds\n",
      "epoch: 31 average loss: 1.438\n",
      "Test Accuracy : 54.9%, Test Loss: 1.4271547235548496\n",
      "Epoch Time (Training + Test) = 9.43 seconds\n",
      "epoch: 32 average loss: 1.441\n",
      "Test Accuracy : 54.9%, Test Loss: 1.4227772019803524\n",
      "Epoch Time (Training + Test) = 9.55 seconds\n",
      "epoch: 33 average loss: 1.418\n",
      "Test Accuracy : 56.8%, Test Loss: 1.387404553592205\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.45 seconds\n",
      "epoch: 34 average loss: 1.436\n",
      "Test Accuracy : 55.4%, Test Loss: 1.4071455337107182\n",
      "Epoch Time (Training + Test) = 9.37 seconds\n",
      "epoch: 35 average loss: 1.406\n",
      "Test Accuracy : 56.1%, Test Loss: 1.406581237912178\n",
      "Epoch Time (Training + Test) = 9.48 seconds\n",
      "epoch: 36 average loss: 1.401\n",
      "Test Accuracy : 57.6%, Test Loss: 1.3656468577682972\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.53 seconds\n",
      "epoch: 37 average loss: 1.391\n",
      "Test Accuracy : 57.0%, Test Loss: 1.3754049576818943\n",
      "Epoch Time (Training + Test) = 9.49 seconds\n",
      "epoch: 38 average loss: 1.406\n",
      "Test Accuracy : 56.7%, Test Loss: 1.362168312072754\n",
      "Epoch Time (Training + Test) = 9.50 seconds\n",
      "epoch: 39 average loss: 1.389\n",
      "Test Accuracy : 56.8%, Test Loss: 1.360006183385849\n",
      "Epoch Time (Training + Test) = 9.46 seconds\n",
      "epoch: 40 average loss: 1.384\n",
      "Test Accuracy : 56.9%, Test Loss: 1.386883832514286\n",
      "Epoch Time (Training + Test) = 9.47 seconds\n",
      "epoch: 41 average loss: 1.368\n",
      "Test Accuracy : 57.0%, Test Loss: 1.368961077183485\n",
      "Epoch Time (Training + Test) = 9.46 seconds\n",
      "epoch: 42 average loss: 1.369\n",
      "Test Accuracy : 56.5%, Test Loss: 1.3629421070218086\n",
      "Epoch Time (Training + Test) = 9.46 seconds\n",
      "epoch: 43 average loss: 1.350\n",
      "Test Accuracy : 58.5%, Test Loss: 1.3527109138667583\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.51 seconds\n",
      "epoch: 44 average loss: 1.352\n",
      "Test Accuracy : 58.3%, Test Loss: 1.3316896725445986\n",
      "Epoch Time (Training + Test) = 9.50 seconds\n",
      "epoch: 45 average loss: 1.365\n",
      "Test Accuracy : 57.6%, Test Loss: 1.357698678970337\n",
      "Epoch Time (Training + Test) = 9.48 seconds\n",
      "epoch: 46 average loss: 1.351\n",
      "Test Accuracy : 57.1%, Test Loss: 1.35104313865304\n",
      "Epoch Time (Training + Test) = 9.37 seconds\n",
      "epoch: 47 average loss: 1.347\n",
      "Test Accuracy : 56.4%, Test Loss: 1.3724582716822624\n",
      "Epoch Time (Training + Test) = 9.53 seconds\n",
      "epoch: 48 average loss: 1.351\n",
      "Test Accuracy : 58.1%, Test Loss: 1.3329889550805092\n",
      "Epoch Time (Training + Test) = 9.43 seconds\n",
      "epoch: 49 average loss: 1.334\n",
      "Test Accuracy : 57.9%, Test Loss: 1.3418431356549263\n",
      "Epoch Time (Training + Test) = 9.41 seconds\n",
      "epoch: 50 average loss: 1.347\n",
      "Test Accuracy : 56.9%, Test Loss: 1.3541189059615135\n",
      "Epoch Time (Training + Test) = 9.43 seconds\n",
      "epoch: 51 average loss: 1.343\n",
      "Test Accuracy : 57.1%, Test Loss: 1.3477724399417639\n",
      "Epoch Time (Training + Test) = 9.47 seconds\n",
      "epoch: 52 average loss: 1.330\n",
      "Test Accuracy : 57.8%, Test Loss: 1.3109805807471275\n",
      "Epoch Time (Training + Test) = 9.46 seconds\n",
      "epoch: 53 average loss: 1.326\n",
      "Test Accuracy : 58.9%, Test Loss: 1.3448787964880466\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.47 seconds\n",
      "epoch: 54 average loss: 1.337\n",
      "Test Accuracy : 57.8%, Test Loss: 1.319263532757759\n",
      "Epoch Time (Training + Test) = 9.48 seconds\n",
      "epoch: 55 average loss: 1.311\n",
      "Test Accuracy : 58.9%, Test Loss: 1.317700281739235\n",
      "Epoch Time (Training + Test) = 9.44 seconds\n",
      "epoch: 56 average loss: 1.315\n",
      "Test Accuracy : 60.8%, Test Loss: 1.312989491969347\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.48 seconds\n",
      "epoch: 57 average loss: 1.312\n",
      "Test Accuracy : 59.5%, Test Loss: 1.311878215521574\n",
      "Epoch Time (Training + Test) = 9.44 seconds\n",
      "epoch: 58 average loss: 1.317\n",
      "Test Accuracy : 59.1%, Test Loss: 1.3067543916404247\n",
      "Epoch Time (Training + Test) = 9.44 seconds\n",
      "epoch: 59 average loss: 1.308\n",
      "Test Accuracy : 57.9%, Test Loss: 1.3208685591816902\n",
      "Epoch Time (Training + Test) = 9.52 seconds\n",
      "epoch: 60 average loss: 1.290\n",
      "Test Accuracy : 60.6%, Test Loss: 1.293273538351059\n",
      "Epoch Time (Training + Test) = 9.42 seconds\n",
      "epoch: 61 average loss: 1.312\n",
      "Test Accuracy : 58.2%, Test Loss: 1.2971274740993977\n",
      "Epoch Time (Training + Test) = 9.53 seconds\n",
      "epoch: 62 average loss: 1.297\n",
      "Test Accuracy : 59.2%, Test Loss: 1.2935976274311543\n",
      "Epoch Time (Training + Test) = 9.36 seconds\n",
      "epoch: 63 average loss: 1.297\n",
      "Test Accuracy : 58.6%, Test Loss: 1.3148540779948235\n",
      "Epoch Time (Training + Test) = 9.42 seconds\n",
      "epoch: 64 average loss: 1.285\n",
      "Test Accuracy : 58.1%, Test Loss: 1.3123009130358696\n",
      "Epoch Time (Training + Test) = 9.61 seconds\n",
      "epoch: 65 average loss: 1.285\n",
      "Test Accuracy : 59.2%, Test Loss: 1.3199913315474987\n",
      "Epoch Time (Training + Test) = 9.33 seconds\n",
      "epoch: 66 average loss: 1.286\n",
      "Test Accuracy : 58.7%, Test Loss: 1.3044055588543415\n",
      "Epoch Time (Training + Test) = 9.47 seconds\n",
      "epoch: 67 average loss: 1.277\n",
      "Test Accuracy : 59.5%, Test Loss: 1.2721216082572937\n",
      "Epoch Time (Training + Test) = 9.58 seconds\n",
      "epoch: 68 average loss: 1.279\n",
      "Test Accuracy : 60.8%, Test Loss: 1.2755663804709911\n",
      "Epoch Time (Training + Test) = 9.35 seconds\n",
      "epoch: 69 average loss: 1.256\n",
      "Test Accuracy : 60.0%, Test Loss: 1.2961017563939095\n",
      "Epoch Time (Training + Test) = 9.55 seconds\n",
      "epoch: 70 average loss: 1.280\n",
      "Test Accuracy : 60.2%, Test Loss: 1.2562255393713713\n",
      "Epoch Time (Training + Test) = 9.41 seconds\n",
      "epoch: 71 average loss: 1.274\n",
      "Test Accuracy : 58.5%, Test Loss: 1.3098212964832783\n",
      "Epoch Time (Training + Test) = 9.39 seconds\n",
      "epoch: 72 average loss: 1.273\n",
      "Test Accuracy : 59.1%, Test Loss: 1.3088645730167627\n",
      "Epoch Time (Training + Test) = 9.47 seconds\n",
      "epoch: 73 average loss: 1.267\n",
      "Test Accuracy : 58.7%, Test Loss: 1.3019438590854406\n",
      "Epoch Time (Training + Test) = 9.39 seconds\n",
      "epoch: 74 average loss: 1.245\n",
      "Test Accuracy : 60.2%, Test Loss: 1.2931324038654566\n",
      "Epoch Time (Training + Test) = 9.79 seconds\n",
      "epoch: 75 average loss: 1.261\n",
      "Test Accuracy : 59.1%, Test Loss: 1.2894899807870388\n",
      "Epoch Time (Training + Test) = 9.40 seconds\n",
      "Data Saved to 3_adapter_groups_1.csv\n",
      "Finished Training: \n",
      "Total Time 0.394894 hours\n",
      " Average Time Per Epoch 18.95 seconds\n",
      "No lr scheduler\n",
      "Optimizer Set to New: {'state': {}, 'param_groups': [{'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': False, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69]}]}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3ntyn800) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Current Best Acc</td><td>▁▄▅▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇█████████████████████</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>epoch time (s)</td><td>█▃▂▂▂▂▄▂▁▁▁▂▂▂▁▂▂▂▁▂▂▂▂▂▁▂▂▂▂▂▂▂▂▂▁▂▂▁▁▁</td></tr><tr><td>lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_accuracy</td><td>▁▅▅▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇█████████████████████</td></tr><tr><td>test_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>training_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Best Accuracy</td><td>0.608</td></tr><tr><td>Current Best Acc</td><td>0.608</td></tr><tr><td>Total Time (hours)</td><td>0.39489</td></tr><tr><td>epoch</td><td>75</td></tr><tr><td>epoch time (s)</td><td>9.40156</td></tr><tr><td>lr</td><td>0.0001</td></tr><tr><td>test_accuracy</td><td>0.5905</td></tr><tr><td>test_loss</td><td>1.28949</td></tr><tr><td>training_loss</td><td>1.26083</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">kind-snow-1</strong>: <a href=\"https://wandb.ai/johnny_suu/Cifar80-20_Serial_Final/runs/3ntyn800\" target=\"_blank\">https://wandb.ai/johnny_suu/Cifar80-20_Serial_Final/runs/3ntyn800</a><br/>Synced 5 W&B file(s), 1 media file(s), 1 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230502_132221-3ntyn800\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3ntyn800). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b145f22a37742cdbe964f83aff1b6a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\John Su\\Downloads\\SydneyUni\\2022\\thesis\\Thesis\\wandb\\run-20230502_133425-bb7yrs7u</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/johnny_suu/Cifar80-20_Serial_Final/runs/bb7yrs7u\" target=\"_blank\">fresh-jazz-2</a></strong> to <a href=\"https://wandb.ai/johnny_suu/Cifar80-20_Serial_Final\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "Project Name: Cifar80-20_Serial_Final, Run Name 3_adapter_groups_2 \n",
      "\n",
      "\n",
      "Run Start : 2023-05-02 13-34-25\n",
      "start_epoch : 0\n",
      "initial_lr : 0.0001\n",
      "batch_size : 64\n",
      "epochs : 75\n",
      "epoch_chkpts : []\n",
      "device : cuda:0\n",
      "criterion : CrossEntropyLoss()\n",
      "optimizer : Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: False\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "seed : 123\n",
      "model_architecture : <class 'NN_Thesis.nn_classes.BNN_Resnet_UniAdapt'>\n",
      "binerised_training : True\n",
      "Number of Elements : 4450140\n",
      "Initial accuracy:\n",
      "Test Accuracy : 3.8%, Test Loss: 37.6803421700836\n",
      "best_acc.pth saved!\n",
      "Test Accuracy : 3.3%, Test Loss: 37.923436403274536\n",
      "epoch: 1 average loss: 2.606\n",
      "Test Accuracy : 28.6%, Test Loss: 2.3336754366755486\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 12.21 seconds\n",
      "epoch: 2 average loss: 2.333\n",
      "Test Accuracy : 34.9%, Test Loss: 2.170210439711809\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 12.40 seconds\n",
      "epoch: 3 average loss: 2.211\n",
      "Test Accuracy : 37.4%, Test Loss: 2.079652860760689\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 12.31 seconds\n",
      "epoch: 4 average loss: 2.109\n",
      "Test Accuracy : 40.6%, Test Loss: 2.0034575760364532\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 12.37 seconds\n",
      "epoch: 5 average loss: 2.037\n",
      "Test Accuracy : 41.9%, Test Loss: 1.9369406625628471\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 12.29 seconds\n",
      "epoch: 6 average loss: 1.984\n",
      "Test Accuracy : 43.4%, Test Loss: 1.8782968968153\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 12.30 seconds\n",
      "epoch: 7 average loss: 1.941\n",
      "Test Accuracy : 44.3%, Test Loss: 1.8568264916539192\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 12.28 seconds\n",
      "epoch: 8 average loss: 1.897\n",
      "Test Accuracy : 46.2%, Test Loss: 1.7979619838297367\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 12.34 seconds\n",
      "epoch: 9 average loss: 1.859\n",
      "Test Accuracy : 47.3%, Test Loss: 1.7688424363732338\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 12.26 seconds\n",
      "epoch: 10 average loss: 1.823\n",
      "Test Accuracy : 45.7%, Test Loss: 1.7326451279222965\n",
      "Epoch Time (Training + Test) = 12.31 seconds\n",
      "epoch: 11 average loss: 1.790\n",
      "Test Accuracy : 47.3%, Test Loss: 1.7174207605421543\n",
      "Epoch Time (Training + Test) = 12.22 seconds\n",
      "epoch: 12 average loss: 1.776\n",
      "Test Accuracy : 47.6%, Test Loss: 1.7227396443486214\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 12.43 seconds\n",
      "epoch: 13 average loss: 1.746\n",
      "Test Accuracy : 47.5%, Test Loss: 1.6823305003345013\n",
      "Epoch Time (Training + Test) = 12.18 seconds\n",
      "epoch: 14 average loss: 1.739\n",
      "Test Accuracy : 49.0%, Test Loss: 1.6659050062298775\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 12.31 seconds\n",
      "epoch: 15 average loss: 1.730\n",
      "Test Accuracy : 49.1%, Test Loss: 1.6491986960172653\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 12.37 seconds\n",
      "epoch: 16 average loss: 1.692\n",
      "Test Accuracy : 50.4%, Test Loss: 1.62043946236372\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 12.33 seconds\n",
      "epoch: 17 average loss: 1.677\n",
      "Test Accuracy : 50.0%, Test Loss: 1.611466996371746\n",
      "Epoch Time (Training + Test) = 12.37 seconds\n",
      "epoch: 18 average loss: 1.671\n",
      "Test Accuracy : 49.5%, Test Loss: 1.6287217065691948\n",
      "Epoch Time (Training + Test) = 12.17 seconds\n",
      "epoch: 19 average loss: 1.659\n",
      "Test Accuracy : 50.6%, Test Loss: 1.5960623286664486\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 12.28 seconds\n",
      "epoch: 20 average loss: 1.642\n",
      "Test Accuracy : 51.4%, Test Loss: 1.5843406803905964\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 12.25 seconds\n",
      "epoch: 21 average loss: 1.634\n",
      "Test Accuracy : 51.7%, Test Loss: 1.5858686119318008\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 12.27 seconds\n",
      "epoch: 22 average loss: 1.631\n",
      "Test Accuracy : 50.6%, Test Loss: 1.590396810323\n",
      "Epoch Time (Training + Test) = 12.24 seconds\n",
      "epoch: 23 average loss: 1.608\n",
      "Test Accuracy : 51.2%, Test Loss: 1.5613917745649815\n",
      "Epoch Time (Training + Test) = 12.36 seconds\n",
      "epoch: 24 average loss: 1.618\n",
      "Test Accuracy : 50.4%, Test Loss: 1.5566532723605633\n",
      "Epoch Time (Training + Test) = 11.81 seconds\n",
      "epoch: 25 average loss: 1.591\n",
      "Test Accuracy : 51.1%, Test Loss: 1.563817486166954\n",
      "Epoch Time (Training + Test) = 11.74 seconds\n",
      "epoch: 26 average loss: 1.588\n",
      "Test Accuracy : 52.5%, Test Loss: 1.5490369535982609\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 11.83 seconds\n",
      "epoch: 27 average loss: 1.575\n",
      "Test Accuracy : 52.1%, Test Loss: 1.50952585414052\n",
      "Epoch Time (Training + Test) = 11.74 seconds\n",
      "epoch: 28 average loss: 1.579\n",
      "Test Accuracy : 51.9%, Test Loss: 1.5230486653745174\n",
      "Epoch Time (Training + Test) = 11.66 seconds\n",
      "epoch: 29 average loss: 1.573\n",
      "Test Accuracy : 52.6%, Test Loss: 1.5073567815124989\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 11.85 seconds\n",
      "epoch: 30 average loss: 1.567\n",
      "Test Accuracy : 51.7%, Test Loss: 1.5315961390733719\n",
      "Epoch Time (Training + Test) = 11.79 seconds\n",
      "epoch: 31 average loss: 1.537\n",
      "Test Accuracy : 54.1%, Test Loss: 1.4871867150068283\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 11.77 seconds\n",
      "epoch: 32 average loss: 1.553\n",
      "Test Accuracy : 52.1%, Test Loss: 1.5153719894587994\n",
      "Epoch Time (Training + Test) = 11.78 seconds\n",
      "epoch: 33 average loss: 1.540\n",
      "Test Accuracy : 53.8%, Test Loss: 1.4839829578995705\n",
      "Epoch Time (Training + Test) = 11.73 seconds\n",
      "epoch: 34 average loss: 1.539\n",
      "Test Accuracy : 53.2%, Test Loss: 1.4796909764409065\n",
      "Epoch Time (Training + Test) = 11.66 seconds\n",
      "epoch: 35 average loss: 1.516\n",
      "Test Accuracy : 52.5%, Test Loss: 1.5308780409395695\n",
      "Epoch Time (Training + Test) = 11.78 seconds\n",
      "epoch: 36 average loss: 1.511\n",
      "Test Accuracy : 54.2%, Test Loss: 1.492529958486557\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 11.77 seconds\n",
      "epoch: 37 average loss: 1.515\n",
      "Test Accuracy : 53.4%, Test Loss: 1.4836102053523064\n",
      "Epoch Time (Training + Test) = 11.78 seconds\n",
      "epoch: 38 average loss: 1.515\n",
      "Test Accuracy : 52.3%, Test Loss: 1.5057106278836727\n",
      "Epoch Time (Training + Test) = 11.71 seconds\n",
      "epoch: 39 average loss: 1.508\n",
      "Test Accuracy : 53.1%, Test Loss: 1.4893665127456188\n",
      "Epoch Time (Training + Test) = 11.70 seconds\n",
      "epoch: 40 average loss: 1.505\n",
      "Test Accuracy : 53.3%, Test Loss: 1.5077173449099064\n",
      "Epoch Time (Training + Test) = 11.84 seconds\n",
      "epoch: 41 average loss: 1.491\n",
      "Test Accuracy : 51.3%, Test Loss: 1.5276887938380241\n",
      "Epoch Time (Training + Test) = 11.79 seconds\n",
      "epoch: 42 average loss: 1.488\n",
      "Test Accuracy : 53.1%, Test Loss: 1.4899024479091167\n",
      "Epoch Time (Training + Test) = 11.74 seconds\n",
      "epoch: 43 average loss: 1.486\n",
      "Test Accuracy : 54.1%, Test Loss: 1.4577135927975178\n",
      "Epoch Time (Training + Test) = 11.69 seconds\n",
      "epoch: 44 average loss: 1.482\n",
      "Test Accuracy : 52.9%, Test Loss: 1.4567259699106216\n",
      "Epoch Time (Training + Test) = 11.82 seconds\n",
      "epoch: 45 average loss: 1.495\n",
      "Test Accuracy : 54.2%, Test Loss: 1.4523521400988102\n",
      "Epoch Time (Training + Test) = 11.69 seconds\n",
      "epoch: 46 average loss: 1.477\n",
      "Test Accuracy : 54.2%, Test Loss: 1.455961063504219\n",
      "Epoch Time (Training + Test) = 11.82 seconds\n",
      "epoch: 47 average loss: 1.472\n",
      "Test Accuracy : 54.8%, Test Loss: 1.462251104414463\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 11.82 seconds\n",
      "epoch: 48 average loss: 1.485\n",
      "Test Accuracy : 54.4%, Test Loss: 1.4581811726093292\n",
      "Epoch Time (Training + Test) = 11.77 seconds\n",
      "epoch: 49 average loss: 1.462\n",
      "Test Accuracy : 54.6%, Test Loss: 1.4269280955195427\n",
      "Epoch Time (Training + Test) = 11.81 seconds\n",
      "epoch: 50 average loss: 1.470\n",
      "Test Accuracy : 54.9%, Test Loss: 1.4508167281746864\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 11.74 seconds\n",
      "epoch: 51 average loss: 1.456\n",
      "Test Accuracy : 53.5%, Test Loss: 1.4653736986219883\n",
      "Epoch Time (Training + Test) = 11.80 seconds\n",
      "epoch: 52 average loss: 1.467\n",
      "Test Accuracy : 54.4%, Test Loss: 1.4364434257149696\n",
      "Epoch Time (Training + Test) = 11.71 seconds\n",
      "epoch: 53 average loss: 1.451\n",
      "Test Accuracy : 54.6%, Test Loss: 1.4377256222069263\n",
      "Epoch Time (Training + Test) = 11.69 seconds\n",
      "epoch: 54 average loss: 1.456\n",
      "Test Accuracy : 55.9%, Test Loss: 1.4242157153785229\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 11.78 seconds\n",
      "epoch: 55 average loss: 1.441\n",
      "Test Accuracy : 55.5%, Test Loss: 1.4572742767632008\n",
      "Epoch Time (Training + Test) = 11.74 seconds\n",
      "epoch: 56 average loss: 1.456\n",
      "Test Accuracy : 55.7%, Test Loss: 1.4157009981572628\n",
      "Epoch Time (Training + Test) = 11.83 seconds\n",
      "epoch: 57 average loss: 1.444\n",
      "Test Accuracy : 56.0%, Test Loss: 1.4094721861183643\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 11.70 seconds\n",
      "epoch: 58 average loss: 1.438\n",
      "Test Accuracy : 54.4%, Test Loss: 1.4467459805309772\n",
      "Epoch Time (Training + Test) = 11.82 seconds\n",
      "epoch: 59 average loss: 1.436\n",
      "Test Accuracy : 56.1%, Test Loss: 1.3868777845054865\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 11.74 seconds\n",
      "epoch: 60 average loss: 1.430\n",
      "Test Accuracy : 54.9%, Test Loss: 1.452414233237505\n",
      "Epoch Time (Training + Test) = 11.82 seconds\n",
      "epoch: 61 average loss: 1.416\n",
      "Test Accuracy : 55.0%, Test Loss: 1.4574274979531765\n",
      "Epoch Time (Training + Test) = 11.70 seconds\n",
      "epoch: 62 average loss: 1.432\n",
      "Test Accuracy : 55.1%, Test Loss: 1.4211832471191883\n",
      "Epoch Time (Training + Test) = 11.76 seconds\n",
      "epoch: 63 average loss: 1.430\n",
      "Test Accuracy : 55.8%, Test Loss: 1.4013106487691402\n",
      "Epoch Time (Training + Test) = 11.75 seconds\n",
      "epoch: 64 average loss: 1.427\n",
      "Test Accuracy : 55.5%, Test Loss: 1.4155851379036903\n",
      "Epoch Time (Training + Test) = 11.69 seconds\n",
      "epoch: 65 average loss: 1.419\n",
      "Test Accuracy : 54.8%, Test Loss: 1.432475931942463\n",
      "Epoch Time (Training + Test) = 11.77 seconds\n",
      "epoch: 66 average loss: 1.415\n",
      "Test Accuracy : 55.3%, Test Loss: 1.4192519076168537\n",
      "Epoch Time (Training + Test) = 11.74 seconds\n",
      "epoch: 67 average loss: 1.408\n",
      "Test Accuracy : 55.3%, Test Loss: 1.4215721860527992\n",
      "Epoch Time (Training + Test) = 11.82 seconds\n",
      "epoch: 68 average loss: 1.415\n",
      "Test Accuracy : 56.1%, Test Loss: 1.3948318846523762\n",
      "Epoch Time (Training + Test) = 11.70 seconds\n",
      "epoch: 69 average loss: 1.413\n",
      "Test Accuracy : 54.9%, Test Loss: 1.423576895147562\n",
      "Epoch Time (Training + Test) = 11.80 seconds\n",
      "epoch: 70 average loss: 1.411\n",
      "Test Accuracy : 55.5%, Test Loss: 1.4138667061924934\n",
      "Epoch Time (Training + Test) = 11.71 seconds\n",
      "epoch: 71 average loss: 1.411\n",
      "Test Accuracy : 54.5%, Test Loss: 1.4300377517938614\n",
      "Epoch Time (Training + Test) = 11.68 seconds\n",
      "epoch: 72 average loss: 1.394\n",
      "Test Accuracy : 55.5%, Test Loss: 1.3960627876222134\n",
      "Epoch Time (Training + Test) = 11.83 seconds\n",
      "epoch: 73 average loss: 1.400\n",
      "Test Accuracy : 54.9%, Test Loss: 1.4172618724405766\n",
      "Epoch Time (Training + Test) = 11.71 seconds\n",
      "epoch: 74 average loss: 1.381\n",
      "Test Accuracy : 54.5%, Test Loss: 1.4422228336334229\n",
      "Epoch Time (Training + Test) = 11.80 seconds\n",
      "epoch: 75 average loss: 1.386\n",
      "Test Accuracy : 55.5%, Test Loss: 1.397945936769247\n",
      "Epoch Time (Training + Test) = 11.72 seconds\n",
      "Data Saved to 3_adapter_groups_2.csv\n",
      "Finished Training: \n",
      "Total Time 0.496826 hours\n",
      " Average Time Per Epoch 23.85 seconds\n",
      "No lr scheduler\n",
      "Optimizer Set to New: {'state': {}, 'param_groups': [{'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': False, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69]}]}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:bb7yrs7u) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Current Best Acc</td><td>▁▄▆▆▆▇▇▇▇▇▇▇▇▇██████████████████████████</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>epoch time (s)</td><td>▁████████████▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_accuracy</td><td>▁▄▆▆▆▇▇▇▇▇▇▇▇▇█▇▇▇██▇███████████████████</td></tr><tr><td>test_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>training_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Best Accuracy</td><td>0.5615</td></tr><tr><td>Current Best Acc</td><td>0.5615</td></tr><tr><td>Total Time (hours)</td><td>0.49683</td></tr><tr><td>epoch</td><td>75</td></tr><tr><td>epoch time (s)</td><td>11.71586</td></tr><tr><td>lr</td><td>0.0001</td></tr><tr><td>test_accuracy</td><td>0.5555</td></tr><tr><td>test_loss</td><td>1.39795</td></tr><tr><td>training_loss</td><td>1.38632</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">fresh-jazz-2</strong>: <a href=\"https://wandb.ai/johnny_suu/Cifar80-20_Serial_Final/runs/bb7yrs7u\" target=\"_blank\">https://wandb.ai/johnny_suu/Cifar80-20_Serial_Final/runs/bb7yrs7u</a><br/>Synced 5 W&B file(s), 1 media file(s), 1 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230502_133425-bb7yrs7u\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:bb7yrs7u). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c909552702e4775b64276f94b42394c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01691666666883975, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\John Su\\Downloads\\SydneyUni\\2022\\thesis\\Thesis\\wandb\\run-20230502_134939-2k66t0eb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/johnny_suu/Cifar80-20_Serial_Final/runs/2k66t0eb\" target=\"_blank\">autumn-mountain-3</a></strong> to <a href=\"https://wandb.ai/johnny_suu/Cifar80-20_Serial_Final\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "Project Name: Cifar80-20_Serial_Final, Run Name 3_adapter_groups_5 \n",
      "\n",
      "\n",
      "Run Start : 2023-05-02 13-49-39\n",
      "start_epoch : 0\n",
      "initial_lr : 0.0001\n",
      "batch_size : 64\n",
      "epochs : 75\n",
      "epoch_chkpts : []\n",
      "device : cuda:0\n",
      "criterion : CrossEntropyLoss()\n",
      "optimizer : Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: False\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "seed : 123\n",
      "model_architecture : <class 'NN_Thesis.nn_classes.BNN_Resnet_UniAdapt'>\n",
      "binerised_training : True\n",
      "Number of Elements : 4409820\n",
      "Initial accuracy:\n",
      "Test Accuracy : 5.9%, Test Loss: 26.660157865779414\n",
      "best_acc.pth saved!\n",
      "Test Accuracy : 4.9%, Test Loss: 26.855656921863556\n",
      "epoch: 1 average loss: 2.699\n",
      "Test Accuracy : 24.9%, Test Loss: 2.4534633979201317\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.11 seconds\n",
      "epoch: 2 average loss: 2.438\n",
      "Test Accuracy : 29.4%, Test Loss: 2.3036824464797974\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.07 seconds\n",
      "epoch: 3 average loss: 2.323\n",
      "Test Accuracy : 33.2%, Test Loss: 2.1992532685399055\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.20 seconds\n",
      "epoch: 4 average loss: 2.248\n",
      "Test Accuracy : 35.1%, Test Loss: 2.1473847441375256\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.10 seconds\n",
      "epoch: 5 average loss: 2.183\n",
      "Test Accuracy : 37.8%, Test Loss: 2.084193877875805\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.20 seconds\n",
      "epoch: 6 average loss: 2.137\n",
      "Test Accuracy : 37.8%, Test Loss: 2.028645183891058\n",
      "Epoch Time (Training + Test) = 9.06 seconds\n",
      "epoch: 7 average loss: 2.094\n",
      "Test Accuracy : 40.5%, Test Loss: 2.0023886933922768\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.12 seconds\n",
      "epoch: 8 average loss: 2.061\n",
      "Test Accuracy : 40.7%, Test Loss: 1.963623970746994\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.11 seconds\n",
      "epoch: 9 average loss: 2.032\n",
      "Test Accuracy : 41.6%, Test Loss: 1.9232141561806202\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.20 seconds\n",
      "epoch: 10 average loss: 1.997\n",
      "Test Accuracy : 41.2%, Test Loss: 1.8813038915395737\n",
      "Epoch Time (Training + Test) = 9.19 seconds\n",
      "epoch: 11 average loss: 1.962\n",
      "Test Accuracy : 42.5%, Test Loss: 1.8888946026563644\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.10 seconds\n",
      "epoch: 12 average loss: 1.951\n",
      "Test Accuracy : 42.2%, Test Loss: 1.847501277923584\n",
      "Epoch Time (Training + Test) = 9.07 seconds\n",
      "epoch: 13 average loss: 1.921\n",
      "Test Accuracy : 42.8%, Test Loss: 1.8472552821040154\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.22 seconds\n",
      "epoch: 14 average loss: 1.917\n",
      "Test Accuracy : 44.9%, Test Loss: 1.811358842998743\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.10 seconds\n",
      "epoch: 15 average loss: 1.907\n",
      "Test Accuracy : 44.9%, Test Loss: 1.8153947368264198\n",
      "Epoch Time (Training + Test) = 9.06 seconds\n",
      "epoch: 16 average loss: 1.876\n",
      "Test Accuracy : 43.4%, Test Loss: 1.8174840584397316\n",
      "Epoch Time (Training + Test) = 9.09 seconds\n",
      "epoch: 17 average loss: 1.865\n",
      "Test Accuracy : 43.4%, Test Loss: 1.7907954156398773\n",
      "Epoch Time (Training + Test) = 9.07 seconds\n",
      "epoch: 18 average loss: 1.860\n",
      "Test Accuracy : 45.0%, Test Loss: 1.7811521515250206\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.17 seconds\n",
      "epoch: 19 average loss: 1.855\n",
      "Test Accuracy : 45.2%, Test Loss: 1.7622846029698849\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.10 seconds\n",
      "epoch: 20 average loss: 1.824\n",
      "Test Accuracy : 46.2%, Test Loss: 1.7411680184304714\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.09 seconds\n",
      "epoch: 21 average loss: 1.828\n",
      "Test Accuracy : 45.9%, Test Loss: 1.742371667176485\n",
      "Epoch Time (Training + Test) = 9.04 seconds\n",
      "epoch: 22 average loss: 1.827\n",
      "Test Accuracy : 47.4%, Test Loss: 1.7273206822574139\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.10 seconds\n",
      "epoch: 23 average loss: 1.808\n",
      "Test Accuracy : 47.1%, Test Loss: 1.7108851931989193\n",
      "Epoch Time (Training + Test) = 9.16 seconds\n",
      "epoch: 24 average loss: 1.809\n",
      "Test Accuracy : 47.4%, Test Loss: 1.7010113894939423\n",
      "Epoch Time (Training + Test) = 9.05 seconds\n",
      "epoch: 25 average loss: 1.788\n",
      "Test Accuracy : 48.0%, Test Loss: 1.7194835469126701\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.24 seconds\n",
      "epoch: 26 average loss: 1.789\n",
      "Test Accuracy : 47.2%, Test Loss: 1.694539599120617\n",
      "Epoch Time (Training + Test) = 9.06 seconds\n",
      "epoch: 27 average loss: 1.768\n",
      "Test Accuracy : 49.1%, Test Loss: 1.6739855743944645\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.14 seconds\n",
      "epoch: 28 average loss: 1.772\n",
      "Test Accuracy : 46.6%, Test Loss: 1.7096004635095596\n",
      "Epoch Time (Training + Test) = 9.18 seconds\n",
      "epoch: 29 average loss: 1.776\n",
      "Test Accuracy : 47.2%, Test Loss: 1.681538425385952\n",
      "Epoch Time (Training + Test) = 9.01 seconds\n",
      "epoch: 30 average loss: 1.756\n",
      "Test Accuracy : 46.9%, Test Loss: 1.6726104691624641\n",
      "Epoch Time (Training + Test) = 9.09 seconds\n",
      "epoch: 31 average loss: 1.752\n",
      "Test Accuracy : 48.4%, Test Loss: 1.6699675396084785\n",
      "Epoch Time (Training + Test) = 9.07 seconds\n",
      "epoch: 32 average loss: 1.763\n",
      "Test Accuracy : 48.2%, Test Loss: 1.6569054462015629\n",
      "Epoch Time (Training + Test) = 9.03 seconds\n",
      "epoch: 33 average loss: 1.735\n",
      "Test Accuracy : 48.4%, Test Loss: 1.6599478796124458\n",
      "Epoch Time (Training + Test) = 9.06 seconds\n",
      "epoch: 34 average loss: 1.742\n",
      "Test Accuracy : 47.5%, Test Loss: 1.6817906945943832\n",
      "Epoch Time (Training + Test) = 9.07 seconds\n",
      "epoch: 35 average loss: 1.725\n",
      "Test Accuracy : 49.0%, Test Loss: 1.676621112972498\n",
      "Epoch Time (Training + Test) = 9.10 seconds\n",
      "epoch: 36 average loss: 1.718\n",
      "Test Accuracy : 47.5%, Test Loss: 1.6667544804513454\n",
      "Epoch Time (Training + Test) = 9.08 seconds\n",
      "epoch: 37 average loss: 1.720\n",
      "Test Accuracy : 47.3%, Test Loss: 1.6809282712638378\n",
      "Epoch Time (Training + Test) = 9.09 seconds\n",
      "epoch: 38 average loss: 1.727\n",
      "Test Accuracy : 48.0%, Test Loss: 1.6640006564557552\n",
      "Epoch Time (Training + Test) = 9.05 seconds\n",
      "epoch: 39 average loss: 1.714\n",
      "Test Accuracy : 48.3%, Test Loss: 1.6549805738031864\n",
      "Epoch Time (Training + Test) = 9.14 seconds\n",
      "epoch: 40 average loss: 1.716\n",
      "Test Accuracy : 48.0%, Test Loss: 1.6386264227330685\n",
      "Epoch Time (Training + Test) = 9.06 seconds\n",
      "epoch: 41 average loss: 1.681\n",
      "Test Accuracy : 46.8%, Test Loss: 1.655826710164547\n",
      "Epoch Time (Training + Test) = 9.09 seconds\n",
      "epoch: 42 average loss: 1.683\n",
      "Test Accuracy : 50.2%, Test Loss: 1.6584299057722092\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.10 seconds\n",
      "epoch: 43 average loss: 1.695\n",
      "Test Accuracy : 49.0%, Test Loss: 1.632155004888773\n",
      "Epoch Time (Training + Test) = 9.03 seconds\n",
      "epoch: 44 average loss: 1.678\n",
      "Test Accuracy : 48.1%, Test Loss: 1.6263150461018085\n",
      "Epoch Time (Training + Test) = 9.03 seconds\n",
      "epoch: 45 average loss: 1.684\n",
      "Test Accuracy : 49.8%, Test Loss: 1.6461070328950882\n",
      "Epoch Time (Training + Test) = 9.24 seconds\n",
      "epoch: 46 average loss: 1.681\n",
      "Test Accuracy : 48.9%, Test Loss: 1.6287159062922\n",
      "Epoch Time (Training + Test) = 9.04 seconds\n",
      "epoch: 47 average loss: 1.670\n",
      "Test Accuracy : 49.7%, Test Loss: 1.6192572079598904\n",
      "Epoch Time (Training + Test) = 9.10 seconds\n",
      "epoch: 48 average loss: 1.677\n",
      "Test Accuracy : 49.8%, Test Loss: 1.59897818043828\n",
      "Epoch Time (Training + Test) = 9.03 seconds\n",
      "epoch: 49 average loss: 1.677\n",
      "Test Accuracy : 49.8%, Test Loss: 1.619527980685234\n",
      "Epoch Time (Training + Test) = 9.06 seconds\n",
      "epoch: 50 average loss: 1.676\n",
      "Test Accuracy : 49.5%, Test Loss: 1.5986392088234425\n",
      "Epoch Time (Training + Test) = 9.06 seconds\n",
      "epoch: 51 average loss: 1.668\n",
      "Test Accuracy : 48.5%, Test Loss: 1.6383802182972431\n",
      "Epoch Time (Training + Test) = 9.22 seconds\n",
      "epoch: 52 average loss: 1.671\n",
      "Test Accuracy : 48.8%, Test Loss: 1.6004227101802826\n",
      "Epoch Time (Training + Test) = 9.07 seconds\n",
      "epoch: 53 average loss: 1.650\n",
      "Test Accuracy : 49.4%, Test Loss: 1.6103554032742977\n",
      "Epoch Time (Training + Test) = 9.08 seconds\n",
      "epoch: 54 average loss: 1.662\n",
      "Test Accuracy : 49.2%, Test Loss: 1.6044527254998684\n",
      "Epoch Time (Training + Test) = 9.07 seconds\n",
      "epoch: 55 average loss: 1.659\n",
      "Test Accuracy : 50.2%, Test Loss: 1.5684990100562572\n",
      "Epoch Time (Training + Test) = 9.02 seconds\n",
      "epoch: 56 average loss: 1.654\n",
      "Test Accuracy : 50.9%, Test Loss: 1.5964896231889725\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.10 seconds\n",
      "epoch: 57 average loss: 1.662\n",
      "Test Accuracy : 50.4%, Test Loss: 1.6038256399333477\n",
      "Epoch Time (Training + Test) = 9.08 seconds\n",
      "epoch: 58 average loss: 1.640\n",
      "Test Accuracy : 48.8%, Test Loss: 1.6200811080634594\n",
      "Epoch Time (Training + Test) = 9.19 seconds\n",
      "epoch: 59 average loss: 1.658\n",
      "Test Accuracy : 50.9%, Test Loss: 1.5642615295946598\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.09 seconds\n",
      "epoch: 60 average loss: 1.649\n",
      "Test Accuracy : 51.2%, Test Loss: 1.566034596413374\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.03 seconds\n",
      "epoch: 61 average loss: 1.638\n",
      "Test Accuracy : 49.2%, Test Loss: 1.6060256473720074\n",
      "Epoch Time (Training + Test) = 9.08 seconds\n",
      "epoch: 62 average loss: 1.635\n",
      "Test Accuracy : 51.4%, Test Loss: 1.5759352892637253\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.14 seconds\n",
      "epoch: 63 average loss: 1.640\n",
      "Test Accuracy : 49.8%, Test Loss: 1.5876061581075191\n",
      "Epoch Time (Training + Test) = 9.14 seconds\n",
      "epoch: 64 average loss: 1.628\n",
      "Test Accuracy : 49.7%, Test Loss: 1.61278061196208\n",
      "Epoch Time (Training + Test) = 9.09 seconds\n",
      "epoch: 65 average loss: 1.624\n",
      "Test Accuracy : 48.5%, Test Loss: 1.6261733174324036\n",
      "Epoch Time (Training + Test) = 9.03 seconds\n",
      "epoch: 66 average loss: 1.636\n",
      "Test Accuracy : 49.8%, Test Loss: 1.5712687969207764\n",
      "Epoch Time (Training + Test) = 9.03 seconds\n",
      "epoch: 67 average loss: 1.633\n",
      "Test Accuracy : 49.6%, Test Loss: 1.5694750137627125\n",
      "Epoch Time (Training + Test) = 9.09 seconds\n",
      "epoch: 68 average loss: 1.637\n",
      "Test Accuracy : 50.6%, Test Loss: 1.5670985914766788\n",
      "Epoch Time (Training + Test) = 9.02 seconds\n",
      "epoch: 69 average loss: 1.613\n",
      "Test Accuracy : 49.8%, Test Loss: 1.6123761683702469\n",
      "Epoch Time (Training + Test) = 9.07 seconds\n",
      "epoch: 70 average loss: 1.633\n",
      "Test Accuracy : 49.6%, Test Loss: 1.6002809815108776\n",
      "Epoch Time (Training + Test) = 9.03 seconds\n",
      "epoch: 71 average loss: 1.623\n",
      "Test Accuracy : 50.3%, Test Loss: 1.5890102572739124\n",
      "Epoch Time (Training + Test) = 9.06 seconds\n",
      "epoch: 72 average loss: 1.614\n",
      "Test Accuracy : 49.9%, Test Loss: 1.5895838774740696\n",
      "Epoch Time (Training + Test) = 9.06 seconds\n",
      "epoch: 73 average loss: 1.620\n",
      "Test Accuracy : 52.1%, Test Loss: 1.5851090401411057\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.13 seconds\n",
      "epoch: 74 average loss: 1.614\n",
      "Test Accuracy : 51.3%, Test Loss: 1.5728426091372967\n",
      "Epoch Time (Training + Test) = 9.08 seconds\n",
      "epoch: 75 average loss: 1.605\n",
      "Test Accuracy : 51.1%, Test Loss: 1.5447211004793644\n",
      "Epoch Time (Training + Test) = 9.18 seconds\n",
      "Data Saved to 3_adapter_groups_5.csv\n",
      "Finished Training: \n",
      "Total Time 0.379008 hours\n",
      " Average Time Per Epoch 18.19 seconds\n",
      "No lr scheduler\n",
      "Optimizer Set to New: {'state': {}, 'param_groups': [{'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': False, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69]}]}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2k66t0eb) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Current Best Acc</td><td>▁▄▅▆▆▆▇▇▇▇▇▇▇▇▇█████████████████████████</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>epoch time (s)</td><td>▁▇████▇█▇▇▇▇██▇█▇▇▇▇▇▇▇▇▇▇▇█▇▇▇▇▇█▇▇▇▇██</td></tr><tr><td>lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_accuracy</td><td>▁▄▅▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇███▇██████▇█████</td></tr><tr><td>test_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>training_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Best Accuracy</td><td>0.521</td></tr><tr><td>Current Best Acc</td><td>0.521</td></tr><tr><td>Total Time (hours)</td><td>0.37901</td></tr><tr><td>epoch</td><td>75</td></tr><tr><td>epoch time (s)</td><td>9.17798</td></tr><tr><td>lr</td><td>0.0001</td></tr><tr><td>test_accuracy</td><td>0.511</td></tr><tr><td>test_loss</td><td>1.54472</td></tr><tr><td>training_loss</td><td>1.60511</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">autumn-mountain-3</strong>: <a href=\"https://wandb.ai/johnny_suu/Cifar80-20_Serial_Final/runs/2k66t0eb\" target=\"_blank\">https://wandb.ai/johnny_suu/Cifar80-20_Serial_Final/runs/2k66t0eb</a><br/>Synced 5 W&B file(s), 1 media file(s), 1 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230502_134939-2k66t0eb\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2k66t0eb). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31546718586c4b67bebebddff5089681",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\John Su\\Downloads\\SydneyUni\\2022\\thesis\\Thesis\\wandb\\run-20230502_140120-x5fgt9jh</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/johnny_suu/Cifar80-20_Serial_Final/runs/x5fgt9jh\" target=\"_blank\">fearless-energy-4</a></strong> to <a href=\"https://wandb.ai/johnny_suu/Cifar80-20_Serial_Final\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "Project Name: Cifar80-20_Serial_Final, Run Name 3_adapter_groups_10 \n",
      "\n",
      "\n",
      "Run Start : 2023-05-02 14-01-20\n",
      "start_epoch : 0\n",
      "initial_lr : 0.0001\n",
      "batch_size : 64\n",
      "epochs : 75\n",
      "epoch_chkpts : []\n",
      "device : cuda:0\n",
      "criterion : CrossEntropyLoss()\n",
      "optimizer : Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: False\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "seed : 123\n",
      "model_architecture : <class 'NN_Thesis.nn_classes.BNN_Resnet_UniAdapt'>\n",
      "binerised_training : True\n",
      "Number of Elements : 4396380\n",
      "Initial accuracy:\n",
      "Test Accuracy : 6.0%, Test Loss: 34.227029776117604\n",
      "best_acc.pth saved!\n",
      "Test Accuracy : 5.9%, Test Loss: 34.836553156375885\n",
      "epoch: 1 average loss: 2.741\n",
      "Test Accuracy : 25.1%, Test Loss: 2.4944059923291206\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.16 seconds\n",
      "epoch: 2 average loss: 2.487\n",
      "Test Accuracy : 29.0%, Test Loss: 2.367971710860729\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.23 seconds\n",
      "epoch: 3 average loss: 2.387\n",
      "Test Accuracy : 30.3%, Test Loss: 2.286326415836811\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.28 seconds\n",
      "epoch: 4 average loss: 2.322\n",
      "Test Accuracy : 31.8%, Test Loss: 2.241264447569847\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.26 seconds\n",
      "epoch: 5 average loss: 2.277\n",
      "Test Accuracy : 33.5%, Test Loss: 2.180229276418686\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.26 seconds\n",
      "epoch: 6 average loss: 2.234\n",
      "Test Accuracy : 34.9%, Test Loss: 2.1518806368112564\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.23 seconds\n",
      "epoch: 7 average loss: 2.199\n",
      "Test Accuracy : 36.5%, Test Loss: 2.1041697189211845\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.30 seconds\n",
      "epoch: 8 average loss: 2.166\n",
      "Test Accuracy : 36.5%, Test Loss: 2.0909092351794243\n",
      "Epoch Time (Training + Test) = 9.22 seconds\n",
      "epoch: 9 average loss: 2.137\n",
      "Test Accuracy : 37.1%, Test Loss: 2.0402183830738068\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.23 seconds\n",
      "epoch: 10 average loss: 2.122\n",
      "Test Accuracy : 37.4%, Test Loss: 2.029914729297161\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.25 seconds\n",
      "epoch: 11 average loss: 2.094\n",
      "Test Accuracy : 38.0%, Test Loss: 1.9905298873782158\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.24 seconds\n",
      "epoch: 12 average loss: 2.082\n",
      "Test Accuracy : 37.5%, Test Loss: 2.0166702158749104\n",
      "Epoch Time (Training + Test) = 9.25 seconds\n",
      "epoch: 13 average loss: 2.061\n",
      "Test Accuracy : 38.6%, Test Loss: 1.9741414971649647\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.30 seconds\n",
      "epoch: 14 average loss: 2.047\n",
      "Test Accuracy : 39.5%, Test Loss: 1.9428212381899357\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.12 seconds\n",
      "epoch: 15 average loss: 2.029\n",
      "Test Accuracy : 39.7%, Test Loss: 1.9488751292228699\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.26 seconds\n",
      "epoch: 16 average loss: 2.001\n",
      "Test Accuracy : 41.3%, Test Loss: 1.9367180690169334\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.34 seconds\n",
      "epoch: 17 average loss: 2.000\n",
      "Test Accuracy : 40.4%, Test Loss: 1.9289716929197311\n",
      "Epoch Time (Training + Test) = 9.35 seconds\n",
      "epoch: 18 average loss: 1.989\n",
      "Test Accuracy : 40.9%, Test Loss: 1.8980199173092842\n",
      "Epoch Time (Training + Test) = 9.27 seconds\n",
      "epoch: 19 average loss: 1.976\n",
      "Test Accuracy : 40.8%, Test Loss: 1.895173728466034\n",
      "Epoch Time (Training + Test) = 9.23 seconds\n",
      "epoch: 20 average loss: 1.965\n",
      "Test Accuracy : 41.3%, Test Loss: 1.8727807216346264\n",
      "Epoch Time (Training + Test) = 9.10 seconds\n",
      "epoch: 21 average loss: 1.952\n",
      "Test Accuracy : 42.4%, Test Loss: 1.8661692924797535\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.23 seconds\n",
      "epoch: 22 average loss: 1.957\n",
      "Test Accuracy : 41.2%, Test Loss: 1.8545845039188862\n",
      "Epoch Time (Training + Test) = 9.20 seconds\n",
      "epoch: 23 average loss: 1.942\n",
      "Test Accuracy : 43.6%, Test Loss: 1.8295560516417027\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.16 seconds\n",
      "epoch: 24 average loss: 1.943\n",
      "Test Accuracy : 43.0%, Test Loss: 1.832803025841713\n",
      "Epoch Time (Training + Test) = 9.22 seconds\n",
      "epoch: 25 average loss: 1.931\n",
      "Test Accuracy : 43.8%, Test Loss: 1.8514157608151436\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.24 seconds\n",
      "epoch: 26 average loss: 1.919\n",
      "Test Accuracy : 44.1%, Test Loss: 1.8365107960999012\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.26 seconds\n",
      "epoch: 27 average loss: 1.908\n",
      "Test Accuracy : 42.4%, Test Loss: 1.8185629211366177\n",
      "Epoch Time (Training + Test) = 9.23 seconds\n",
      "epoch: 28 average loss: 1.917\n",
      "Test Accuracy : 42.5%, Test Loss: 1.8121484629809856\n",
      "Epoch Time (Training + Test) = 9.23 seconds\n",
      "epoch: 29 average loss: 1.905\n",
      "Test Accuracy : 43.1%, Test Loss: 1.825162772089243\n",
      "Epoch Time (Training + Test) = 9.12 seconds\n",
      "epoch: 30 average loss: 1.893\n",
      "Test Accuracy : 43.8%, Test Loss: 1.7951106168329716\n",
      "Epoch Time (Training + Test) = 9.22 seconds\n",
      "epoch: 31 average loss: 1.883\n",
      "Test Accuracy : 43.9%, Test Loss: 1.8035511411726475\n",
      "Epoch Time (Training + Test) = 9.19 seconds\n",
      "epoch: 32 average loss: 1.907\n",
      "Test Accuracy : 44.1%, Test Loss: 1.7987082079052925\n",
      "Epoch Time (Training + Test) = 9.23 seconds\n",
      "epoch: 33 average loss: 1.877\n",
      "Test Accuracy : 44.0%, Test Loss: 1.7985168807208538\n",
      "Epoch Time (Training + Test) = 9.20 seconds\n",
      "epoch: 34 average loss: 1.885\n",
      "Test Accuracy : 43.6%, Test Loss: 1.7989262342453003\n",
      "Epoch Time (Training + Test) = 9.19 seconds\n",
      "epoch: 35 average loss: 1.867\n",
      "Test Accuracy : 43.6%, Test Loss: 1.8100527934730053\n",
      "Epoch Time (Training + Test) = 9.18 seconds\n",
      "epoch: 36 average loss: 1.872\n",
      "Test Accuracy : 43.8%, Test Loss: 1.7904286198318005\n",
      "Epoch Time (Training + Test) = 9.16 seconds\n",
      "epoch: 37 average loss: 1.865\n",
      "Test Accuracy : 43.8%, Test Loss: 1.7874758504331112\n",
      "Epoch Time (Training + Test) = 9.26 seconds\n",
      "epoch: 38 average loss: 1.878\n",
      "Test Accuracy : 44.9%, Test Loss: 1.7657798789441586\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.12 seconds\n",
      "epoch: 39 average loss: 1.852\n",
      "Test Accuracy : 44.0%, Test Loss: 1.7772751450538635\n",
      "Epoch Time (Training + Test) = 9.23 seconds\n",
      "epoch: 40 average loss: 1.846\n",
      "Test Accuracy : 44.0%, Test Loss: 1.7894705757498741\n",
      "Epoch Time (Training + Test) = 9.22 seconds\n",
      "epoch: 41 average loss: 1.834\n",
      "Test Accuracy : 43.1%, Test Loss: 1.7883305698633194\n",
      "Epoch Time (Training + Test) = 9.18 seconds\n",
      "epoch: 42 average loss: 1.849\n",
      "Test Accuracy : 45.1%, Test Loss: 1.7641953490674496\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.17 seconds\n",
      "epoch: 43 average loss: 1.838\n",
      "Test Accuracy : 45.4%, Test Loss: 1.7599518410861492\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.32 seconds\n",
      "epoch: 44 average loss: 1.829\n",
      "Test Accuracy : 46.0%, Test Loss: 1.724297370761633\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.23 seconds\n",
      "epoch: 45 average loss: 1.820\n",
      "Test Accuracy : 43.8%, Test Loss: 1.7723182365298271\n",
      "Epoch Time (Training + Test) = 9.21 seconds\n",
      "epoch: 46 average loss: 1.834\n",
      "Test Accuracy : 45.6%, Test Loss: 1.7513885796070099\n",
      "Epoch Time (Training + Test) = 9.21 seconds\n",
      "epoch: 47 average loss: 1.824\n",
      "Test Accuracy : 45.1%, Test Loss: 1.7552150040864944\n",
      "Epoch Time (Training + Test) = 9.10 seconds\n",
      "epoch: 48 average loss: 1.837\n",
      "Test Accuracy : 46.5%, Test Loss: 1.752398420125246\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.21 seconds\n",
      "epoch: 49 average loss: 1.830\n",
      "Test Accuracy : 45.6%, Test Loss: 1.7257068865001202\n",
      "Epoch Time (Training + Test) = 9.09 seconds\n",
      "epoch: 50 average loss: 1.830\n",
      "Test Accuracy : 45.8%, Test Loss: 1.7365945018827915\n",
      "Epoch Time (Training + Test) = 9.19 seconds\n",
      "epoch: 51 average loss: 1.825\n",
      "Test Accuracy : 46.9%, Test Loss: 1.6907444149255753\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.17 seconds\n",
      "epoch: 52 average loss: 1.825\n",
      "Test Accuracy : 46.0%, Test Loss: 1.7061675749719143\n",
      "Epoch Time (Training + Test) = 9.18 seconds\n",
      "epoch: 53 average loss: 1.817\n",
      "Test Accuracy : 46.7%, Test Loss: 1.7029343657195568\n",
      "Epoch Time (Training + Test) = 9.13 seconds\n",
      "epoch: 54 average loss: 1.807\n",
      "Test Accuracy : 45.7%, Test Loss: 1.7325468212366104\n",
      "Epoch Time (Training + Test) = 9.22 seconds\n",
      "epoch: 55 average loss: 1.811\n",
      "Test Accuracy : 47.0%, Test Loss: 1.7297395020723343\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.25 seconds\n",
      "epoch: 56 average loss: 1.816\n",
      "Test Accuracy : 45.6%, Test Loss: 1.7380094900727272\n",
      "Epoch Time (Training + Test) = 9.23 seconds\n",
      "epoch: 57 average loss: 1.813\n",
      "Test Accuracy : 45.2%, Test Loss: 1.7162540331482887\n",
      "Epoch Time (Training + Test) = 9.10 seconds\n",
      "epoch: 58 average loss: 1.797\n",
      "Test Accuracy : 45.0%, Test Loss: 1.7502247244119644\n",
      "Epoch Time (Training + Test) = 9.23 seconds\n",
      "epoch: 59 average loss: 1.812\n",
      "Test Accuracy : 45.5%, Test Loss: 1.691533524543047\n",
      "Epoch Time (Training + Test) = 9.19 seconds\n",
      "epoch: 60 average loss: 1.794\n",
      "Test Accuracy : 47.0%, Test Loss: 1.6970778219401836\n",
      "Epoch Time (Training + Test) = 9.14 seconds\n",
      "epoch: 61 average loss: 1.804\n",
      "Test Accuracy : 45.9%, Test Loss: 1.6957929134368896\n",
      "Epoch Time (Training + Test) = 9.21 seconds\n",
      "epoch: 62 average loss: 1.801\n",
      "Test Accuracy : 46.7%, Test Loss: 1.7052352838218212\n",
      "Epoch Time (Training + Test) = 9.10 seconds\n",
      "epoch: 63 average loss: 1.792\n",
      "Test Accuracy : 46.3%, Test Loss: 1.7070231288671494\n",
      "Epoch Time (Training + Test) = 9.20 seconds\n",
      "epoch: 64 average loss: 1.785\n",
      "Test Accuracy : 46.6%, Test Loss: 1.6960238218307495\n",
      "Epoch Time (Training + Test) = 9.13 seconds\n",
      "epoch: 65 average loss: 1.787\n",
      "Test Accuracy : 46.3%, Test Loss: 1.7384206242859364\n",
      "Epoch Time (Training + Test) = 9.17 seconds\n",
      "epoch: 66 average loss: 1.781\n",
      "Test Accuracy : 47.1%, Test Loss: 1.7000301964581013\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.26 seconds\n",
      "epoch: 67 average loss: 1.771\n",
      "Test Accuracy : 46.8%, Test Loss: 1.7088118456304073\n",
      "Epoch Time (Training + Test) = 9.14 seconds\n",
      "epoch: 68 average loss: 1.781\n",
      "Test Accuracy : 46.4%, Test Loss: 1.6980252154171467\n",
      "Epoch Time (Training + Test) = 9.20 seconds\n",
      "epoch: 69 average loss: 1.772\n",
      "Test Accuracy : 46.6%, Test Loss: 1.697642371058464\n",
      "Epoch Time (Training + Test) = 9.22 seconds\n",
      "epoch: 70 average loss: 1.775\n",
      "Test Accuracy : 46.9%, Test Loss: 1.6789091676473618\n",
      "Epoch Time (Training + Test) = 9.13 seconds\n",
      "epoch: 71 average loss: 1.773\n",
      "Test Accuracy : 46.9%, Test Loss: 1.68921510130167\n",
      "Epoch Time (Training + Test) = 9.23 seconds\n",
      "epoch: 72 average loss: 1.773\n",
      "Test Accuracy : 47.3%, Test Loss: 1.6747081466019154\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.28 seconds\n",
      "epoch: 73 average loss: 1.775\n",
      "Test Accuracy : 47.6%, Test Loss: 1.665263794362545\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.25 seconds\n",
      "epoch: 74 average loss: 1.756\n",
      "Test Accuracy : 45.1%, Test Loss: 1.6919133961200714\n",
      "Epoch Time (Training + Test) = 9.24 seconds\n",
      "epoch: 75 average loss: 1.774\n",
      "Test Accuracy : 47.7%, Test Loss: 1.651834636926651\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.29 seconds\n",
      "Data Saved to 3_adapter_groups_10.csv\n",
      "Finished Training: \n",
      "Total Time 0.383762 hours\n",
      " Average Time Per Epoch 18.42 seconds\n",
      "No lr scheduler\n",
      "Optimizer Set to New: {'state': {}, 'param_groups': [{'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': False, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69]}]}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:x5fgt9jh) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Current Best Acc</td><td>▁▄▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇████████████████████</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>epoch time (s)</td><td>▁▇██████████▇█████▇▇▇█▇█▇▇▇▇▇█▇▇▇▇▇▇████</td></tr><tr><td>lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_accuracy</td><td>▁▄▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇█▇██████████████████</td></tr><tr><td>test_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>training_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Best Accuracy</td><td>0.477</td></tr><tr><td>Current Best Acc</td><td>0.477</td></tr><tr><td>Total Time (hours)</td><td>0.38376</td></tr><tr><td>epoch</td><td>75</td></tr><tr><td>epoch time (s)</td><td>9.29007</td></tr><tr><td>lr</td><td>0.0001</td></tr><tr><td>test_accuracy</td><td>0.477</td></tr><tr><td>test_loss</td><td>1.65183</td></tr><tr><td>training_loss</td><td>1.77394</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">fearless-energy-4</strong>: <a href=\"https://wandb.ai/johnny_suu/Cifar80-20_Serial_Final/runs/x5fgt9jh\" target=\"_blank\">https://wandb.ai/johnny_suu/Cifar80-20_Serial_Final/runs/x5fgt9jh</a><br/>Synced 5 W&B file(s), 1 media file(s), 1 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230502_140120-x5fgt9jh\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:x5fgt9jh). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "387045ffebfb4534a0720c69e04703e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\John Su\\Downloads\\SydneyUni\\2022\\thesis\\Thesis\\wandb\\run-20230502_141312-s7mllp6l</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/johnny_suu/Cifar80-20_Serial_Final/runs/s7mllp6l\" target=\"_blank\">atomic-glade-5</a></strong> to <a href=\"https://wandb.ai/johnny_suu/Cifar80-20_Serial_Final\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "Project Name: Cifar80-20_Serial_Final, Run Name 3_adapter_groups_20 \n",
      "\n",
      "\n",
      "Run Start : 2023-05-02 14-13-12\n",
      "start_epoch : 0\n",
      "initial_lr : 0.0001\n",
      "batch_size : 64\n",
      "epochs : 75\n",
      "epoch_chkpts : []\n",
      "device : cuda:0\n",
      "criterion : CrossEntropyLoss()\n",
      "optimizer : Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: False\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "seed : 123\n",
      "model_architecture : <class 'NN_Thesis.nn_classes.BNN_Resnet_UniAdapt'>\n",
      "binerised_training : True\n",
      "Number of Elements : 4389660\n",
      "Initial accuracy:\n",
      "Test Accuracy : 4.6%, Test Loss: 39.31318465311816\n",
      "best_acc.pth saved!\n",
      "Test Accuracy : 5.0%, Test Loss: 39.90152990818024\n",
      "best_acc.pth saved!\n",
      "epoch: 1 average loss: 2.775\n",
      "Test Accuracy : 22.9%, Test Loss: 2.518232934176922\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.12 seconds\n",
      "epoch: 2 average loss: 2.504\n",
      "Test Accuracy : 27.0%, Test Loss: 2.402627818286419\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.21 seconds\n",
      "epoch: 3 average loss: 2.432\n",
      "Test Accuracy : 30.0%, Test Loss: 2.3239105492830276\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.24 seconds\n",
      "epoch: 4 average loss: 2.386\n",
      "Test Accuracy : 30.4%, Test Loss: 2.305947758257389\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.21 seconds\n",
      "epoch: 5 average loss: 2.343\n",
      "Test Accuracy : 30.0%, Test Loss: 2.265453301370144\n",
      "Epoch Time (Training + Test) = 9.28 seconds\n",
      "epoch: 6 average loss: 2.324\n",
      "Test Accuracy : 32.5%, Test Loss: 2.2261900827288628\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.11 seconds\n",
      "epoch: 7 average loss: 2.297\n",
      "Test Accuracy : 33.4%, Test Loss: 2.208505157381296\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.26 seconds\n",
      "epoch: 8 average loss: 2.262\n",
      "Test Accuracy : 33.5%, Test Loss: 2.1985099464654922\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.20 seconds\n",
      "epoch: 9 average loss: 2.240\n",
      "Test Accuracy : 34.1%, Test Loss: 2.1673069670796394\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.12 seconds\n",
      "epoch: 10 average loss: 2.230\n",
      "Test Accuracy : 36.9%, Test Loss: 2.151363916695118\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.22 seconds\n",
      "epoch: 11 average loss: 2.209\n",
      "Test Accuracy : 35.1%, Test Loss: 2.137565866112709\n",
      "Epoch Time (Training + Test) = 9.21 seconds\n",
      "epoch: 12 average loss: 2.187\n",
      "Test Accuracy : 34.1%, Test Loss: 2.1357987001538277\n",
      "Epoch Time (Training + Test) = 9.19 seconds\n",
      "epoch: 13 average loss: 2.183\n",
      "Test Accuracy : 35.5%, Test Loss: 2.1235733963549137\n",
      "Epoch Time (Training + Test) = 9.18 seconds\n",
      "epoch: 14 average loss: 2.172\n",
      "Test Accuracy : 35.5%, Test Loss: 2.1002258956432343\n",
      "Epoch Time (Training + Test) = 9.09 seconds\n",
      "epoch: 15 average loss: 2.165\n",
      "Test Accuracy : 37.1%, Test Loss: 2.0940713435411453\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.23 seconds\n",
      "epoch: 16 average loss: 2.139\n",
      "Test Accuracy : 36.3%, Test Loss: 2.0719694830477238\n",
      "Epoch Time (Training + Test) = 9.09 seconds\n",
      "epoch: 17 average loss: 2.132\n",
      "Test Accuracy : 36.1%, Test Loss: 2.0449925251305103\n",
      "Epoch Time (Training + Test) = 9.21 seconds\n",
      "epoch: 18 average loss: 2.133\n",
      "Test Accuracy : 36.7%, Test Loss: 2.06828223913908\n",
      "Epoch Time (Training + Test) = 9.08 seconds\n",
      "epoch: 19 average loss: 2.113\n",
      "Test Accuracy : 37.4%, Test Loss: 2.0322021655738354\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.24 seconds\n",
      "epoch: 20 average loss: 2.095\n",
      "Test Accuracy : 37.1%, Test Loss: 2.0260617434978485\n",
      "Epoch Time (Training + Test) = 9.26 seconds\n",
      "epoch: 21 average loss: 2.107\n",
      "Test Accuracy : 37.8%, Test Loss: 2.020170673727989\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.24 seconds\n",
      "epoch: 22 average loss: 2.084\n",
      "Test Accuracy : 38.2%, Test Loss: 2.0120269395411015\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.27 seconds\n",
      "epoch: 23 average loss: 2.074\n",
      "Test Accuracy : 38.8%, Test Loss: 2.010798029601574\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.12 seconds\n",
      "epoch: 24 average loss: 2.093\n",
      "Test Accuracy : 38.5%, Test Loss: 2.0097114965319633\n",
      "Epoch Time (Training + Test) = 9.17 seconds\n",
      "epoch: 25 average loss: 2.074\n",
      "Test Accuracy : 39.5%, Test Loss: 1.9985773675143719\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.14 seconds\n",
      "epoch: 26 average loss: 2.059\n",
      "Test Accuracy : 38.8%, Test Loss: 1.9823399484157562\n",
      "Epoch Time (Training + Test) = 9.17 seconds\n",
      "epoch: 27 average loss: 2.058\n",
      "Test Accuracy : 37.2%, Test Loss: 1.9874848425388336\n",
      "Epoch Time (Training + Test) = 9.09 seconds\n",
      "epoch: 28 average loss: 2.066\n",
      "Test Accuracy : 37.6%, Test Loss: 2.011233016848564\n",
      "Epoch Time (Training + Test) = 9.20 seconds\n",
      "epoch: 29 average loss: 2.063\n",
      "Test Accuracy : 38.6%, Test Loss: 1.9817181304097176\n",
      "Epoch Time (Training + Test) = 9.10 seconds\n",
      "epoch: 30 average loss: 2.046\n",
      "Test Accuracy : 38.6%, Test Loss: 1.9922749362885952\n",
      "Epoch Time (Training + Test) = 9.08 seconds\n",
      "epoch: 31 average loss: 2.042\n",
      "Test Accuracy : 38.9%, Test Loss: 2.0022237934172153\n",
      "Epoch Time (Training + Test) = 9.08 seconds\n",
      "epoch: 32 average loss: 2.042\n",
      "Test Accuracy : 38.9%, Test Loss: 1.9833348654210567\n",
      "Epoch Time (Training + Test) = 9.16 seconds\n",
      "epoch: 33 average loss: 2.036\n",
      "Test Accuracy : 39.4%, Test Loss: 1.9788904748857021\n",
      "Epoch Time (Training + Test) = 9.11 seconds\n",
      "epoch: 34 average loss: 2.027\n",
      "Test Accuracy : 38.6%, Test Loss: 1.964209221303463\n",
      "Epoch Time (Training + Test) = 9.22 seconds\n",
      "epoch: 35 average loss: 2.030\n",
      "Test Accuracy : 38.8%, Test Loss: 1.9715580381453037\n",
      "Epoch Time (Training + Test) = 9.21 seconds\n",
      "epoch: 36 average loss: 2.023\n",
      "Test Accuracy : 39.3%, Test Loss: 1.9746944978833199\n",
      "Epoch Time (Training + Test) = 9.15 seconds\n",
      "epoch: 37 average loss: 2.025\n",
      "Test Accuracy : 40.4%, Test Loss: 1.9479984678328037\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.07 seconds\n",
      "epoch: 38 average loss: 2.042\n",
      "Test Accuracy : 39.2%, Test Loss: 1.968647487461567\n",
      "Epoch Time (Training + Test) = 9.16 seconds\n",
      "epoch: 39 average loss: 2.020\n",
      "Test Accuracy : 41.6%, Test Loss: 1.9322612918913364\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.17 seconds\n",
      "epoch: 40 average loss: 2.034\n",
      "Test Accuracy : 39.6%, Test Loss: 1.952269297093153\n",
      "Epoch Time (Training + Test) = 9.16 seconds\n",
      "epoch: 41 average loss: 2.010\n",
      "Test Accuracy : 39.1%, Test Loss: 1.9484731703996658\n",
      "Epoch Time (Training + Test) = 9.11 seconds\n",
      "epoch: 42 average loss: 2.010\n",
      "Test Accuracy : 39.8%, Test Loss: 1.9638875685632229\n",
      "Epoch Time (Training + Test) = 9.10 seconds\n",
      "epoch: 43 average loss: 2.013\n",
      "Test Accuracy : 39.9%, Test Loss: 1.9318367205560207\n",
      "Epoch Time (Training + Test) = 9.07 seconds\n",
      "epoch: 44 average loss: 2.006\n",
      "Test Accuracy : 39.6%, Test Loss: 1.960177980363369\n",
      "Epoch Time (Training + Test) = 9.19 seconds\n",
      "epoch: 45 average loss: 2.006\n",
      "Test Accuracy : 40.6%, Test Loss: 1.9136515893042088\n",
      "Epoch Time (Training + Test) = 9.09 seconds\n",
      "epoch: 46 average loss: 1.997\n",
      "Test Accuracy : 40.9%, Test Loss: 1.9374969340860844\n",
      "Epoch Time (Training + Test) = 9.06 seconds\n",
      "epoch: 47 average loss: 1.993\n",
      "Test Accuracy : 38.5%, Test Loss: 1.953460268676281\n",
      "Epoch Time (Training + Test) = 9.20 seconds\n",
      "epoch: 48 average loss: 2.002\n",
      "Test Accuracy : 39.9%, Test Loss: 1.9199883379042149\n",
      "Epoch Time (Training + Test) = 9.10 seconds\n",
      "epoch: 49 average loss: 1.992\n",
      "Test Accuracy : 39.9%, Test Loss: 1.9432900696992874\n",
      "Epoch Time (Training + Test) = 9.07 seconds\n",
      "epoch: 50 average loss: 1.989\n",
      "Test Accuracy : 40.4%, Test Loss: 1.8975357748568058\n",
      "Epoch Time (Training + Test) = 9.19 seconds\n",
      "epoch: 51 average loss: 1.985\n",
      "Test Accuracy : 39.4%, Test Loss: 1.9287139065563679\n",
      "Epoch Time (Training + Test) = 9.09 seconds\n",
      "epoch: 52 average loss: 1.999\n",
      "Test Accuracy : 39.7%, Test Loss: 1.91085684299469\n",
      "Epoch Time (Training + Test) = 9.21 seconds\n",
      "epoch: 53 average loss: 1.981\n",
      "Test Accuracy : 41.1%, Test Loss: 1.9079284816980362\n",
      "Epoch Time (Training + Test) = 9.11 seconds\n",
      "epoch: 54 average loss: 1.982\n",
      "Test Accuracy : 42.4%, Test Loss: 1.8813670873641968\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.08 seconds\n",
      "epoch: 55 average loss: 1.966\n",
      "Test Accuracy : 41.8%, Test Loss: 1.8941216692328453\n",
      "Epoch Time (Training + Test) = 9.21 seconds\n",
      "epoch: 56 average loss: 1.991\n",
      "Test Accuracy : 40.8%, Test Loss: 1.8814025558531284\n",
      "Epoch Time (Training + Test) = 9.24 seconds\n",
      "epoch: 57 average loss: 1.977\n",
      "Test Accuracy : 40.4%, Test Loss: 1.9007790721952915\n",
      "Epoch Time (Training + Test) = 9.09 seconds\n",
      "epoch: 58 average loss: 1.973\n",
      "Test Accuracy : 40.6%, Test Loss: 1.9320199079811573\n",
      "Epoch Time (Training + Test) = 9.07 seconds\n",
      "epoch: 59 average loss: 1.973\n",
      "Test Accuracy : 41.6%, Test Loss: 1.8847349770367146\n",
      "Epoch Time (Training + Test) = 9.05 seconds\n",
      "epoch: 60 average loss: 1.976\n",
      "Test Accuracy : 40.7%, Test Loss: 1.9068669192492962\n",
      "Epoch Time (Training + Test) = 9.08 seconds\n",
      "epoch: 61 average loss: 1.964\n",
      "Test Accuracy : 42.3%, Test Loss: 1.8827042430639267\n",
      "Epoch Time (Training + Test) = 9.21 seconds\n",
      "epoch: 62 average loss: 1.973\n",
      "Test Accuracy : 40.1%, Test Loss: 1.9026143737137318\n",
      "Epoch Time (Training + Test) = 9.11 seconds\n",
      "epoch: 63 average loss: 1.961\n",
      "Test Accuracy : 40.1%, Test Loss: 1.8991196043789387\n",
      "Epoch Time (Training + Test) = 9.21 seconds\n",
      "epoch: 64 average loss: 1.967\n",
      "Test Accuracy : 42.6%, Test Loss: 1.8775782622396946\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.09 seconds\n",
      "epoch: 65 average loss: 1.968\n",
      "Test Accuracy : 41.8%, Test Loss: 1.870229922235012\n",
      "Epoch Time (Training + Test) = 9.08 seconds\n",
      "epoch: 66 average loss: 1.957\n",
      "Test Accuracy : 41.5%, Test Loss: 1.8843672163784504\n",
      "Epoch Time (Training + Test) = 9.19 seconds\n",
      "epoch: 67 average loss: 1.960\n",
      "Test Accuracy : 40.2%, Test Loss: 1.8989227376878262\n",
      "Epoch Time (Training + Test) = 9.24 seconds\n",
      "epoch: 68 average loss: 1.964\n",
      "Test Accuracy : 41.5%, Test Loss: 1.8780253268778324\n",
      "Epoch Time (Training + Test) = 9.30 seconds\n",
      "epoch: 69 average loss: 1.957\n",
      "Test Accuracy : 42.8%, Test Loss: 1.8882363736629486\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.09 seconds\n",
      "epoch: 70 average loss: 1.966\n",
      "Test Accuracy : 41.3%, Test Loss: 1.890750303864479\n",
      "Epoch Time (Training + Test) = 9.21 seconds\n",
      "epoch: 71 average loss: 1.958\n",
      "Test Accuracy : 41.4%, Test Loss: 1.8759142979979515\n",
      "Epoch Time (Training + Test) = 9.08 seconds\n",
      "epoch: 72 average loss: 1.941\n",
      "Test Accuracy : 41.7%, Test Loss: 1.8892643451690674\n",
      "Epoch Time (Training + Test) = 9.21 seconds\n",
      "epoch: 73 average loss: 1.952\n",
      "Test Accuracy : 42.1%, Test Loss: 1.8789686411619186\n",
      "Epoch Time (Training + Test) = 9.07 seconds\n",
      "epoch: 74 average loss: 1.953\n",
      "Test Accuracy : 40.8%, Test Loss: 1.889970239251852\n",
      "Epoch Time (Training + Test) = 9.07 seconds\n",
      "epoch: 75 average loss: 1.965\n",
      "Test Accuracy : 41.9%, Test Loss: 1.860107447952032\n",
      "Epoch Time (Training + Test) = 9.07 seconds\n",
      "Data Saved to 3_adapter_groups_20.csv\n",
      "Finished Training: \n",
      "Total Time 0.381361 hours\n",
      " Average Time Per Epoch 18.31 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for group in [1,2,5,10,20]:\n",
    "\n",
    "    conv_bn = conv_channel_adapter3(80,groups = group,kernel = 1, padding= 0,nonlinearity='relu')\n",
    "    conv_bn2 = conv_channel_adapter3(160,groups = group,kernel = 1, padding= 0,nonlinearity='relu')\n",
    "    conv_bn3 = conv_channel_adapter3(320,groups = group,kernel = 1, padding= 0,nonlinearity='relu')\n",
    "    # adapt_fine2.add_adapter(after = 'bn2',adapter = bottleneck)\n",
    "    \n",
    "    # conv_bn = conv_channel_adapter3(input_channel,kernel = 1, padding= 0,groups= input_channel//10)\n",
    "    adapt_fine2 = BNN_Resnet_UniAdapt(num_classes=80)\n",
    "    \n",
    "    adapt_fine2.load_state_dict(state,strict=False)\n",
    "    \n",
    "    # adapt_fine2.head = nn.Linear(320+160,80)\n",
    "    # adapt_fine2.head_bn = nn.BatchNorm1d(480)\n",
    "    adapt_fine2.freeze()\n",
    "    # adapt_fine2.add_adapter(after = 'bn2',adapter = bottleneck)\n",
    "    adapt_fine2.add_adapter(after = 'layer1',adapter = conv_bn)\n",
    "    adapt_fine2.add_adapter(after = 'layer2',adapter = conv_bn2)\n",
    "    adapt_fine2.add_adapter(after = 'layer3',adapter = conv_bn3)\n",
    "\n",
    "\n",
    "    trainer = Trainer(model = adapt_fine2,seed = 123,model_name = f'3_adapter_groups_{group}',project_name = 'Cifar80-20_Serial_Final',classes = train_20.classes,binarise= True)\n",
    "    m = trainer.model\n",
    "    m.to(trainer.device)\n",
    "    m.fc = BinarizeLinear(320,20)\n",
    "    m.bn3 = nn.BatchNorm1d(20)\n",
    "    # for layer in [m.fc,m.bn3]:\n",
    "    #     for p in layer.parameters():\n",
    "    #         p.requires_grad = True\n",
    "    #     layer.train()\n",
    "    trainer.lr = 1e-4\n",
    "    trainer.batch_size = 64\n",
    "    trainer.epochs =75\n",
    "    trainer.epoch_chkpts = []\n",
    "    trainer.start_epoch = 0\n",
    "    trainer.set_scheduler(None)\n",
    "    # adpt = getattr(m,after_layer)[-1]\n",
    "    # print(adpt)\n",
    "    # print(m.fc.weight[0,0])\n",
    "    # params = nn.ModuleList([m.fc,m.bn3,adpt]).parameters()\n",
    "    trainer.set_optimizer(torch.optim.Adam)\n",
    "    trainer.train(train20_DL,test20_DL)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4280fb70",
   "metadata": {},
   "source": [
    "# Uni Adapt FineTune\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8e6d269e",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'SavedModels\\UniAdapt_Cifar100\\BNN_Cifar80_Backbone_2023-04-18 16-54-59\\BNN_Cifar80_Backbone_best_acc.pth'\n",
    "\n",
    "state = torch.load(path,map_location='cuda:0')\n",
    "BNN = BNN_Resnet_UniAdapt(80,freeze_pre_weights=True)\n",
    "BNN.layer3[-1].do_bntan = True\n",
    "# for key in state.keys():\n",
    "#     if 'decoders' in key or 'encoders' in key:\n",
    "#         del state[key] \n",
    "\n",
    "BNN.load_state_dict(state,strict=False)\n",
    "BNN.freeze()\n",
    "\n",
    "BNN = BNN.to('cuda:0')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca32cd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "BNN.freeze()\n",
    "BNN.head_bn =  nn.BatchNorm1d(20)\n",
    "BNN.head =  BinarizeLinear(320 + 320,20)\n",
    "BNN.add_UniAdapter(adapt_net)\n",
    "BNN = BNN.to('cuda:0')\n",
    "BNN.uniAdapt = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "74724ede",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 320])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "BNN.encoder_net = placeholder()\n",
    "\n",
    "n1 = uniAdapt_Net([80,160,320],[80,160,320],block = thinBlock2,pooling='max')\n",
    "n1 = n1.cuda()\n",
    "n1.train()\n",
    "# adapt_net = nn.Sequential(n1,nn.AdaptiveMaxPool2d(1),nn.Flatten(),nn.Linear(320,100))\n",
    "adapt_net = nn.Sequential(n1,nn.Flatten())\n",
    "adapt_net.to('cuda:0')\n",
    "\n",
    "y = [torch.rand((4,80,32,32)).cuda(),torch.rand((4,160,16,16)).cuda(),torch.rand((4,320,8,8)).cuda()]\n",
    "adapt_net(y).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6d9055e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No lr scheduler\n",
      "Optimizer Set to New: {'state': {}, 'param_groups': [{'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': False, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72]}]}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:36pi21pk) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb04dbcf72974f06bb603da32ece3c99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Current Best Acc</td><td>▁</td></tr><tr><td>epoch</td><td>▁</td></tr><tr><td>epoch time (s)</td><td>▁</td></tr><tr><td>lr</td><td>▁</td></tr><tr><td>test_accuracy</td><td>▁</td></tr><tr><td>test_loss</td><td>▁</td></tr><tr><td>training_loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Current Best Acc</td><td>0.056</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>epoch time (s)</td><td>8.53296</td></tr><tr><td>lr</td><td>0.001</td></tr><tr><td>test_accuracy</td><td>0.056</td></tr><tr><td>test_loss</td><td>9.31592</td></tr><tr><td>training_loss</td><td>9.37752</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">solar-dream-11</strong>: <a href=\"https://wandb.ai/johnny_suu/UniAdapt_Cifar100_Finetuning/runs/36pi21pk\" target=\"_blank\">https://wandb.ai/johnny_suu/UniAdapt_Cifar100_Finetuning/runs/36pi21pk</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230421_114352-36pi21pk\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:36pi21pk). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebf1495b0e1f48698135482d05d99279",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\John Su\\Downloads\\SydneyUni\\2022\\thesis\\Thesis\\wandb\\run-20230421_114531-2g5pacpn</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/johnny_suu/UniAdapt_Cifar100_Finetuning/runs/2g5pacpn\" target=\"_blank\">daily-field-12</a></strong> to <a href=\"https://wandb.ai/johnny_suu/UniAdapt_Cifar100_Finetuning\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "Project Name: UniAdapt_Cifar100_Finetuning, Run Name BNN_uniadapt_maxpooling_thinblock2 \n",
      "\n",
      "\n",
      "Run Start : 2023-04-21 11-45-31\n",
      "start_epoch : 0\n",
      "initial_lr : 0.001\n",
      "batch_size : 64\n",
      "epochs : 75\n",
      "epoch_chkpts : []\n",
      "device : cuda:0\n",
      "criterion : CrossEntropyLoss()\n",
      "optimizer : Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: False\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "seed : None\n",
      "model_architecture : <class 'NN_Thesis.nn_classes.BNN_Resnet_UniAdapt'>\n",
      "binerised_training : True\n",
      "Number of Elements : 4382060\n",
      "Initial accuracy:\n",
      "Test Accuracy : 4.9%, Test Loss: 45.947143105184956\n",
      "best_acc.pth saved!\n",
      "Test Accuracy : 5.1%, Test Loss: 47.83417749404907\n",
      "best_acc.pth saved!\n",
      "epoch: 1 average loss: 1.877\n",
      "Test Accuracy : 63.6%, Test Loss: 1.4546271674335003\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.97 seconds\n",
      "epoch: 2 average loss: 1.491\n",
      "Test Accuracy : 65.5%, Test Loss: 1.3525913059711456\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 10.39 seconds\n",
      "epoch: 3 average loss: 1.389\n",
      "Test Accuracy : 64.8%, Test Loss: 1.2963851802051067\n",
      "Epoch Time (Training + Test) = 10.23 seconds\n",
      "epoch: 4 average loss: 1.344\n",
      "Test Accuracy : 67.4%, Test Loss: 1.2417513616383076\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 10.14 seconds\n",
      "epoch: 5 average loss: 1.303\n",
      "Test Accuracy : 67.0%, Test Loss: 1.2052138298749924\n",
      "Epoch Time (Training + Test) = 10.06 seconds\n",
      "epoch: 6 average loss: 1.270\n",
      "Test Accuracy : 66.1%, Test Loss: 1.1788365878164768\n",
      "Epoch Time (Training + Test) = 10.04 seconds\n",
      "epoch: 7 average loss: 1.245\n",
      "Test Accuracy : 67.5%, Test Loss: 1.1589868310838938\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.77 seconds\n",
      "epoch: 8 average loss: 1.221\n",
      "Test Accuracy : 67.1%, Test Loss: 1.1294445507228374\n",
      "Epoch Time (Training + Test) = 9.72 seconds\n",
      "epoch: 9 average loss: 1.198\n",
      "Test Accuracy : 67.7%, Test Loss: 1.114029647782445\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.71 seconds\n",
      "epoch: 10 average loss: 1.183\n",
      "Test Accuracy : 66.6%, Test Loss: 1.1105725839734077\n",
      "Epoch Time (Training + Test) = 10.38 seconds\n",
      "epoch: 11 average loss: 1.164\n",
      "Test Accuracy : 66.0%, Test Loss: 1.0933040045201778\n",
      "Epoch Time (Training + Test) = 10.32 seconds\n",
      "epoch: 12 average loss: 1.154\n",
      "Test Accuracy : 66.3%, Test Loss: 1.0967530310153961\n",
      "Epoch Time (Training + Test) = 10.36 seconds\n",
      "epoch: 13 average loss: 1.144\n",
      "Test Accuracy : 67.5%, Test Loss: 1.0847659166902304\n",
      "Epoch Time (Training + Test) = 10.23 seconds\n",
      "epoch: 14 average loss: 1.136\n",
      "Test Accuracy : 66.5%, Test Loss: 1.053278461098671\n",
      "Epoch Time (Training + Test) = 10.18 seconds\n",
      "epoch: 15 average loss: 1.122\n",
      "Test Accuracy : 66.8%, Test Loss: 1.0711071528494358\n",
      "Epoch Time (Training + Test) = 10.04 seconds\n",
      "epoch: 16 average loss: 1.119\n",
      "Test Accuracy : 68.2%, Test Loss: 1.0586159583181143\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 10.04 seconds\n",
      "epoch: 17 average loss: 1.101\n",
      "Test Accuracy : 67.7%, Test Loss: 1.0465469770133495\n",
      "Epoch Time (Training + Test) = 9.99 seconds\n",
      "epoch: 18 average loss: 1.107\n",
      "Test Accuracy : 67.2%, Test Loss: 1.0508799757808447\n",
      "Epoch Time (Training + Test) = 10.23 seconds\n",
      "epoch: 19 average loss: 1.090\n",
      "Test Accuracy : 68.8%, Test Loss: 1.0128111485391855\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 10.16 seconds\n",
      "epoch: 20 average loss: 1.093\n",
      "Test Accuracy : 67.3%, Test Loss: 1.0425234641879797\n",
      "Epoch Time (Training + Test) = 10.02 seconds\n",
      "epoch: 21 average loss: 1.084\n",
      "Test Accuracy : 67.2%, Test Loss: 1.0441348366439342\n",
      "Epoch Time (Training + Test) = 9.99 seconds\n",
      "epoch: 22 average loss: 1.092\n",
      "Test Accuracy : 67.0%, Test Loss: 1.0401820857077837\n",
      "Epoch Time (Training + Test) = 10.60 seconds\n",
      "epoch: 23 average loss: 1.090\n",
      "Test Accuracy : 66.3%, Test Loss: 1.0453402530401945\n",
      "Epoch Time (Training + Test) = 10.13 seconds\n",
      "epoch: 24 average loss: 1.085\n",
      "Test Accuracy : 67.9%, Test Loss: 0.9967698603868484\n",
      "Epoch Time (Training + Test) = 9.89 seconds\n",
      "epoch: 25 average loss: 1.075\n",
      "Test Accuracy : 68.2%, Test Loss: 1.0260960385203362\n",
      "Epoch Time (Training + Test) = 9.84 seconds\n",
      "epoch: 26 average loss: 1.077\n",
      "Test Accuracy : 67.5%, Test Loss: 1.03018444404006\n",
      "Epoch Time (Training + Test) = 9.81 seconds\n",
      "epoch: 27 average loss: 1.063\n",
      "Test Accuracy : 67.2%, Test Loss: 1.023394638672471\n",
      "Epoch Time (Training + Test) = 9.75 seconds\n",
      "epoch: 28 average loss: 1.044\n",
      "Test Accuracy : 67.8%, Test Loss: 1.0001729540526867\n",
      "Epoch Time (Training + Test) = 9.68 seconds\n",
      "epoch: 29 average loss: 1.048\n",
      "Test Accuracy : 69.0%, Test Loss: 0.9839091915637255\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.84 seconds\n",
      "epoch: 30 average loss: 1.061\n",
      "Test Accuracy : 68.0%, Test Loss: 1.0043559465557337\n",
      "Epoch Time (Training + Test) = 9.67 seconds\n",
      "epoch: 31 average loss: 1.049\n",
      "Test Accuracy : 68.2%, Test Loss: 1.0044752694666386\n",
      "Epoch Time (Training + Test) = 9.66 seconds\n",
      "epoch: 32 average loss: 1.050\n",
      "Test Accuracy : 68.7%, Test Loss: 0.9954398162662983\n",
      "Epoch Time (Training + Test) = 9.81 seconds\n",
      "epoch: 33 average loss: 1.041\n",
      "Test Accuracy : 68.5%, Test Loss: 1.0023807361721992\n",
      "Epoch Time (Training + Test) = 9.74 seconds\n",
      "epoch: 34 average loss: 1.035\n",
      "Test Accuracy : 68.2%, Test Loss: 1.0044232103973627\n",
      "Epoch Time (Training + Test) = 9.69 seconds\n",
      "epoch: 35 average loss: 1.039\n",
      "Test Accuracy : 68.0%, Test Loss: 0.9930964764207602\n",
      "Epoch Time (Training + Test) = 9.64 seconds\n",
      "epoch: 36 average loss: 1.044\n",
      "Test Accuracy : 68.6%, Test Loss: 0.9775190856307745\n",
      "Epoch Time (Training + Test) = 9.83 seconds\n",
      "epoch: 37 average loss: 1.049\n",
      "Test Accuracy : 67.5%, Test Loss: 0.9979144651442766\n",
      "Epoch Time (Training + Test) = 9.69 seconds\n",
      "epoch: 38 average loss: 1.037\n",
      "Test Accuracy : 70.7%, Test Loss: 0.9647934678941965\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.70 seconds\n",
      "epoch: 39 average loss: 1.042\n",
      "Test Accuracy : 67.8%, Test Loss: 0.9987519029527903\n",
      "Epoch Time (Training + Test) = 9.85 seconds\n",
      "epoch: 40 average loss: 1.034\n",
      "Test Accuracy : 67.8%, Test Loss: 0.9930017273873091\n",
      "Epoch Time (Training + Test) = 9.76 seconds\n",
      "epoch: 41 average loss: 1.036\n",
      "Test Accuracy : 68.2%, Test Loss: 0.9834393467754126\n",
      "Epoch Time (Training + Test) = 9.67 seconds\n",
      "epoch: 42 average loss: 1.045\n",
      "Test Accuracy : 68.2%, Test Loss: 1.0006861183792353\n",
      "Epoch Time (Training + Test) = 9.78 seconds\n",
      "epoch: 43 average loss: 1.024\n",
      "Test Accuracy : 68.2%, Test Loss: 0.997723039239645\n",
      "Epoch Time (Training + Test) = 9.70 seconds\n",
      "epoch: 44 average loss: 1.023\n",
      "Test Accuracy : 68.0%, Test Loss: 1.0017409902065992\n",
      "Epoch Time (Training + Test) = 9.72 seconds\n",
      "epoch: 45 average loss: 1.037\n",
      "Test Accuracy : 67.8%, Test Loss: 0.9958150386810303\n",
      "Epoch Time (Training + Test) = 9.73 seconds\n",
      "epoch: 46 average loss: 1.026\n",
      "Test Accuracy : 67.5%, Test Loss: 0.9973597098141909\n",
      "Epoch Time (Training + Test) = 9.60 seconds\n",
      "epoch: 47 average loss: 1.032\n",
      "Test Accuracy : 68.9%, Test Loss: 0.9705831538885832\n",
      "Epoch Time (Training + Test) = 9.73 seconds\n",
      "epoch: 48 average loss: 1.028\n",
      "Test Accuracy : 68.7%, Test Loss: 0.9898782838135958\n",
      "Epoch Time (Training + Test) = 9.67 seconds\n",
      "epoch: 49 average loss: 1.012\n",
      "Test Accuracy : 69.8%, Test Loss: 0.9621498826891184\n",
      "Epoch Time (Training + Test) = 9.76 seconds\n",
      "epoch: 50 average loss: 1.022\n",
      "Test Accuracy : 69.0%, Test Loss: 0.978806821629405\n",
      "Epoch Time (Training + Test) = 9.73 seconds\n",
      "epoch: 51 average loss: 1.019\n",
      "Test Accuracy : 68.5%, Test Loss: 0.9803298097103834\n",
      "Epoch Time (Training + Test) = 9.69 seconds\n",
      "epoch: 52 average loss: 1.017\n",
      "Test Accuracy : 69.4%, Test Loss: 0.9654541332274675\n",
      "Epoch Time (Training + Test) = 9.72 seconds\n",
      "epoch: 53 average loss: 1.020\n",
      "Test Accuracy : 67.8%, Test Loss: 0.9544002842158079\n",
      "Epoch Time (Training + Test) = 9.93 seconds\n",
      "epoch: 54 average loss: 1.018\n",
      "Test Accuracy : 68.9%, Test Loss: 0.9603347461670637\n",
      "Epoch Time (Training + Test) = 9.74 seconds\n",
      "epoch: 55 average loss: 1.010\n",
      "Test Accuracy : 68.4%, Test Loss: 0.9780327919870615\n",
      "Epoch Time (Training + Test) = 9.79 seconds\n",
      "epoch: 56 average loss: 1.026\n",
      "Test Accuracy : 69.5%, Test Loss: 0.9537019766867161\n",
      "Epoch Time (Training + Test) = 9.81 seconds\n",
      "epoch: 57 average loss: 0.993\n",
      "Test Accuracy : 69.5%, Test Loss: 0.9804510865360498\n",
      "Epoch Time (Training + Test) = 9.71 seconds\n",
      "epoch: 58 average loss: 1.006\n",
      "Test Accuracy : 68.6%, Test Loss: 0.9541684333235025\n",
      "Epoch Time (Training + Test) = 9.74 seconds\n",
      "epoch: 59 average loss: 1.017\n",
      "Test Accuracy : 68.2%, Test Loss: 0.9796927832067013\n",
      "Epoch Time (Training + Test) = 9.78 seconds\n",
      "epoch: 60 average loss: 1.002\n",
      "Test Accuracy : 69.5%, Test Loss: 0.9675032962113619\n",
      "Epoch Time (Training + Test) = 9.65 seconds\n",
      "epoch: 61 average loss: 1.008\n",
      "Test Accuracy : 70.1%, Test Loss: 0.9682725891470909\n",
      "Epoch Time (Training + Test) = 9.69 seconds\n",
      "epoch: 62 average loss: 1.023\n",
      "Test Accuracy : 69.2%, Test Loss: 0.9817603658884764\n",
      "Epoch Time (Training + Test) = 9.72 seconds\n",
      "epoch: 63 average loss: 1.007\n",
      "Test Accuracy : 68.3%, Test Loss: 0.9870062079280615\n",
      "Epoch Time (Training + Test) = 9.60 seconds\n",
      "epoch: 64 average loss: 1.014\n",
      "Test Accuracy : 69.8%, Test Loss: 0.9674387499690056\n",
      "Epoch Time (Training + Test) = 9.66 seconds\n",
      "epoch: 65 average loss: 1.012\n",
      "Test Accuracy : 69.4%, Test Loss: 0.9769576694816351\n",
      "Epoch Time (Training + Test) = 9.66 seconds\n",
      "epoch: 66 average loss: 1.006\n",
      "Test Accuracy : 67.5%, Test Loss: 0.982294101268053\n",
      "Epoch Time (Training + Test) = 9.81 seconds\n",
      "epoch: 67 average loss: 1.016\n",
      "Test Accuracy : 69.2%, Test Loss: 0.9685104805976152\n",
      "Epoch Time (Training + Test) = 9.73 seconds\n",
      "epoch: 68 average loss: 0.997\n",
      "Test Accuracy : 68.6%, Test Loss: 0.9682686403393745\n",
      "Epoch Time (Training + Test) = 9.66 seconds\n",
      "epoch: 69 average loss: 1.010\n",
      "Test Accuracy : 69.8%, Test Loss: 0.9609904624521732\n",
      "Epoch Time (Training + Test) = 9.73 seconds\n",
      "epoch: 70 average loss: 1.004\n",
      "Test Accuracy : 68.7%, Test Loss: 0.9721308778971434\n",
      "Epoch Time (Training + Test) = 9.62 seconds\n",
      "epoch: 71 average loss: 1.002\n",
      "Test Accuracy : 68.4%, Test Loss: 0.9819297268986702\n",
      "Epoch Time (Training + Test) = 9.65 seconds\n",
      "epoch: 72 average loss: 0.993\n",
      "Test Accuracy : 68.7%, Test Loss: 0.9637244064360857\n",
      "Epoch Time (Training + Test) = 9.82 seconds\n",
      "epoch: 73 average loss: 1.000\n",
      "Test Accuracy : 68.0%, Test Loss: 0.9668422844260931\n",
      "Epoch Time (Training + Test) = 9.61 seconds\n",
      "epoch: 74 average loss: 0.987\n",
      "Test Accuracy : 69.0%, Test Loss: 0.9888707213103771\n",
      "Epoch Time (Training + Test) = 9.69 seconds\n",
      "epoch: 75 average loss: 1.006\n",
      "Test Accuracy : 69.0%, Test Loss: 0.9564327634871006\n",
      "Epoch Time (Training + Test) = 9.73 seconds\n",
      "Data Saved to BNN_uniadapt_maxpooling_thinblock2.csv\n",
      "Finished Training: \n",
      "Total Time 0.410303 hours\n",
      " Average Time Per Epoch 19.69 seconds\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(BNN,model_name='BNN_uniadapt_maxpooling_thinblock2',project_name='UniAdapt_Cifar100_Finetuning',binarise=True,classes=train_20.classes)\n",
    "trainer.lr = 1e-3\n",
    "trainer.batch_size = 64\n",
    "trainer.epochs =75\n",
    "trainer.epoch_chkpts = []\n",
    "trainer.start_epoch = 0\n",
    "# params = nn.ModuleList([BNN.uniAdaptNet,BNN.head_bn,BNN.head]).parameters()\n",
    "trainer.set_scheduler(None)\n",
    "trainer.set_optimizer(torch.optim.Adam,lr = 1e-3)\n",
    "trainer.train(train20_DL,test20_DL)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "365b2034",
   "metadata": {},
   "source": [
    "# Uni Adapt FineTune Pruning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ef40f7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'SavedModels\\UniAdapt_Cifar100\\BNN_Cifar80_Backbone_2023-04-18 16-54-59\\BNN_Cifar80_Backbone_best_acc.pth'\n",
    "\n",
    "state = torch.load(path,map_location='cuda:0')\n",
    "BNN = BNN_Resnet_UniAdapt(80,freeze_pre_weights=True)\n",
    "BNN.layer3[-1].do_bntan = True\n",
    "BNN.load_state_dict(state,strict=False)\n",
    "BNN.freeze()\n",
    "\n",
    "BNN = BNN.to('cuda:0')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4b0a1eda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 320])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class placeholder(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return x\n",
    "\n",
    "BNN.encoder_net = placeholder()\n",
    "\n",
    "n1 = uniAdapt_Net([80,160,320],[80,160,320],block = thinBlock2)\n",
    "n1 = n1.cuda()\n",
    "n1.train()\n",
    "\n",
    "adapt_net = nn.Sequential(n1,nn.Flatten())\n",
    "\n",
    "adapt_net.to('cuda:0')\n",
    "\n",
    "y = [torch.rand((4,80,32,32)).cuda(),torch.rand((4,160,16,16)).cuda(),torch.rand((4,320,8,8)).cuda()]\n",
    "adapt_net(y).shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "782b4871",
   "metadata": {},
   "source": [
    "## Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "10813fe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BNN_Resnet_UniAdapt(\n",
      "  (conv1): BinarizeConv2d(3, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (bn1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "  (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(80, 160, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): BinarizeConv2d(80, 160, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(160, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): BinarizeConv2d(160, 320, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)\n",
      "  (bn2): BatchNorm1d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (bn3): BatchNorm1d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (logsoftmax): LogSoftmax(dim=-1)\n",
      "  (fc): BinarizeLinear(in_features=320, out_features=80, bias=True)\n",
      "  (adapter_dict): ModuleDict()\n",
      "  (head_bn): placeholder()\n",
      "  (head): Linear(in_features=640, out_features=20, bias=True)\n",
      "  (encoder_net): placeholder()\n",
      "  (uniAdaptNet): Sequential(\n",
      "    (0): uniAdapt_Net(\n",
      "      (layer1): thinBlock3(\n",
      "        (conv2): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=80, bias=False)\n",
      "        (bn2): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(80, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (layer2): thinBlock3(\n",
      "        (conv2): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=160, bias=False)\n",
      "        (bn2): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(160, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (layer3): thinBlock3(\n",
      "        (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=320, bias=False)\n",
      "        (bn2): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (pool1): AdaptiveAvgPool2d(output_size=16)\n",
      "      (pool2): AdaptiveAvgPool2d(output_size=8)\n",
      "      (pool3): AdaptiveAvgPool2d(output_size=1)\n",
      "    )\n",
      "    (1): Flatten(start_dim=1, end_dim=-1)\n",
      "  )\n",
      ")\n",
      "No lr scheduler\n",
      "Optimizer Set to New: {'state': {}, 'param_groups': [{'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': False, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]}]}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:ka04fmfr) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">copper-capybara-2</strong>: <a href=\"https://wandb.ai/johnny_suu/Cifar100_UniAdapt_Pruning_Test/runs/ka04fmfr\" target=\"_blank\">https://wandb.ai/johnny_suu/Cifar100_UniAdapt_Pruning_Test/runs/ka04fmfr</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230509_124419-ka04fmfr\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:ka04fmfr). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6593a0b58231405a81052c779eaa92bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\John Su\\Downloads\\SydneyUni\\2022\\thesis\\Thesis\\wandb\\run-20230509_124439-2ot3x1j5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/johnny_suu/Cifar100_UniAdapt_Pruning_Test/runs/2ot3x1j5\" target=\"_blank\">driven-wildflower-3</a></strong> to <a href=\"https://wandb.ai/johnny_suu/Cifar100_UniAdapt_Pruning_Test\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "Project Name: Cifar100_UniAdapt_Pruning_Test, Run Name Pruned_0.00% \n",
      "\n",
      "\n",
      "Run Start : 2023-05-09 12-44-39\n",
      "start_epoch : 0\n",
      "initial_lr : 0.001\n",
      "batch_size : 64\n",
      "epochs : 75\n",
      "epoch_chkpts : []\n",
      "device : cuda:0\n",
      "criterion : CrossEntropyLoss()\n",
      "optimizer : Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: False\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "seed : None\n",
      "model_architecture : <class 'NN_Thesis.nn_classes.BNN_Resnet_UniAdapt'>\n",
      "binerised_training : True\n",
      "Number of Elements : 4547620\n",
      "Initial accuracy:\n",
      "Test Accuracy : 5.1%, Test Loss: 3.0587667826634304\n",
      "best_acc.pth saved!\n",
      "Test Accuracy : 4.8%, Test Loss: 3.0596694499254227\n",
      "epoch: 1 average loss: 2.349\n",
      "Test Accuracy : 37.4%, Test Loss: 2.053634714335203\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 17.46 seconds\n",
      "epoch: 2 average loss: 2.047\n",
      "Test Accuracy : 41.1%, Test Loss: 1.9023884497582912\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 17.64 seconds\n",
      "epoch: 3 average loss: 1.916\n",
      "Test Accuracy : 40.6%, Test Loss: 1.9286342300474644\n",
      "Epoch Time (Training + Test) = 17.97 seconds\n",
      "epoch: 4 average loss: 1.821\n",
      "Test Accuracy : 45.2%, Test Loss: 1.7441800460219383\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 17.81 seconds\n",
      "epoch: 5 average loss: 1.746\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\John Su\\Downloads\\SydneyUni\\2022\\thesis\\Thesis\\Untitled.ipynb Cell 65\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/John%20Su/Downloads/SydneyUni/2022/thesis/Thesis/Untitled.ipynb#Y120sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m trainer\u001b[39m.\u001b[39mset_scheduler(\u001b[39mNone\u001b[39;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/John%20Su/Downloads/SydneyUni/2022/thesis/Thesis/Untitled.ipynb#Y120sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m trainer\u001b[39m.\u001b[39mset_optimizer(torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdam,params,lr \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39mlr)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/John%20Su/Downloads/SydneyUni/2022/thesis/Thesis/Untitled.ipynb#Y120sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain(train20_DL,test20_DL,prune_strat\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m)\n",
      "File \u001b[1;32mc:\\Users\\John Su\\Downloads\\SydneyUni\\2022\\thesis\\Thesis\\NN_Thesis\\trainer.py:175\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, trainloader, testloader, prune_strat, **wandb_kwargs)\u001b[0m\n\u001b[0;32m    172\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepoch_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mperf_counter()\n\u001b[0;32m    174\u001b[0m \u001b[39m#Train and Test Results\u001b[39;00m\n\u001b[1;32m--> 175\u001b[0m train_loss,(test_loss,test_acc)\u001b[39m=\u001b[39m  (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_epoch(epoch,trainloader),\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtest(testloader))\n\u001b[0;32m    176\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscheduler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    177\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscheduler\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\John Su\\Downloads\\SydneyUni\\2022\\thesis\\Thesis\\NN_Thesis\\trainer.py:322\u001b[0m, in \u001b[0;36mTrainer.test\u001b[1;34m(self, testloader, valid)\u001b[0m\n\u001b[0;32m    320\u001b[0m test_loss \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m    321\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m--> 322\u001b[0m     \u001b[39mfor\u001b[39;00m data \u001b[39min\u001b[39;00m (testloader):\n\u001b[0;32m    323\u001b[0m         images, labels \u001b[39m=\u001b[39m data[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice),data[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m    324\u001b[0m         outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel(images)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\utils\\data\\dataloader.py:435\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    433\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterator\n\u001b[0;32m    434\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 435\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_iterator()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\utils\\data\\dataloader.py:381\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    379\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    380\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_worker_number_rationality()\n\u001b[1;32m--> 381\u001b[0m     \u001b[39mreturn\u001b[39;00m _MultiProcessingDataLoaderIter(\u001b[39mself\u001b[39;49m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\utils\\data\\dataloader.py:1034\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[1;34m(self, loader)\u001b[0m\n\u001b[0;32m   1027\u001b[0m w\u001b[39m.\u001b[39mdaemon \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m   1028\u001b[0m \u001b[39m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[0;32m   1029\u001b[0m \u001b[39m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[0;32m   1030\u001b[0m \u001b[39m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[0;32m   1031\u001b[0m \u001b[39m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[0;32m   1032\u001b[0m \u001b[39m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[0;32m   1033\u001b[0m \u001b[39m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[1;32m-> 1034\u001b[0m w\u001b[39m.\u001b[39;49mstart()\n\u001b[0;32m   1035\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_index_queues\u001b[39m.\u001b[39mappend(index_queue)\n\u001b[0;32m   1036\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_workers\u001b[39m.\u001b[39mappend(w)\n",
      "File \u001b[1;32mc:\\Program Files\\Python39\\lib\\multiprocessing\\process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m _current_process\u001b[39m.\u001b[39m_config\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mdaemon\u001b[39m\u001b[39m'\u001b[39m), \\\n\u001b[0;32m    119\u001b[0m        \u001b[39m'\u001b[39m\u001b[39mdaemonic processes are not allowed to have children\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    120\u001b[0m _cleanup()\n\u001b[1;32m--> 121\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_popen \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_Popen(\u001b[39mself\u001b[39;49m)\n\u001b[0;32m    122\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sentinel \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_popen\u001b[39m.\u001b[39msentinel\n\u001b[0;32m    123\u001b[0m \u001b[39m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[39m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Program Files\\Python39\\lib\\multiprocessing\\context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[0;32m    223\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_Popen\u001b[39m(process_obj):\n\u001b[1;32m--> 224\u001b[0m     \u001b[39mreturn\u001b[39;00m _default_context\u001b[39m.\u001b[39;49mget_context()\u001b[39m.\u001b[39;49mProcess\u001b[39m.\u001b[39;49m_Popen(process_obj)\n",
      "File \u001b[1;32mc:\\Program Files\\Python39\\lib\\multiprocessing\\context.py:327\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    324\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[0;32m    325\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_Popen\u001b[39m(process_obj):\n\u001b[0;32m    326\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mpopen_spawn_win32\u001b[39;00m \u001b[39mimport\u001b[39;00m Popen\n\u001b[1;32m--> 327\u001b[0m     \u001b[39mreturn\u001b[39;00m Popen(process_obj)\n",
      "File \u001b[1;32mc:\\Program Files\\Python39\\lib\\multiprocessing\\popen_spawn_win32.py:95\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, process_obj)\u001b[0m\n\u001b[0;32m     93\u001b[0m     reduction\u001b[39m.\u001b[39mdump(process_obj, to_child)\n\u001b[0;32m     94\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m---> 95\u001b[0m     set_spawning_popen(\u001b[39mNone\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "conv2_groups = [1,2,5,10,20,None]\n",
    "conv1_groups = [None]*len(conv2_groups)\n",
    "\n",
    "for prune_amount in [0]:\n",
    "    \n",
    "    BNN = BNN_Resnet_UniAdapt(80,freeze_pre_weights=False)\n",
    "    BNN.layer3[-1].do_bntan = True\n",
    "    BNN.load_state_dict(state,strict=True)\n",
    "    BNN.freeze()\n",
    "\n",
    "    BNN = BNN.to('cuda:0')\n",
    "    BNN.encoder_net = placeholder()\n",
    "\n",
    "    n1 = uniAdapt_Net([80,160,320],[80,160,320],block = thinBlock3)\n",
    "    n1 = n1.cuda()\n",
    "    n1.train()\n",
    "\n",
    "    adapt_net = nn.Sequential(n1,nn.Flatten())\n",
    "    \n",
    "    adapt_net.to('cuda:0')\n",
    "\n",
    "    BNN.head_bn =  placeholder()\n",
    "    BNN.head =  nn.Linear(320 + 320,20)\n",
    "    BNN.add_UniAdapter(adapt_net)\n",
    "    BNN = BNN.to('cuda:0')\n",
    "    BNN.uniAdapt = True\n",
    "    # x = torch.rand((4,3,32,32)).to('cuda:0')\n",
    "    # print(BNN(x).shape)\n",
    "    # break\n",
    "    trainer = Trainer(BNN,model_name=f'Pruned_{prune_amount*100 :.2f}%',project_name='Cifar100_UniAdapt_Pruning_Test',binarise=True,classes=train_20.classes)\n",
    "    trainer.lr = 1e-3\n",
    "    trainer.batch_size = 64\n",
    "    trainer.epochs =75\n",
    "    trainer.epoch_chkpts = []\n",
    "    trainer.start_epoch = 0\n",
    "    trainer.amount = [prune_amount,prune_amount]\n",
    "    # trainer.pruning_rounds = 1 # Only need this for iterative pruning\n",
    "    trainer.model_to_prune = trainer.model.uniAdaptNet[0]\n",
    "    # params = nn.ModuleList([BNN.uniAdaptNet,BNN.head_bn,BNN.head]).parameters()\n",
    "    # params = nn.ModuleList([BNN.head_bn,BNN.head]).parameters()\n",
    "    print(trainer.model)\n",
    "    trainer.set_scheduler(None)\n",
    "    trainer.set_optimizer(torch.optim.Adam,lr = trainer.lr)\n",
    "    trainer.train(train20_DL,test20_DL,prune_strat=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "27c2194b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BNN_Resnet_UniAdapt(\n",
      "  (conv1): BinarizeConv2d(3, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (bn1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "  (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(80, 160, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): BinarizeConv2d(80, 160, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(160, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): BinarizeConv2d(160, 320, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)\n",
      "  (bn2): BatchNorm1d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (bn3): BatchNorm1d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (logsoftmax): LogSoftmax(dim=-1)\n",
      "  (fc): BinarizeLinear(in_features=320, out_features=80, bias=True)\n",
      "  (adapter_dict): ModuleDict()\n",
      "  (head_bn): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (head): BinarizeLinear(in_features=640, out_features=20, bias=True)\n",
      "  (encoder_net): placeholder()\n",
      "  (uniAdaptNet): Sequential(\n",
      "    (0): uniAdapt_Net(\n",
      "      (layer1): thinBlock3(\n",
      "        (conv2): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=80, bias=False)\n",
      "        (bn2): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(80, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (layer2): thinBlock3(\n",
      "        (conv2): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=160, bias=False)\n",
      "        (bn2): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(160, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (layer3): thinBlock3(\n",
      "        (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=320, bias=False)\n",
      "        (bn2): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (pool1): AdaptiveAvgPool2d(output_size=16)\n",
      "      (pool2): AdaptiveAvgPool2d(output_size=8)\n",
      "      (pool3): AdaptiveAvgPool2d(output_size=1)\n",
      "    )\n",
      "    (1): Flatten(start_dim=1, end_dim=-1)\n",
      "  )\n",
      ")\n",
      "No lr scheduler\n",
      "Optimizer Set to New: {'state': {}, 'param_groups': [{'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': False, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]}]}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:x010dydt) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Current Best Acc</td><td>▁▅▆▆▆▇▇▇▇▇▇▇▇███████████████████████████</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>epoch time (s)</td><td>▁▇▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆█▆▅▆▆▆▆▆▆▆▆▅▅▆▆▆▆▆▆▆▆▆</td></tr><tr><td>lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_accuracy</td><td>▁▅▆▆▆▇▇▇▇▇▇▇▇███████▃▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▅▄▄▅</td></tr><tr><td>test_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>training_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Best Accuracy</td><td>0.632</td></tr><tr><td>Current Best Acc</td><td>0.632</td></tr><tr><td>Total Time (hours)</td><td>0.43181</td></tr><tr><td>epoch</td><td>75</td></tr><tr><td>epoch time (s)</td><td>10.27937</td></tr><tr><td>lr</td><td>0.001</td></tr><tr><td>test_accuracy</td><td>0.3425</td></tr><tr><td>test_loss</td><td>2.18579</td></tr><tr><td>training_loss</td><td>2.1943</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">imperial-podracer-6</strong>: <a href=\"https://wandb.ai/johnny_suu/Cifar100_UniAdapt_Pruning/runs/x010dydt\" target=\"_blank\">https://wandb.ai/johnny_suu/Cifar100_UniAdapt_Pruning/runs/x010dydt</a><br/>Synced 5 W&B file(s), 1 media file(s), 1 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230505_100728-x010dydt\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:x010dydt). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef877c9429944c0488f9b20e822e2293",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\John Su\\Downloads\\SydneyUni\\2022\\thesis\\Thesis\\wandb\\run-20230505_102048-umrc8sks</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/johnny_suu/Cifar100_UniAdapt_Pruning/runs/umrc8sks\" target=\"_blank\">tusken-federation-7</a></strong> to <a href=\"https://wandb.ai/johnny_suu/Cifar100_UniAdapt_Pruning\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "Project Name: Cifar100_UniAdapt_Pruning, Run Name Pruned_Conv2_80%_Conv3_0.00% \n",
      "\n",
      "\n",
      "Run Start : 2023-05-05 10-20-48\n",
      "start_epoch : 0\n",
      "initial_lr : 0.001\n",
      "batch_size : 64\n",
      "epochs : 75\n",
      "epoch_chkpts : []\n",
      "device : cuda:0\n",
      "criterion : CrossEntropyLoss()\n",
      "optimizer : Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: False\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "seed : None\n",
      "model_architecture : <class 'NN_Thesis.nn_classes.BNN_Resnet_UniAdapt'>\n",
      "binerised_training : True\n",
      "Number of Elements : 4547660\n",
      "Initial accuracy:\n",
      "Test Accuracy : 5.6%, Test Loss: 39.30828451654714\n",
      "best_acc.pth saved!\n",
      "Test Accuracy : 5.7%, Test Loss: 39.02663779258728\n",
      "best_acc.pth saved!\n",
      "epoch: 1 average loss: 2.453\n",
      "Test Accuracy : 34.8%, Test Loss: 2.187877595424652\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 10.38 seconds\n",
      "epoch: 2 average loss: 2.156\n",
      "Test Accuracy : 40.2%, Test Loss: 2.0325478836894035\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 10.33 seconds\n",
      "epoch: 3 average loss: 2.048\n",
      "Test Accuracy : 42.5%, Test Loss: 1.9232063964009285\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 10.25 seconds\n",
      "epoch: 4 average loss: 1.953\n",
      "Test Accuracy : 44.0%, Test Loss: 1.8731957450509071\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 10.35 seconds\n",
      "epoch: 5 average loss: 1.874\n",
      "Test Accuracy : 47.5%, Test Loss: 1.7833321541547775\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 10.32 seconds\n",
      "epoch: 6 average loss: 1.822\n",
      "Test Accuracy : 48.4%, Test Loss: 1.7291630953550339\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 10.31 seconds\n",
      "epoch: 7 average loss: 1.770\n",
      "Test Accuracy : 49.5%, Test Loss: 1.696056667715311\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 10.37 seconds\n",
      "epoch: 8 average loss: 1.706\n",
      "Test Accuracy : 49.6%, Test Loss: 1.653224177658558\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 10.35 seconds\n",
      "epoch: 9 average loss: 1.683\n",
      "Test Accuracy : 51.3%, Test Loss: 1.621021930128336\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 10.27 seconds\n",
      "epoch: 10 average loss: 1.638\n",
      "Test Accuracy : 53.0%, Test Loss: 1.5686154700815678\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 10.38 seconds\n",
      "epoch: 11 average loss: 1.610\n",
      "Test Accuracy : 50.7%, Test Loss: 1.6196999698877335\n",
      "Epoch Time (Training + Test) = 10.24 seconds\n",
      "epoch: 12 average loss: 1.582\n",
      "Test Accuracy : 54.2%, Test Loss: 1.5192318148911\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 10.33 seconds\n",
      "epoch: 13 average loss: 1.551\n",
      "Test Accuracy : 52.1%, Test Loss: 1.5281575433909893\n",
      "Epoch Time (Training + Test) = 10.27 seconds\n",
      "epoch: 14 average loss: 1.506\n",
      "Test Accuracy : 53.9%, Test Loss: 1.497482243925333\n",
      "Epoch Time (Training + Test) = 10.27 seconds\n",
      "epoch: 15 average loss: 1.490\n",
      "Test Accuracy : 55.2%, Test Loss: 1.4499838203191757\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 10.29 seconds\n",
      "epoch: 16 average loss: 1.465\n",
      "Test Accuracy : 55.2%, Test Loss: 1.4852459393441677\n",
      "Epoch Time (Training + Test) = 10.32 seconds\n",
      "epoch: 17 average loss: 1.453\n",
      "Test Accuracy : 56.0%, Test Loss: 1.420422736555338\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 10.29 seconds\n",
      "epoch: 18 average loss: 1.416\n",
      "Test Accuracy : 54.9%, Test Loss: 1.4236012697219849\n",
      "Epoch Time (Training + Test) = 10.33 seconds\n",
      "epoch: 19 average loss: 1.383\n",
      "Test Accuracy : 58.1%, Test Loss: 1.3680373430252075\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 10.30 seconds\n",
      "epoch: 20 average loss: 1.365\n",
      "Test Accuracy : 57.0%, Test Loss: 1.394825790077448\n",
      "Epoch Time (Training + Test) = 10.35 seconds\n",
      "epoch: 21 average loss: 1.346\n",
      "Test Accuracy : 57.5%, Test Loss: 1.3560152910649776\n",
      "Epoch Time (Training + Test) = 9.84 seconds\n",
      "epoch: 22 average loss: 1.341\n",
      "Test Accuracy : 57.5%, Test Loss: 1.3691158071160316\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 23 average loss: 1.326\n",
      "Test Accuracy : 57.9%, Test Loss: 1.348869115114212\n",
      "Epoch Time (Training + Test) = 9.85 seconds\n",
      "epoch: 24 average loss: 1.322\n",
      "Test Accuracy : 58.7%, Test Loss: 1.3372344821691513\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.89 seconds\n",
      "epoch: 25 average loss: 1.277\n",
      "Test Accuracy : 58.6%, Test Loss: 1.3067161105573177\n",
      "Epoch Time (Training + Test) = 9.81 seconds\n",
      "epoch: 26 average loss: 1.273\n",
      "Test Accuracy : 60.2%, Test Loss: 1.301292259246111\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.88 seconds\n",
      "epoch: 27 average loss: 1.262\n",
      "Test Accuracy : 58.1%, Test Loss: 1.3543543554842472\n",
      "Epoch Time (Training + Test) = 9.83 seconds\n",
      "epoch: 28 average loss: 1.245\n",
      "Test Accuracy : 58.9%, Test Loss: 1.2992233224213123\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 29 average loss: 1.241\n",
      "Test Accuracy : 60.2%, Test Loss: 1.2714168764650822\n",
      "Epoch Time (Training + Test) = 9.84 seconds\n",
      "epoch: 30 average loss: 1.211\n",
      "Test Accuracy : 61.3%, Test Loss: 1.2433273568749428\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.87 seconds\n",
      "epoch: 31 average loss: 1.208\n",
      "Test Accuracy : 60.8%, Test Loss: 1.259447792544961\n",
      "Epoch Time (Training + Test) = 9.83 seconds\n",
      "epoch: 32 average loss: 1.201\n",
      "Test Accuracy : 61.9%, Test Loss: 1.2406540252268314\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 33 average loss: 1.185\n",
      "Test Accuracy : 60.9%, Test Loss: 1.2349296547472477\n",
      "Epoch Time (Training + Test) = 9.83 seconds\n",
      "epoch: 34 average loss: 1.160\n",
      "Test Accuracy : 61.8%, Test Loss: 1.2506116069853306\n",
      "Epoch Time (Training + Test) = 9.87 seconds\n",
      "epoch: 35 average loss: 1.172\n",
      "Test Accuracy : 61.9%, Test Loss: 1.2619570270180702\n",
      "Epoch Time (Training + Test) = 9.82 seconds\n",
      "epoch: 36 average loss: 1.153\n",
      "Test Accuracy : 61.2%, Test Loss: 1.2268431391566992\n",
      "Epoch Time (Training + Test) = 9.90 seconds\n",
      "epoch: 37 average loss: 1.140\n",
      "Test Accuracy : 62.2%, Test Loss: 1.219548650085926\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.88 seconds\n",
      "conv2\n",
      "conv3\n",
      "conv2\n",
      "conv3\n",
      "conv2\n",
      "conv3\n",
      "Pruned [0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0]% of weights at epoch 37\n",
      "epoch: 38 average loss: 2.111\n",
      "Test Accuracy : 45.6%, Test Loss: 1.8151881881058216\n",
      "Epoch Time (Training + Test) = 9.80 seconds\n",
      "epoch: 39 average loss: 1.771\n",
      "Test Accuracy : 49.8%, Test Loss: 1.6549504734575748\n",
      "Epoch Time (Training + Test) = 9.85 seconds\n",
      "epoch: 40 average loss: 1.672\n",
      "Test Accuracy : 51.4%, Test Loss: 1.6250902265310287\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 41 average loss: 1.623\n",
      "Test Accuracy : 51.2%, Test Loss: 1.5788367688655853\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 42 average loss: 1.575\n",
      "Test Accuracy : 51.8%, Test Loss: 1.5407557152211666\n",
      "Epoch Time (Training + Test) = 9.88 seconds\n",
      "epoch: 43 average loss: 1.565\n",
      "Test Accuracy : 52.6%, Test Loss: 1.5404315292835236\n",
      "Epoch Time (Training + Test) = 9.84 seconds\n",
      "epoch: 44 average loss: 1.525\n",
      "Test Accuracy : 54.5%, Test Loss: 1.4987040869891644\n",
      "Epoch Time (Training + Test) = 9.85 seconds\n",
      "epoch: 45 average loss: 1.521\n",
      "Test Accuracy : 53.3%, Test Loss: 1.5131266005337238\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 46 average loss: 1.491\n",
      "Test Accuracy : 54.8%, Test Loss: 1.475453533232212\n",
      "Epoch Time (Training + Test) = 9.83 seconds\n",
      "epoch: 47 average loss: 1.488\n",
      "Test Accuracy : 52.6%, Test Loss: 1.498262021690607\n",
      "Epoch Time (Training + Test) = 9.87 seconds\n",
      "epoch: 48 average loss: 1.474\n",
      "Test Accuracy : 53.3%, Test Loss: 1.5007349178195\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 49 average loss: 1.462\n",
      "Test Accuracy : 53.4%, Test Loss: 1.4779638946056366\n",
      "Epoch Time (Training + Test) = 9.77 seconds\n",
      "epoch: 50 average loss: 1.453\n",
      "Test Accuracy : 53.8%, Test Loss: 1.470652248710394\n",
      "Epoch Time (Training + Test) = 9.80 seconds\n",
      "epoch: 51 average loss: 1.443\n",
      "Test Accuracy : 55.5%, Test Loss: 1.4395116046071053\n",
      "Epoch Time (Training + Test) = 9.84 seconds\n",
      "epoch: 52 average loss: 1.432\n",
      "Test Accuracy : 54.9%, Test Loss: 1.4449388794600964\n",
      "Epoch Time (Training + Test) = 9.83 seconds\n",
      "epoch: 53 average loss: 1.415\n",
      "Test Accuracy : 56.6%, Test Loss: 1.4149242527782917\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 54 average loss: 1.416\n",
      "Test Accuracy : 54.4%, Test Loss: 1.4434314258396626\n",
      "Epoch Time (Training + Test) = 9.83 seconds\n",
      "epoch: 55 average loss: 1.406\n",
      "Test Accuracy : 56.1%, Test Loss: 1.4201197437942028\n",
      "Epoch Time (Training + Test) = 9.89 seconds\n",
      "epoch: 56 average loss: 1.402\n",
      "Test Accuracy : 55.2%, Test Loss: 1.440923485904932\n",
      "Epoch Time (Training + Test) = 9.83 seconds\n",
      "epoch: 57 average loss: 1.387\n",
      "Test Accuracy : 55.4%, Test Loss: 1.427761398255825\n",
      "Epoch Time (Training + Test) = 9.84 seconds\n",
      "epoch: 58 average loss: 1.370\n",
      "Test Accuracy : 56.5%, Test Loss: 1.4032545052468777\n",
      "Epoch Time (Training + Test) = 9.85 seconds\n",
      "epoch: 59 average loss: 1.382\n",
      "Test Accuracy : 56.8%, Test Loss: 1.3952349871397018\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 60 average loss: 1.380\n",
      "Test Accuracy : 55.5%, Test Loss: 1.4455900974571705\n",
      "Epoch Time (Training + Test) = 9.83 seconds\n",
      "epoch: 61 average loss: 1.364\n",
      "Test Accuracy : 56.0%, Test Loss: 1.412272047251463\n",
      "Epoch Time (Training + Test) = 9.78 seconds\n",
      "epoch: 62 average loss: 1.374\n",
      "Test Accuracy : 56.0%, Test Loss: 1.4350422769784927\n",
      "Epoch Time (Training + Test) = 9.79 seconds\n",
      "epoch: 63 average loss: 1.350\n",
      "Test Accuracy : 56.7%, Test Loss: 1.3908956125378609\n",
      "Epoch Time (Training + Test) = 9.84 seconds\n",
      "epoch: 64 average loss: 1.357\n",
      "Test Accuracy : 55.4%, Test Loss: 1.388886857777834\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 65 average loss: 1.360\n",
      "Test Accuracy : 56.6%, Test Loss: 1.3716372922062874\n",
      "Epoch Time (Training + Test) = 9.82 seconds\n",
      "epoch: 66 average loss: 1.346\n",
      "Test Accuracy : 57.1%, Test Loss: 1.377413872629404\n",
      "Epoch Time (Training + Test) = 9.85 seconds\n",
      "epoch: 67 average loss: 1.348\n",
      "Test Accuracy : 57.2%, Test Loss: 1.3814251273870468\n",
      "Epoch Time (Training + Test) = 9.84 seconds\n",
      "epoch: 68 average loss: 1.350\n",
      "Test Accuracy : 56.6%, Test Loss: 1.3702164441347122\n",
      "Epoch Time (Training + Test) = 9.85 seconds\n",
      "epoch: 69 average loss: 1.316\n",
      "Test Accuracy : 57.6%, Test Loss: 1.3657384850084782\n",
      "Epoch Time (Training + Test) = 9.87 seconds\n",
      "epoch: 70 average loss: 1.324\n",
      "Test Accuracy : 56.8%, Test Loss: 1.367081854492426\n",
      "Epoch Time (Training + Test) = 9.83 seconds\n",
      "epoch: 71 average loss: 1.313\n",
      "Test Accuracy : 58.3%, Test Loss: 1.3736185804009438\n",
      "Epoch Time (Training + Test) = 9.87 seconds\n",
      "epoch: 72 average loss: 1.322\n",
      "Test Accuracy : 57.6%, Test Loss: 1.3401034474372864\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 73 average loss: 1.321\n",
      "Test Accuracy : 56.5%, Test Loss: 1.3644994609057903\n",
      "Epoch Time (Training + Test) = 9.76 seconds\n",
      "epoch: 74 average loss: 1.305\n",
      "Test Accuracy : 58.6%, Test Loss: 1.3419520668685436\n",
      "Epoch Time (Training + Test) = 9.85 seconds\n",
      "epoch: 75 average loss: 1.302\n",
      "Test Accuracy : 58.7%, Test Loss: 1.3507236912846565\n",
      "Epoch Time (Training + Test) = 9.78 seconds\n",
      "Data Saved to Pruned_Conv2_80%_Conv3_0.00%.csv\n",
      "Finished Training: \n",
      "Total Time 0.415357 hours\n",
      " Average Time Per Epoch 19.94 seconds\n",
      "BNN_Resnet_UniAdapt(\n",
      "  (conv1): BinarizeConv2d(3, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (bn1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "  (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(80, 160, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): BinarizeConv2d(80, 160, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(160, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): BinarizeConv2d(160, 320, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)\n",
      "  (bn2): BatchNorm1d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (bn3): BatchNorm1d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (logsoftmax): LogSoftmax(dim=-1)\n",
      "  (fc): BinarizeLinear(in_features=320, out_features=80, bias=True)\n",
      "  (adapter_dict): ModuleDict()\n",
      "  (head_bn): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (head): BinarizeLinear(in_features=640, out_features=20, bias=True)\n",
      "  (encoder_net): placeholder()\n",
      "  (uniAdaptNet): Sequential(\n",
      "    (0): uniAdapt_Net(\n",
      "      (layer1): thinBlock3(\n",
      "        (conv2): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=80, bias=False)\n",
      "        (bn2): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(80, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (layer2): thinBlock3(\n",
      "        (conv2): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=160, bias=False)\n",
      "        (bn2): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(160, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (layer3): thinBlock3(\n",
      "        (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=320, bias=False)\n",
      "        (bn2): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (pool1): AdaptiveAvgPool2d(output_size=16)\n",
      "      (pool2): AdaptiveAvgPool2d(output_size=8)\n",
      "      (pool3): AdaptiveAvgPool2d(output_size=1)\n",
      "    )\n",
      "    (1): Flatten(start_dim=1, end_dim=-1)\n",
      "  )\n",
      ")\n",
      "No lr scheduler\n",
      "Optimizer Set to New: {'state': {}, 'param_groups': [{'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': False, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]}]}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:umrc8sks) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Current Best Acc</td><td>▁▅▆▆▆▇▇▇▇▇▇▇▇███████████████████████████</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>epoch time (s)</td><td>▁█▇███▇████▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▅▆</td></tr><tr><td>lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_accuracy</td><td>▁▅▆▆▆▇▇▇▇▇█▇▇███████▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇█</td></tr><tr><td>test_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>training_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Best Accuracy</td><td>0.622</td></tr><tr><td>Current Best Acc</td><td>0.622</td></tr><tr><td>Total Time (hours)</td><td>0.41536</td></tr><tr><td>epoch</td><td>75</td></tr><tr><td>epoch time (s)</td><td>9.78376</td></tr><tr><td>lr</td><td>0.001</td></tr><tr><td>test_accuracy</td><td>0.587</td></tr><tr><td>test_loss</td><td>1.35072</td></tr><tr><td>training_loss</td><td>1.3022</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">tusken-federation-7</strong>: <a href=\"https://wandb.ai/johnny_suu/Cifar100_UniAdapt_Pruning/runs/umrc8sks\" target=\"_blank\">https://wandb.ai/johnny_suu/Cifar100_UniAdapt_Pruning/runs/umrc8sks</a><br/>Synced 5 W&B file(s), 1 media file(s), 1 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230505_102048-umrc8sks\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:umrc8sks). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1b95438e3774d1b918265e3a268f31f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016933333330477276, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\John Su\\Downloads\\SydneyUni\\2022\\thesis\\Thesis\\wandb\\run-20230505_103336-3paegwr9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/johnny_suu/Cifar100_UniAdapt_Pruning/runs/3paegwr9\" target=\"_blank\">mythical-tauntaun-8</a></strong> to <a href=\"https://wandb.ai/johnny_suu/Cifar100_UniAdapt_Pruning\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "Project Name: Cifar100_UniAdapt_Pruning, Run Name Pruned_Conv2_80%_Conv3_70.00% \n",
      "\n",
      "\n",
      "Run Start : 2023-05-05 10-33-36\n",
      "start_epoch : 0\n",
      "initial_lr : 0.001\n",
      "batch_size : 64\n",
      "epochs : 75\n",
      "epoch_chkpts : []\n",
      "device : cuda:0\n",
      "criterion : CrossEntropyLoss()\n",
      "optimizer : Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: False\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "seed : None\n",
      "model_architecture : <class 'NN_Thesis.nn_classes.BNN_Resnet_UniAdapt'>\n",
      "binerised_training : True\n",
      "Number of Elements : 4547660\n",
      "Initial accuracy:\n",
      "Test Accuracy : 5.2%, Test Loss: 33.32873066215758\n",
      "best_acc.pth saved!\n",
      "Test Accuracy : 5.9%, Test Loss: 33.35433220863342\n",
      "best_acc.pth saved!\n",
      "epoch: 1 average loss: 2.433\n",
      "Test Accuracy : 32.6%, Test Loss: 2.259440064430237\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.89 seconds\n",
      "epoch: 2 average loss: 2.144\n",
      "Test Accuracy : 40.1%, Test Loss: 2.0058488100767136\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 10.00 seconds\n",
      "epoch: 3 average loss: 2.029\n",
      "Test Accuracy : 43.4%, Test Loss: 1.9129376970231533\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.96 seconds\n",
      "epoch: 4 average loss: 1.941\n",
      "Test Accuracy : 45.8%, Test Loss: 1.8375518135726452\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.88 seconds\n",
      "epoch: 5 average loss: 1.863\n",
      "Test Accuracy : 46.7%, Test Loss: 1.7673532851040363\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.85 seconds\n",
      "epoch: 6 average loss: 1.816\n",
      "Test Accuracy : 48.8%, Test Loss: 1.7252704314887524\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.84 seconds\n",
      "epoch: 7 average loss: 1.757\n",
      "Test Accuracy : 48.2%, Test Loss: 1.6835295110940933\n",
      "Epoch Time (Training + Test) = 9.83 seconds\n",
      "epoch: 8 average loss: 1.730\n",
      "Test Accuracy : 49.2%, Test Loss: 1.6576274298131466\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.88 seconds\n",
      "epoch: 9 average loss: 1.674\n",
      "Test Accuracy : 52.1%, Test Loss: 1.6092863269150257\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.85 seconds\n",
      "epoch: 10 average loss: 1.649\n",
      "Test Accuracy : 50.7%, Test Loss: 1.5969282425940037\n",
      "Epoch Time (Training + Test) = 9.84 seconds\n",
      "epoch: 11 average loss: 1.600\n",
      "Test Accuracy : 51.4%, Test Loss: 1.5533568784594536\n",
      "Epoch Time (Training + Test) = 9.84 seconds\n",
      "epoch: 12 average loss: 1.572\n",
      "Test Accuracy : 54.0%, Test Loss: 1.5232998691499233\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.89 seconds\n",
      "epoch: 13 average loss: 1.543\n",
      "Test Accuracy : 54.2%, Test Loss: 1.535088635981083\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 14 average loss: 1.523\n",
      "Test Accuracy : 54.9%, Test Loss: 1.492266234010458\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.84 seconds\n",
      "epoch: 15 average loss: 1.499\n",
      "Test Accuracy : 54.9%, Test Loss: 1.4964816346764565\n",
      "Epoch Time (Training + Test) = 9.84 seconds\n",
      "epoch: 16 average loss: 1.474\n",
      "Test Accuracy : 56.0%, Test Loss: 1.4358160383999348\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.88 seconds\n",
      "epoch: 17 average loss: 1.450\n",
      "Test Accuracy : 54.1%, Test Loss: 1.4416652917861938\n",
      "Epoch Time (Training + Test) = 9.80 seconds\n",
      "epoch: 18 average loss: 1.427\n",
      "Test Accuracy : 56.4%, Test Loss: 1.4282918088138103\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.89 seconds\n",
      "epoch: 19 average loss: 1.404\n",
      "Test Accuracy : 56.9%, Test Loss: 1.3774157986044884\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.84 seconds\n",
      "epoch: 20 average loss: 1.374\n",
      "Test Accuracy : 56.1%, Test Loss: 1.4171384312212467\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 21 average loss: 1.367\n",
      "Test Accuracy : 58.2%, Test Loss: 1.3529470451176167\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.87 seconds\n",
      "epoch: 22 average loss: 1.353\n",
      "Test Accuracy : 56.5%, Test Loss: 1.3734812922775745\n",
      "Epoch Time (Training + Test) = 9.82 seconds\n",
      "epoch: 23 average loss: 1.341\n",
      "Test Accuracy : 58.4%, Test Loss: 1.3523228913545609\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 24 average loss: 1.307\n",
      "Test Accuracy : 59.8%, Test Loss: 1.3153836838901043\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.88 seconds\n",
      "epoch: 25 average loss: 1.308\n",
      "Test Accuracy : 59.0%, Test Loss: 1.309506081044674\n",
      "Epoch Time (Training + Test) = 9.77 seconds\n",
      "epoch: 26 average loss: 1.284\n",
      "Test Accuracy : 57.8%, Test Loss: 1.319172415882349\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 27 average loss: 1.274\n",
      "Test Accuracy : 58.5%, Test Loss: 1.310432557016611\n",
      "Epoch Time (Training + Test) = 9.85 seconds\n",
      "epoch: 28 average loss: 1.247\n",
      "Test Accuracy : 59.4%, Test Loss: 1.284116130322218\n",
      "Epoch Time (Training + Test) = 9.88 seconds\n",
      "epoch: 29 average loss: 1.234\n",
      "Test Accuracy : 60.3%, Test Loss: 1.2743294574320316\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.89 seconds\n",
      "epoch: 30 average loss: 1.223\n",
      "Test Accuracy : 60.1%, Test Loss: 1.2751002348959446\n",
      "Epoch Time (Training + Test) = 9.81 seconds\n",
      "epoch: 31 average loss: 1.207\n",
      "Test Accuracy : 61.1%, Test Loss: 1.242164507508278\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.93 seconds\n",
      "epoch: 32 average loss: 1.196\n",
      "Test Accuracy : 59.9%, Test Loss: 1.2771323956549168\n",
      "Epoch Time (Training + Test) = 9.81 seconds\n",
      "epoch: 33 average loss: 1.192\n",
      "Test Accuracy : 61.7%, Test Loss: 1.2159166671335697\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.87 seconds\n",
      "epoch: 34 average loss: 1.164\n",
      "Test Accuracy : 61.5%, Test Loss: 1.2427454218268394\n",
      "Epoch Time (Training + Test) = 9.83 seconds\n",
      "epoch: 35 average loss: 1.167\n",
      "Test Accuracy : 61.0%, Test Loss: 1.2279585488140583\n",
      "Epoch Time (Training + Test) = 9.76 seconds\n",
      "epoch: 36 average loss: 1.161\n",
      "Test Accuracy : 58.8%, Test Loss: 1.2846483998000622\n",
      "Epoch Time (Training + Test) = 9.84 seconds\n",
      "epoch: 37 average loss: 1.137\n",
      "Test Accuracy : 62.5%, Test Loss: 1.2257142178714275\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.95 seconds\n",
      "conv2\n",
      "conv3\n",
      "conv2\n",
      "conv3\n",
      "conv2\n",
      "conv3\n",
      "Pruned [0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7]% of weights at epoch 37\n",
      "epoch: 38 average loss: 2.299\n",
      "Test Accuracy : 40.7%, Test Loss: 1.9525699019432068\n",
      "Epoch Time (Training + Test) = 9.87 seconds\n",
      "epoch: 39 average loss: 1.905\n",
      "Test Accuracy : 45.8%, Test Loss: 1.756368424743414\n",
      "Epoch Time (Training + Test) = 9.84 seconds\n",
      "epoch: 40 average loss: 1.799\n",
      "Test Accuracy : 47.5%, Test Loss: 1.6998260915279388\n",
      "Epoch Time (Training + Test) = 9.85 seconds\n",
      "epoch: 41 average loss: 1.722\n",
      "Test Accuracy : 48.7%, Test Loss: 1.6427363231778145\n",
      "Epoch Time (Training + Test) = 9.76 seconds\n",
      "epoch: 42 average loss: 1.690\n",
      "Test Accuracy : 48.8%, Test Loss: 1.6673334278166294\n",
      "Epoch Time (Training + Test) = 9.84 seconds\n",
      "epoch: 43 average loss: 1.666\n",
      "Test Accuracy : 50.0%, Test Loss: 1.6172483824193478\n",
      "Epoch Time (Training + Test) = 9.80 seconds\n",
      "epoch: 44 average loss: 1.648\n",
      "Test Accuracy : 50.8%, Test Loss: 1.5719534792006016\n",
      "Epoch Time (Training + Test) = 9.84 seconds\n",
      "epoch: 45 average loss: 1.631\n",
      "Test Accuracy : 50.5%, Test Loss: 1.5759162940084934\n",
      "Epoch Time (Training + Test) = 9.87 seconds\n",
      "epoch: 46 average loss: 1.598\n",
      "Test Accuracy : 52.4%, Test Loss: 1.5252371653914452\n",
      "Epoch Time (Training + Test) = 9.81 seconds\n",
      "epoch: 47 average loss: 1.572\n",
      "Test Accuracy : 51.6%, Test Loss: 1.5488069765269756\n",
      "Epoch Time (Training + Test) = 9.77 seconds\n",
      "epoch: 48 average loss: 1.562\n",
      "Test Accuracy : 51.8%, Test Loss: 1.5525940917432308\n",
      "Epoch Time (Training + Test) = 9.79 seconds\n",
      "epoch: 49 average loss: 1.562\n",
      "Test Accuracy : 52.9%, Test Loss: 1.5075560919940472\n",
      "Epoch Time (Training + Test) = 9.72 seconds\n",
      "epoch: 50 average loss: 1.534\n",
      "Test Accuracy : 53.4%, Test Loss: 1.4995171912014484\n",
      "Epoch Time (Training + Test) = 9.82 seconds\n",
      "epoch: 51 average loss: 1.531\n",
      "Test Accuracy : 54.2%, Test Loss: 1.4730502292513847\n",
      "Epoch Time (Training + Test) = 9.78 seconds\n",
      "epoch: 52 average loss: 1.528\n",
      "Test Accuracy : 53.1%, Test Loss: 1.4918390177190304\n",
      "Epoch Time (Training + Test) = 9.75 seconds\n",
      "epoch: 53 average loss: 1.515\n",
      "Test Accuracy : 54.1%, Test Loss: 1.4539919942617416\n",
      "Epoch Time (Training + Test) = 9.79 seconds\n",
      "epoch: 54 average loss: 1.504\n",
      "Test Accuracy : 55.0%, Test Loss: 1.4692895896732807\n",
      "Epoch Time (Training + Test) = 9.79 seconds\n",
      "epoch: 55 average loss: 1.498\n",
      "Test Accuracy : 55.3%, Test Loss: 1.4584304988384247\n",
      "Epoch Time (Training + Test) = 9.85 seconds\n",
      "epoch: 56 average loss: 1.504\n",
      "Test Accuracy : 55.8%, Test Loss: 1.4562635608017445\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 57 average loss: 1.484\n",
      "Test Accuracy : 54.0%, Test Loss: 1.4402554258704185\n",
      "Epoch Time (Training + Test) = 9.76 seconds\n",
      "epoch: 58 average loss: 1.483\n",
      "Test Accuracy : 55.4%, Test Loss: 1.4592481404542923\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 59 average loss: 1.474\n",
      "Test Accuracy : 54.5%, Test Loss: 1.4408674910664558\n",
      "Epoch Time (Training + Test) = 9.89 seconds\n",
      "epoch: 60 average loss: 1.466\n",
      "Test Accuracy : 55.4%, Test Loss: 1.437205869704485\n",
      "Epoch Time (Training + Test) = 9.81 seconds\n",
      "epoch: 61 average loss: 1.464\n",
      "Test Accuracy : 54.5%, Test Loss: 1.4712948948144913\n",
      "Epoch Time (Training + Test) = 9.80 seconds\n",
      "epoch: 62 average loss: 1.459\n",
      "Test Accuracy : 55.1%, Test Loss: 1.4281509965658188\n",
      "Epoch Time (Training + Test) = 9.81 seconds\n",
      "epoch: 63 average loss: 1.450\n",
      "Test Accuracy : 56.1%, Test Loss: 1.3979711905121803\n",
      "Epoch Time (Training + Test) = 9.90 seconds\n",
      "epoch: 64 average loss: 1.440\n",
      "Test Accuracy : 56.2%, Test Loss: 1.4159585051238537\n",
      "Epoch Time (Training + Test) = 9.82 seconds\n",
      "epoch: 65 average loss: 1.438\n",
      "Test Accuracy : 55.9%, Test Loss: 1.4190976582467556\n",
      "Epoch Time (Training + Test) = 9.75 seconds\n",
      "epoch: 66 average loss: 1.446\n",
      "Test Accuracy : 55.1%, Test Loss: 1.4375885277986526\n",
      "Epoch Time (Training + Test) = 9.84 seconds\n",
      "epoch: 67 average loss: 1.442\n",
      "Test Accuracy : 57.1%, Test Loss: 1.3955152779817581\n",
      "Epoch Time (Training + Test) = 9.85 seconds\n",
      "epoch: 68 average loss: 1.436\n",
      "Test Accuracy : 56.9%, Test Loss: 1.383972018957138\n",
      "Epoch Time (Training + Test) = 9.78 seconds\n",
      "epoch: 69 average loss: 1.436\n",
      "Test Accuracy : 57.1%, Test Loss: 1.3991615995764732\n",
      "Epoch Time (Training + Test) = 9.89 seconds\n",
      "epoch: 70 average loss: 1.434\n",
      "Test Accuracy : 56.1%, Test Loss: 1.4147704765200615\n",
      "Epoch Time (Training + Test) = 9.84 seconds\n",
      "epoch: 71 average loss: 1.433\n",
      "Test Accuracy : 56.2%, Test Loss: 1.4113948047161102\n",
      "Epoch Time (Training + Test) = 9.82 seconds\n",
      "epoch: 72 average loss: 1.426\n",
      "Test Accuracy : 56.4%, Test Loss: 1.400808822363615\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 73 average loss: 1.419\n",
      "Test Accuracy : 56.0%, Test Loss: 1.4063738659024239\n",
      "Epoch Time (Training + Test) = 9.90 seconds\n",
      "epoch: 74 average loss: 1.421\n",
      "Test Accuracy : 55.5%, Test Loss: 1.4269526228308678\n",
      "Epoch Time (Training + Test) = 9.79 seconds\n",
      "epoch: 75 average loss: 1.421\n",
      "Test Accuracy : 56.6%, Test Loss: 1.3949931226670742\n",
      "Epoch Time (Training + Test) = 9.85 seconds\n",
      "Data Saved to Pruned_Conv2_80%_Conv3_70.00%.csv\n",
      "Finished Training: \n",
      "Total Time 0.410052 hours\n",
      " Average Time Per Epoch 19.68 seconds\n",
      "BNN_Resnet_UniAdapt(\n",
      "  (conv1): BinarizeConv2d(3, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (bn1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "  (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(80, 160, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): BinarizeConv2d(80, 160, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(160, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): BinarizeConv2d(160, 320, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)\n",
      "  (bn2): BatchNorm1d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (bn3): BatchNorm1d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (logsoftmax): LogSoftmax(dim=-1)\n",
      "  (fc): BinarizeLinear(in_features=320, out_features=80, bias=True)\n",
      "  (adapter_dict): ModuleDict()\n",
      "  (head_bn): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (head): BinarizeLinear(in_features=640, out_features=20, bias=True)\n",
      "  (encoder_net): placeholder()\n",
      "  (uniAdaptNet): Sequential(\n",
      "    (0): uniAdapt_Net(\n",
      "      (layer1): thinBlock3(\n",
      "        (conv2): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=80, bias=False)\n",
      "        (bn2): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(80, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (layer2): thinBlock3(\n",
      "        (conv2): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=160, bias=False)\n",
      "        (bn2): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(160, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (layer3): thinBlock3(\n",
      "        (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=320, bias=False)\n",
      "        (bn2): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (pool1): AdaptiveAvgPool2d(output_size=16)\n",
      "      (pool2): AdaptiveAvgPool2d(output_size=8)\n",
      "      (pool3): AdaptiveAvgPool2d(output_size=1)\n",
      "    )\n",
      "    (1): Flatten(start_dim=1, end_dim=-1)\n",
      "  )\n",
      ")\n",
      "No lr scheduler\n",
      "Optimizer Set to New: {'state': {}, 'param_groups': [{'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': False, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]}]}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3paegwr9) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Current Best Acc</td><td>▁▄▆▆▆▇▇▇▇▇▇▇▇███████████████████████████</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>epoch time (s)</td><td>▁███▇███▇▇███▇██▇▇▇███▇▇▇▇▇▇▇█▇█▇█▇██▇██</td></tr><tr><td>lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_accuracy</td><td>▁▄▆▆▆▇▇▇▇▇▇█████████▅▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>test_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>training_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Best Accuracy</td><td>0.625</td></tr><tr><td>Current Best Acc</td><td>0.625</td></tr><tr><td>Total Time (hours)</td><td>0.41005</td></tr><tr><td>epoch</td><td>75</td></tr><tr><td>epoch time (s)</td><td>9.85283</td></tr><tr><td>lr</td><td>0.001</td></tr><tr><td>test_accuracy</td><td>0.566</td></tr><tr><td>test_loss</td><td>1.39499</td></tr><tr><td>training_loss</td><td>1.42109</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">mythical-tauntaun-8</strong>: <a href=\"https://wandb.ai/johnny_suu/Cifar100_UniAdapt_Pruning/runs/3paegwr9\" target=\"_blank\">https://wandb.ai/johnny_suu/Cifar100_UniAdapt_Pruning/runs/3paegwr9</a><br/>Synced 5 W&B file(s), 1 media file(s), 1 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230505_103336-3paegwr9\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3paegwr9). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "530b72249c5648b4a0459dba66bbc492",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\John Su\\Downloads\\SydneyUni\\2022\\thesis\\Thesis\\wandb\\run-20230505_104615-2ikaxkpu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/johnny_suu/Cifar100_UniAdapt_Pruning/runs/2ikaxkpu\" target=\"_blank\">mythical-carrier-9</a></strong> to <a href=\"https://wandb.ai/johnny_suu/Cifar100_UniAdapt_Pruning\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "Project Name: Cifar100_UniAdapt_Pruning, Run Name Pruned_Conv2_80%_Conv3_80.00% \n",
      "\n",
      "\n",
      "Run Start : 2023-05-05 10-46-15\n",
      "start_epoch : 0\n",
      "initial_lr : 0.001\n",
      "batch_size : 64\n",
      "epochs : 75\n",
      "epoch_chkpts : []\n",
      "device : cuda:0\n",
      "criterion : CrossEntropyLoss()\n",
      "optimizer : Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: False\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "seed : None\n",
      "model_architecture : <class 'NN_Thesis.nn_classes.BNN_Resnet_UniAdapt'>\n",
      "binerised_training : True\n",
      "Number of Elements : 4547660\n",
      "Initial accuracy:\n",
      "Test Accuracy : 3.6%, Test Loss: 48.048012824574855\n",
      "best_acc.pth saved!\n",
      "Test Accuracy : 3.2%, Test Loss: 48.755441069602966\n",
      "epoch: 1 average loss: 2.443\n",
      "Test Accuracy : 32.3%, Test Loss: 2.2227411344647408\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.87 seconds\n",
      "epoch: 2 average loss: 2.166\n",
      "Test Accuracy : 39.2%, Test Loss: 2.0476343892514706\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.84 seconds\n",
      "epoch: 3 average loss: 2.036\n",
      "Test Accuracy : 41.6%, Test Loss: 1.997948281466961\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.82 seconds\n",
      "epoch: 4 average loss: 1.952\n",
      "Test Accuracy : 44.5%, Test Loss: 1.8823570534586906\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.84 seconds\n",
      "epoch: 5 average loss: 1.873\n",
      "Test Accuracy : 46.3%, Test Loss: 1.791127022355795\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.85 seconds\n",
      "epoch: 6 average loss: 1.810\n",
      "Test Accuracy : 48.0%, Test Loss: 1.7543168738484383\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.90 seconds\n",
      "epoch: 7 average loss: 1.754\n",
      "Test Accuracy : 49.5%, Test Loss: 1.722826298326254\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 8 average loss: 1.712\n",
      "Test Accuracy : 49.9%, Test Loss: 1.6759974658489227\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.84 seconds\n",
      "epoch: 9 average loss: 1.680\n",
      "Test Accuracy : 50.2%, Test Loss: 1.6253003478050232\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.82 seconds\n",
      "epoch: 10 average loss: 1.635\n",
      "Test Accuracy : 49.4%, Test Loss: 1.6601340100169182\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 11 average loss: 1.607\n",
      "Test Accuracy : 52.2%, Test Loss: 1.528155155479908\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.88 seconds\n",
      "epoch: 12 average loss: 1.557\n",
      "Test Accuracy : 53.5%, Test Loss: 1.5160724148154259\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.88 seconds\n",
      "epoch: 13 average loss: 1.535\n",
      "Test Accuracy : 54.5%, Test Loss: 1.4638530164957047\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.87 seconds\n",
      "epoch: 14 average loss: 1.497\n",
      "Test Accuracy : 56.0%, Test Loss: 1.4192644394934177\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.81 seconds\n",
      "epoch: 15 average loss: 1.480\n",
      "Test Accuracy : 55.9%, Test Loss: 1.4602312967181206\n",
      "Epoch Time (Training + Test) = 9.84 seconds\n",
      "epoch: 16 average loss: 1.454\n",
      "Test Accuracy : 56.1%, Test Loss: 1.4047654792666435\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 17 average loss: 1.432\n",
      "Test Accuracy : 56.0%, Test Loss: 1.4211041927337646\n",
      "Epoch Time (Training + Test) = 9.83 seconds\n",
      "epoch: 18 average loss: 1.406\n",
      "Test Accuracy : 58.2%, Test Loss: 1.3527345322072506\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.90 seconds\n",
      "epoch: 19 average loss: 1.372\n",
      "Test Accuracy : 58.2%, Test Loss: 1.3648888505995274\n",
      "Epoch Time (Training + Test) = 9.81 seconds\n",
      "epoch: 20 average loss: 1.370\n",
      "Test Accuracy : 58.6%, Test Loss: 1.3307066522538662\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.89 seconds\n",
      "epoch: 21 average loss: 1.347\n",
      "Test Accuracy : 55.9%, Test Loss: 1.3749022632837296\n",
      "Epoch Time (Training + Test) = 9.84 seconds\n",
      "epoch: 22 average loss: 1.340\n",
      "Test Accuracy : 58.6%, Test Loss: 1.3151263110339642\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 23 average loss: 1.315\n",
      "Test Accuracy : 59.0%, Test Loss: 1.3152703791856766\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 24 average loss: 1.295\n",
      "Test Accuracy : 58.2%, Test Loss: 1.3234923630952835\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 25 average loss: 1.276\n",
      "Test Accuracy : 60.0%, Test Loss: 1.315638929605484\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.88 seconds\n",
      "epoch: 26 average loss: 1.264\n",
      "Test Accuracy : 59.2%, Test Loss: 1.3072785325348377\n",
      "Epoch Time (Training + Test) = 9.83 seconds\n",
      "epoch: 27 average loss: 1.246\n",
      "Test Accuracy : 60.0%, Test Loss: 1.2617302313446999\n",
      "Epoch Time (Training + Test) = 9.83 seconds\n",
      "epoch: 28 average loss: 1.248\n",
      "Test Accuracy : 60.8%, Test Loss: 1.259803181514144\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 29 average loss: 1.226\n",
      "Test Accuracy : 61.4%, Test Loss: 1.2593949511647224\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.88 seconds\n",
      "epoch: 30 average loss: 1.209\n",
      "Test Accuracy : 61.0%, Test Loss: 1.2225069720298052\n",
      "Epoch Time (Training + Test) = 9.82 seconds\n",
      "epoch: 31 average loss: 1.187\n",
      "Test Accuracy : 62.4%, Test Loss: 1.229857362806797\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.88 seconds\n",
      "epoch: 32 average loss: 1.171\n",
      "Test Accuracy : 62.5%, Test Loss: 1.2242724671959877\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.90 seconds\n",
      "epoch: 33 average loss: 1.178\n",
      "Test Accuracy : 61.5%, Test Loss: 1.1947580240666866\n",
      "Epoch Time (Training + Test) = 9.76 seconds\n",
      "epoch: 34 average loss: 1.182\n",
      "Test Accuracy : 62.2%, Test Loss: 1.2350599989295006\n",
      "Epoch Time (Training + Test) = 9.77 seconds\n",
      "epoch: 35 average loss: 1.150\n",
      "Test Accuracy : 60.1%, Test Loss: 1.2294852491468191\n",
      "Epoch Time (Training + Test) = 9.77 seconds\n",
      "epoch: 36 average loss: 1.150\n",
      "Test Accuracy : 62.2%, Test Loss: 1.2047519441694021\n",
      "Epoch Time (Training + Test) = 9.88 seconds\n",
      "epoch: 37 average loss: 1.139\n",
      "Test Accuracy : 62.2%, Test Loss: 1.217777131125331\n",
      "Epoch Time (Training + Test) = 9.84 seconds\n",
      "conv2\n",
      "conv3\n",
      "conv2\n",
      "conv3\n",
      "conv2\n",
      "conv3\n",
      "Pruned [0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8]% of weights at epoch 37\n",
      "epoch: 38 average loss: 2.433\n",
      "Test Accuracy : 39.9%, Test Loss: 2.046066928654909\n",
      "Epoch Time (Training + Test) = 9.74 seconds\n",
      "epoch: 39 average loss: 1.970\n",
      "Test Accuracy : 44.1%, Test Loss: 1.833087831735611\n",
      "Epoch Time (Training + Test) = 9.74 seconds\n",
      "epoch: 40 average loss: 1.870\n",
      "Test Accuracy : 45.1%, Test Loss: 1.7915881052613258\n",
      "Epoch Time (Training + Test) = 9.83 seconds\n",
      "epoch: 41 average loss: 1.789\n",
      "Test Accuracy : 46.5%, Test Loss: 1.7573324367403984\n",
      "Epoch Time (Training + Test) = 9.85 seconds\n",
      "epoch: 42 average loss: 1.776\n",
      "Test Accuracy : 47.2%, Test Loss: 1.7085160948336124\n",
      "Epoch Time (Training + Test) = 9.89 seconds\n",
      "epoch: 43 average loss: 1.740\n",
      "Test Accuracy : 47.9%, Test Loss: 1.684331376105547\n",
      "Epoch Time (Training + Test) = 9.87 seconds\n",
      "epoch: 44 average loss: 1.711\n",
      "Test Accuracy : 47.6%, Test Loss: 1.657080203294754\n",
      "Epoch Time (Training + Test) = 9.83 seconds\n",
      "epoch: 45 average loss: 1.675\n",
      "Test Accuracy : 50.2%, Test Loss: 1.629937931895256\n",
      "Epoch Time (Training + Test) = 9.82 seconds\n",
      "epoch: 46 average loss: 1.674\n",
      "Test Accuracy : 51.3%, Test Loss: 1.639964260160923\n",
      "Epoch Time (Training + Test) = 9.87 seconds\n",
      "epoch: 47 average loss: 1.656\n",
      "Test Accuracy : 49.8%, Test Loss: 1.6041931882500648\n",
      "Epoch Time (Training + Test) = 9.90 seconds\n",
      "epoch: 48 average loss: 1.644\n",
      "Test Accuracy : 50.9%, Test Loss: 1.5955297946929932\n",
      "Epoch Time (Training + Test) = 9.82 seconds\n",
      "epoch: 49 average loss: 1.632\n",
      "Test Accuracy : 51.6%, Test Loss: 1.6094905622303486\n",
      "Epoch Time (Training + Test) = 9.88 seconds\n",
      "epoch: 50 average loss: 1.620\n",
      "Test Accuracy : 51.3%, Test Loss: 1.596084427088499\n",
      "Epoch Time (Training + Test) = 9.87 seconds\n",
      "epoch: 51 average loss: 1.609\n",
      "Test Accuracy : 50.1%, Test Loss: 1.6010687164962292\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 52 average loss: 1.599\n",
      "Test Accuracy : 50.4%, Test Loss: 1.5863515920937061\n",
      "Epoch Time (Training + Test) = 9.79 seconds\n",
      "epoch: 53 average loss: 1.602\n",
      "Test Accuracy : 52.5%, Test Loss: 1.5688796415925026\n",
      "Epoch Time (Training + Test) = 9.90 seconds\n",
      "epoch: 54 average loss: 1.578\n",
      "Test Accuracy : 52.0%, Test Loss: 1.564561054110527\n",
      "Epoch Time (Training + Test) = 9.82 seconds\n",
      "epoch: 55 average loss: 1.579\n",
      "Test Accuracy : 52.3%, Test Loss: 1.5538653247058392\n",
      "Epoch Time (Training + Test) = 9.88 seconds\n",
      "epoch: 56 average loss: 1.582\n",
      "Test Accuracy : 51.3%, Test Loss: 1.568485725671053\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 57 average loss: 1.568\n",
      "Test Accuracy : 52.5%, Test Loss: 1.53593360632658\n",
      "Epoch Time (Training + Test) = 9.85 seconds\n",
      "epoch: 58 average loss: 1.558\n",
      "Test Accuracy : 52.2%, Test Loss: 1.5323264710605145\n",
      "Epoch Time (Training + Test) = 9.84 seconds\n",
      "epoch: 59 average loss: 1.551\n",
      "Test Accuracy : 51.8%, Test Loss: 1.5382437035441399\n",
      "Epoch Time (Training + Test) = 9.90 seconds\n",
      "epoch: 60 average loss: 1.559\n",
      "Test Accuracy : 53.6%, Test Loss: 1.520579669624567\n",
      "Epoch Time (Training + Test) = 9.82 seconds\n",
      "epoch: 61 average loss: 1.547\n",
      "Test Accuracy : 52.8%, Test Loss: 1.5198708064854145\n",
      "Epoch Time (Training + Test) = 9.84 seconds\n",
      "epoch: 62 average loss: 1.544\n",
      "Test Accuracy : 54.1%, Test Loss: 1.4978826008737087\n",
      "Epoch Time (Training + Test) = 9.84 seconds\n",
      "epoch: 63 average loss: 1.534\n",
      "Test Accuracy : 52.7%, Test Loss: 1.513334732502699\n",
      "Epoch Time (Training + Test) = 9.77 seconds\n",
      "epoch: 64 average loss: 1.529\n",
      "Test Accuracy : 53.5%, Test Loss: 1.5093851648271084\n",
      "Epoch Time (Training + Test) = 9.80 seconds\n",
      "epoch: 65 average loss: 1.514\n",
      "Test Accuracy : 52.8%, Test Loss: 1.5025258176028728\n",
      "Epoch Time (Training + Test) = 9.84 seconds\n",
      "epoch: 66 average loss: 1.519\n",
      "Test Accuracy : 53.1%, Test Loss: 1.5042820125818253\n",
      "Epoch Time (Training + Test) = 9.84 seconds\n",
      "epoch: 67 average loss: 1.516\n",
      "Test Accuracy : 51.5%, Test Loss: 1.5174458622932434\n",
      "Epoch Time (Training + Test) = 9.81 seconds\n",
      "epoch: 68 average loss: 1.511\n",
      "Test Accuracy : 54.0%, Test Loss: 1.4907968118786812\n",
      "Epoch Time (Training + Test) = 9.85 seconds\n",
      "epoch: 69 average loss: 1.512\n",
      "Test Accuracy : 53.6%, Test Loss: 1.5028729401528835\n",
      "Epoch Time (Training + Test) = 9.84 seconds\n",
      "epoch: 70 average loss: 1.495\n",
      "Test Accuracy : 52.7%, Test Loss: 1.4962402358651161\n",
      "Epoch Time (Training + Test) = 9.80 seconds\n",
      "epoch: 71 average loss: 1.503\n",
      "Test Accuracy : 53.1%, Test Loss: 1.4921388551592827\n",
      "Epoch Time (Training + Test) = 9.93 seconds\n",
      "epoch: 72 average loss: 1.503\n",
      "Test Accuracy : 53.2%, Test Loss: 1.5197691544890404\n",
      "Epoch Time (Training + Test) = 9.89 seconds\n",
      "epoch: 73 average loss: 1.499\n",
      "Test Accuracy : 55.9%, Test Loss: 1.4815780371427536\n",
      "Epoch Time (Training + Test) = 9.81 seconds\n",
      "epoch: 74 average loss: 1.500\n",
      "Test Accuracy : 53.4%, Test Loss: 1.4870492555201054\n",
      "Epoch Time (Training + Test) = 9.87 seconds\n",
      "epoch: 75 average loss: 1.484\n",
      "Test Accuracy : 54.7%, Test Loss: 1.4523845352232456\n",
      "Epoch Time (Training + Test) = 9.87 seconds\n",
      "Data Saved to Pruned_Conv2_80%_Conv3_80.00%.csv\n",
      "Finished Training: \n",
      "Total Time 0.410231 hours\n",
      " Average Time Per Epoch 19.69 seconds\n",
      "BNN_Resnet_UniAdapt(\n",
      "  (conv1): BinarizeConv2d(3, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (bn1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "  (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(80, 160, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): BinarizeConv2d(80, 160, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(160, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): BinarizeConv2d(160, 320, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)\n",
      "  (bn2): BatchNorm1d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (bn3): BatchNorm1d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (logsoftmax): LogSoftmax(dim=-1)\n",
      "  (fc): BinarizeLinear(in_features=320, out_features=80, bias=True)\n",
      "  (adapter_dict): ModuleDict()\n",
      "  (head_bn): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (head): BinarizeLinear(in_features=640, out_features=20, bias=True)\n",
      "  (encoder_net): placeholder()\n",
      "  (uniAdaptNet): Sequential(\n",
      "    (0): uniAdapt_Net(\n",
      "      (layer1): thinBlock3(\n",
      "        (conv2): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=80, bias=False)\n",
      "        (bn2): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(80, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (layer2): thinBlock3(\n",
      "        (conv2): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=160, bias=False)\n",
      "        (bn2): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(160, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (layer3): thinBlock3(\n",
      "        (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=320, bias=False)\n",
      "        (bn2): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (pool1): AdaptiveAvgPool2d(output_size=16)\n",
      "      (pool2): AdaptiveAvgPool2d(output_size=8)\n",
      "      (pool3): AdaptiveAvgPool2d(output_size=1)\n",
      "    )\n",
      "    (1): Flatten(start_dim=1, end_dim=-1)\n",
      "  )\n",
      ")\n",
      "No lr scheduler\n",
      "Optimizer Set to New: {'state': {}, 'param_groups': [{'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': False, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]}]}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2ikaxkpu) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Current Best Acc</td><td>▁▄▆▆▆▇▇▇▇▇██████████████████████████████</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>epoch time (s)</td><td>▁█████████▇███████▇█▇████████████▇████▇█</td></tr><tr><td>lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_accuracy</td><td>▁▄▆▆▆▇▇▇▇▇▇▇████████▅▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>test_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>training_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Best Accuracy</td><td>0.6245</td></tr><tr><td>Current Best Acc</td><td>0.6245</td></tr><tr><td>Total Time (hours)</td><td>0.41023</td></tr><tr><td>epoch</td><td>75</td></tr><tr><td>epoch time (s)</td><td>9.86723</td></tr><tr><td>lr</td><td>0.001</td></tr><tr><td>test_accuracy</td><td>0.547</td></tr><tr><td>test_loss</td><td>1.45238</td></tr><tr><td>training_loss</td><td>1.48378</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">mythical-carrier-9</strong>: <a href=\"https://wandb.ai/johnny_suu/Cifar100_UniAdapt_Pruning/runs/2ikaxkpu\" target=\"_blank\">https://wandb.ai/johnny_suu/Cifar100_UniAdapt_Pruning/runs/2ikaxkpu</a><br/>Synced 5 W&B file(s), 1 media file(s), 1 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230505_104615-2ikaxkpu\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2ikaxkpu). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96cd04fb0b074830b70a2dc3169e8242",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01691666666883975, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\John Su\\Downloads\\SydneyUni\\2022\\thesis\\Thesis\\wandb\\run-20230505_105854-27wcslt5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/johnny_suu/Cifar100_UniAdapt_Pruning/runs/27wcslt5\" target=\"_blank\">stellar-rancor-10</a></strong> to <a href=\"https://wandb.ai/johnny_suu/Cifar100_UniAdapt_Pruning\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "Project Name: Cifar100_UniAdapt_Pruning, Run Name Pruned_Conv2_80%_Conv3_90.00% \n",
      "\n",
      "\n",
      "Run Start : 2023-05-05 10-58-54\n",
      "start_epoch : 0\n",
      "initial_lr : 0.001\n",
      "batch_size : 64\n",
      "epochs : 75\n",
      "epoch_chkpts : []\n",
      "device : cuda:0\n",
      "criterion : CrossEntropyLoss()\n",
      "optimizer : Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: False\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "seed : None\n",
      "model_architecture : <class 'NN_Thesis.nn_classes.BNN_Resnet_UniAdapt'>\n",
      "binerised_training : True\n",
      "Number of Elements : 4547660\n",
      "Initial accuracy:\n",
      "Test Accuracy : 5.2%, Test Loss: 55.79667906548567\n",
      "best_acc.pth saved!\n",
      "Test Accuracy : 4.9%, Test Loss: 55.2494136095047\n",
      "epoch: 1 average loss: 2.439\n",
      "Test Accuracy : 32.7%, Test Loss: 2.230220578610897\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.84 seconds\n",
      "epoch: 2 average loss: 2.147\n",
      "Test Accuracy : 38.9%, Test Loss: 2.0517314672470093\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.85 seconds\n",
      "epoch: 3 average loss: 2.044\n",
      "Test Accuracy : 43.0%, Test Loss: 1.920956201851368\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.93 seconds\n",
      "epoch: 4 average loss: 1.946\n",
      "Test Accuracy : 44.8%, Test Loss: 1.8784564025700092\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.82 seconds\n",
      "epoch: 5 average loss: 1.878\n",
      "Test Accuracy : 46.2%, Test Loss: 1.7963160052895546\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.87 seconds\n",
      "epoch: 6 average loss: 1.812\n",
      "Test Accuracy : 49.4%, Test Loss: 1.7036106958985329\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.85 seconds\n",
      "epoch: 7 average loss: 1.757\n",
      "Test Accuracy : 49.9%, Test Loss: 1.6686890684068203\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 8 average loss: 1.710\n",
      "Test Accuracy : 50.8%, Test Loss: 1.659176729619503\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.85 seconds\n",
      "epoch: 9 average loss: 1.673\n",
      "Test Accuracy : 51.1%, Test Loss: 1.628131739795208\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.85 seconds\n",
      "epoch: 10 average loss: 1.625\n",
      "Test Accuracy : 53.9%, Test Loss: 1.5677340552210808\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 11 average loss: 1.593\n",
      "Test Accuracy : 53.1%, Test Loss: 1.543171238154173\n",
      "Epoch Time (Training + Test) = 9.82 seconds\n",
      "epoch: 12 average loss: 1.538\n",
      "Test Accuracy : 54.9%, Test Loss: 1.4960535876452923\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.97 seconds\n",
      "epoch: 13 average loss: 1.543\n",
      "Test Accuracy : 54.9%, Test Loss: 1.4875469356775284\n",
      "Epoch Time (Training + Test) = 9.83 seconds\n",
      "epoch: 14 average loss: 1.517\n",
      "Test Accuracy : 53.6%, Test Loss: 1.5047947876155376\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 15 average loss: 1.471\n",
      "Test Accuracy : 54.9%, Test Loss: 1.4497997760772705\n",
      "Epoch Time (Training + Test) = 9.85 seconds\n",
      "epoch: 16 average loss: 1.449\n",
      "Test Accuracy : 57.7%, Test Loss: 1.388624057173729\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.92 seconds\n",
      "epoch: 17 average loss: 1.423\n",
      "Test Accuracy : 56.7%, Test Loss: 1.4095676429569721\n",
      "Epoch Time (Training + Test) = 9.83 seconds\n",
      "epoch: 18 average loss: 1.398\n",
      "Test Accuracy : 57.5%, Test Loss: 1.3831777535378933\n",
      "Epoch Time (Training + Test) = 9.87 seconds\n",
      "epoch: 19 average loss: 1.382\n",
      "Test Accuracy : 58.1%, Test Loss: 1.3566700145602226\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.87 seconds\n",
      "epoch: 20 average loss: 1.364\n",
      "Test Accuracy : 60.1%, Test Loss: 1.3215375356376171\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.89 seconds\n",
      "epoch: 21 average loss: 1.339\n",
      "Test Accuracy : 58.2%, Test Loss: 1.3373265638947487\n",
      "Epoch Time (Training + Test) = 9.83 seconds\n",
      "epoch: 22 average loss: 1.319\n",
      "Test Accuracy : 60.5%, Test Loss: 1.2915933225303888\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.84 seconds\n",
      "epoch: 23 average loss: 1.311\n",
      "Test Accuracy : 60.5%, Test Loss: 1.295518882572651\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 24 average loss: 1.288\n",
      "Test Accuracy : 58.8%, Test Loss: 1.3262803629040718\n",
      "Epoch Time (Training + Test) = 9.85 seconds\n",
      "epoch: 25 average loss: 1.285\n",
      "Test Accuracy : 59.2%, Test Loss: 1.3058823347091675\n",
      "Epoch Time (Training + Test) = 9.85 seconds\n",
      "epoch: 26 average loss: 1.254\n",
      "Test Accuracy : 60.6%, Test Loss: 1.2830159924924374\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.88 seconds\n",
      "epoch: 27 average loss: 1.249\n",
      "Test Accuracy : 60.1%, Test Loss: 1.2866551205515862\n",
      "Epoch Time (Training + Test) = 9.82 seconds\n",
      "epoch: 28 average loss: 1.220\n",
      "Test Accuracy : 59.8%, Test Loss: 1.2832808457314968\n",
      "Epoch Time (Training + Test) = 9.81 seconds\n",
      "epoch: 29 average loss: 1.227\n",
      "Test Accuracy : 60.8%, Test Loss: 1.2391362134367228\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.88 seconds\n",
      "epoch: 30 average loss: 1.199\n",
      "Test Accuracy : 61.9%, Test Loss: 1.2273796144872904\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 10.07 seconds\n",
      "epoch: 31 average loss: 1.182\n",
      "Test Accuracy : 60.9%, Test Loss: 1.2413281109184027\n",
      "Epoch Time (Training + Test) = 10.03 seconds\n",
      "epoch: 32 average loss: 1.173\n",
      "Test Accuracy : 63.5%, Test Loss: 1.1942528877407312\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.88 seconds\n",
      "epoch: 33 average loss: 1.168\n",
      "Test Accuracy : 62.3%, Test Loss: 1.2160335034132004\n",
      "Epoch Time (Training + Test) = 9.85 seconds\n",
      "epoch: 34 average loss: 1.149\n",
      "Test Accuracy : 62.4%, Test Loss: 1.2046361081302166\n",
      "Epoch Time (Training + Test) = 9.85 seconds\n",
      "epoch: 35 average loss: 1.139\n",
      "Test Accuracy : 61.9%, Test Loss: 1.2313631437718868\n",
      "Epoch Time (Training + Test) = 9.82 seconds\n",
      "epoch: 36 average loss: 1.130\n",
      "Test Accuracy : 63.8%, Test Loss: 1.1664758883416653\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.91 seconds\n",
      "epoch: 37 average loss: 1.127\n",
      "Test Accuracy : 62.3%, Test Loss: 1.206609757617116\n",
      "Epoch Time (Training + Test) = 9.85 seconds\n",
      "conv2\n",
      "conv3\n",
      "conv2\n",
      "conv3\n",
      "conv2\n",
      "conv3\n",
      "Pruned [0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9]% of weights at epoch 37\n",
      "epoch: 38 average loss: 2.582\n",
      "Test Accuracy : 34.8%, Test Loss: 2.1762903593480587\n",
      "Epoch Time (Training + Test) = 9.79 seconds\n",
      "epoch: 39 average loss: 2.142\n",
      "Test Accuracy : 37.8%, Test Loss: 2.060782603919506\n",
      "Epoch Time (Training + Test) = 9.85 seconds\n",
      "epoch: 40 average loss: 2.024\n",
      "Test Accuracy : 40.5%, Test Loss: 1.9760514684021473\n",
      "Epoch Time (Training + Test) = 9.84 seconds\n",
      "epoch: 41 average loss: 1.954\n",
      "Test Accuracy : 43.2%, Test Loss: 1.861037950962782\n",
      "Epoch Time (Training + Test) = 9.78 seconds\n",
      "epoch: 42 average loss: 1.914\n",
      "Test Accuracy : 42.9%, Test Loss: 1.8683073297142982\n",
      "Epoch Time (Training + Test) = 9.81 seconds\n",
      "epoch: 43 average loss: 1.864\n",
      "Test Accuracy : 43.8%, Test Loss: 1.8108521699905396\n",
      "Epoch Time (Training + Test) = 9.84 seconds\n",
      "epoch: 44 average loss: 1.843\n",
      "Test Accuracy : 43.8%, Test Loss: 1.8078120313584805\n",
      "Epoch Time (Training + Test) = 9.79 seconds\n",
      "epoch: 45 average loss: 1.823\n",
      "Test Accuracy : 46.1%, Test Loss: 1.7504050619900227\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 46 average loss: 1.812\n",
      "Test Accuracy : 46.0%, Test Loss: 1.7491135522723198\n",
      "Epoch Time (Training + Test) = 9.83 seconds\n",
      "epoch: 47 average loss: 1.785\n",
      "Test Accuracy : 47.2%, Test Loss: 1.736788235604763\n",
      "Epoch Time (Training + Test) = 9.87 seconds\n",
      "epoch: 48 average loss: 1.770\n",
      "Test Accuracy : 45.9%, Test Loss: 1.7643451541662216\n",
      "Epoch Time (Training + Test) = 9.76 seconds\n",
      "epoch: 49 average loss: 1.764\n",
      "Test Accuracy : 46.8%, Test Loss: 1.70441272854805\n",
      "Epoch Time (Training + Test) = 9.82 seconds\n",
      "epoch: 50 average loss: 1.770\n",
      "Test Accuracy : 46.6%, Test Loss: 1.728336252272129\n",
      "Epoch Time (Training + Test) = 9.82 seconds\n",
      "epoch: 51 average loss: 1.742\n",
      "Test Accuracy : 48.0%, Test Loss: 1.7019971683621407\n",
      "Epoch Time (Training + Test) = 9.85 seconds\n",
      "epoch: 52 average loss: 1.732\n",
      "Test Accuracy : 47.8%, Test Loss: 1.7003276348114014\n",
      "Epoch Time (Training + Test) = 9.85 seconds\n",
      "epoch: 53 average loss: 1.723\n",
      "Test Accuracy : 49.0%, Test Loss: 1.6741865240037441\n",
      "Epoch Time (Training + Test) = 9.85 seconds\n",
      "epoch: 54 average loss: 1.734\n",
      "Test Accuracy : 48.8%, Test Loss: 1.6749219931662083\n",
      "Epoch Time (Training + Test) = 9.83 seconds\n",
      "epoch: 55 average loss: 1.705\n",
      "Test Accuracy : 47.9%, Test Loss: 1.7000313214957714\n",
      "Epoch Time (Training + Test) = 9.87 seconds\n",
      "epoch: 56 average loss: 1.707\n",
      "Test Accuracy : 48.2%, Test Loss: 1.6543083377182484\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 57 average loss: 1.688\n",
      "Test Accuracy : 49.7%, Test Loss: 1.6473124586045742\n",
      "Epoch Time (Training + Test) = 9.87 seconds\n",
      "epoch: 58 average loss: 1.682\n",
      "Test Accuracy : 48.8%, Test Loss: 1.641724944114685\n",
      "Epoch Time (Training + Test) = 9.84 seconds\n",
      "epoch: 59 average loss: 1.676\n",
      "Test Accuracy : 49.8%, Test Loss: 1.639836896210909\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 60 average loss: 1.693\n",
      "Test Accuracy : 48.5%, Test Loss: 1.6428999528288841\n",
      "Epoch Time (Training + Test) = 9.76 seconds\n",
      "epoch: 61 average loss: 1.677\n",
      "Test Accuracy : 49.7%, Test Loss: 1.6375338323414326\n",
      "Epoch Time (Training + Test) = 9.82 seconds\n",
      "epoch: 62 average loss: 1.678\n",
      "Test Accuracy : 50.8%, Test Loss: 1.6282660514116287\n",
      "Epoch Time (Training + Test) = 9.79 seconds\n",
      "epoch: 63 average loss: 1.665\n",
      "Test Accuracy : 49.6%, Test Loss: 1.6379773914813995\n",
      "Epoch Time (Training + Test) = 9.88 seconds\n",
      "epoch: 64 average loss: 1.662\n",
      "Test Accuracy : 49.6%, Test Loss: 1.621698722243309\n",
      "Epoch Time (Training + Test) = 9.87 seconds\n",
      "epoch: 65 average loss: 1.651\n",
      "Test Accuracy : 50.0%, Test Loss: 1.6182912662625313\n",
      "Epoch Time (Training + Test) = 9.79 seconds\n",
      "epoch: 66 average loss: 1.644\n",
      "Test Accuracy : 50.6%, Test Loss: 1.6119857802987099\n",
      "Epoch Time (Training + Test) = 9.78 seconds\n",
      "epoch: 67 average loss: 1.650\n",
      "Test Accuracy : 49.5%, Test Loss: 1.6346482634544373\n",
      "Epoch Time (Training + Test) = 9.77 seconds\n",
      "epoch: 68 average loss: 1.639\n",
      "Test Accuracy : 50.1%, Test Loss: 1.5943491980433464\n",
      "Epoch Time (Training + Test) = 9.82 seconds\n",
      "epoch: 69 average loss: 1.648\n",
      "Test Accuracy : 49.9%, Test Loss: 1.6168404966592789\n",
      "Epoch Time (Training + Test) = 9.85 seconds\n",
      "epoch: 70 average loss: 1.616\n",
      "Test Accuracy : 48.5%, Test Loss: 1.6128085553646088\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 71 average loss: 1.642\n",
      "Test Accuracy : 50.6%, Test Loss: 1.5907410606741905\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 72 average loss: 1.620\n",
      "Test Accuracy : 51.2%, Test Loss: 1.600407000631094\n",
      "Epoch Time (Training + Test) = 9.88 seconds\n",
      "epoch: 73 average loss: 1.630\n",
      "Test Accuracy : 50.7%, Test Loss: 1.5961546041071415\n",
      "Epoch Time (Training + Test) = 9.85 seconds\n",
      "epoch: 74 average loss: 1.626\n",
      "Test Accuracy : 50.1%, Test Loss: 1.5858954042196274\n",
      "Epoch Time (Training + Test) = 9.87 seconds\n",
      "epoch: 75 average loss: 1.622\n",
      "Test Accuracy : 50.8%, Test Loss: 1.6037101931869984\n",
      "Epoch Time (Training + Test) = 9.85 seconds\n",
      "Data Saved to Pruned_Conv2_80%_Conv3_90.00%.csv\n",
      "Finished Training: \n",
      "Total Time 0.410438 hours\n",
      " Average Time Per Epoch 19.70 seconds\n",
      "BNN_Resnet_UniAdapt(\n",
      "  (conv1): BinarizeConv2d(3, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (bn1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "  (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(80, 160, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): BinarizeConv2d(80, 160, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(160, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): BinarizeConv2d(160, 320, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)\n",
      "  (bn2): BatchNorm1d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (bn3): BatchNorm1d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (logsoftmax): LogSoftmax(dim=-1)\n",
      "  (fc): BinarizeLinear(in_features=320, out_features=80, bias=True)\n",
      "  (adapter_dict): ModuleDict()\n",
      "  (head_bn): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (head): BinarizeLinear(in_features=640, out_features=20, bias=True)\n",
      "  (encoder_net): placeholder()\n",
      "  (uniAdaptNet): Sequential(\n",
      "    (0): uniAdapt_Net(\n",
      "      (layer1): thinBlock3(\n",
      "        (conv2): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=80, bias=False)\n",
      "        (bn2): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(80, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (layer2): thinBlock3(\n",
      "        (conv2): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=160, bias=False)\n",
      "        (bn2): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(160, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (layer3): thinBlock3(\n",
      "        (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=320, bias=False)\n",
      "        (bn2): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (pool1): AdaptiveAvgPool2d(output_size=16)\n",
      "      (pool2): AdaptiveAvgPool2d(output_size=8)\n",
      "      (pool3): AdaptiveAvgPool2d(output_size=1)\n",
      "    )\n",
      "    (1): Flatten(start_dim=1, end_dim=-1)\n",
      "  )\n",
      ")\n",
      "No lr scheduler\n",
      "Optimizer Set to New: {'state': {}, 'param_groups': [{'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': False, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]}]}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:27wcslt5) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Current Best Acc</td><td>▁▄▆▆▆▆▇▇▇▇▇█████████████████████████████</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>epoch time (s)</td><td>▁▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_accuracy</td><td>▁▄▆▆▆▆▇▇▇▇▇▇█▇██████▅▅▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆</td></tr><tr><td>test_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>training_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Best Accuracy</td><td>0.638</td></tr><tr><td>Current Best Acc</td><td>0.638</td></tr><tr><td>Total Time (hours)</td><td>0.41044</td></tr><tr><td>epoch</td><td>75</td></tr><tr><td>epoch time (s)</td><td>9.84797</td></tr><tr><td>lr</td><td>0.001</td></tr><tr><td>test_accuracy</td><td>0.5085</td></tr><tr><td>test_loss</td><td>1.60371</td></tr><tr><td>training_loss</td><td>1.62153</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">stellar-rancor-10</strong>: <a href=\"https://wandb.ai/johnny_suu/Cifar100_UniAdapt_Pruning/runs/27wcslt5\" target=\"_blank\">https://wandb.ai/johnny_suu/Cifar100_UniAdapt_Pruning/runs/27wcslt5</a><br/>Synced 5 W&B file(s), 1 media file(s), 1 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230505_105854-27wcslt5\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:27wcslt5). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31e540630fb540a187b7188c7537d1f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\John Su\\Downloads\\SydneyUni\\2022\\thesis\\Thesis\\wandb\\run-20230505_111135-36rkt0v3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/johnny_suu/Cifar100_UniAdapt_Pruning/runs/36rkt0v3\" target=\"_blank\">tusken-speeder-11</a></strong> to <a href=\"https://wandb.ai/johnny_suu/Cifar100_UniAdapt_Pruning\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "Project Name: Cifar100_UniAdapt_Pruning, Run Name Pruned_Conv2_80%_Conv3_95.00% \n",
      "\n",
      "\n",
      "Run Start : 2023-05-05 11-11-35\n",
      "start_epoch : 0\n",
      "initial_lr : 0.001\n",
      "batch_size : 64\n",
      "epochs : 75\n",
      "epoch_chkpts : []\n",
      "device : cuda:0\n",
      "criterion : CrossEntropyLoss()\n",
      "optimizer : Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: False\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "seed : None\n",
      "model_architecture : <class 'NN_Thesis.nn_classes.BNN_Resnet_UniAdapt'>\n",
      "binerised_training : True\n",
      "Number of Elements : 4547660\n",
      "Initial accuracy:\n",
      "Test Accuracy : 5.5%, Test Loss: 41.859062316311395\n",
      "best_acc.pth saved!\n",
      "Test Accuracy : 5.5%, Test Loss: 40.51018762588501\n",
      "best_acc.pth saved!\n",
      "epoch: 1 average loss: 2.442\n",
      "Test Accuracy : 34.9%, Test Loss: 2.1831477358937263\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.82 seconds\n",
      "epoch: 2 average loss: 2.156\n",
      "Test Accuracy : 40.8%, Test Loss: 2.022528473287821\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.85 seconds\n",
      "epoch: 3 average loss: 2.030\n",
      "Test Accuracy : 41.3%, Test Loss: 1.985166884958744\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 4 average loss: 1.943\n",
      "Test Accuracy : 42.6%, Test Loss: 1.909396167844534\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.84 seconds\n",
      "epoch: 5 average loss: 1.875\n",
      "Test Accuracy : 45.5%, Test Loss: 1.836825031787157\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 6 average loss: 1.818\n",
      "Test Accuracy : 47.5%, Test Loss: 1.7226039059460163\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.83 seconds\n",
      "epoch: 7 average loss: 1.770\n",
      "Test Accuracy : 48.1%, Test Loss: 1.7605322673916817\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.89 seconds\n",
      "epoch: 8 average loss: 1.716\n",
      "Test Accuracy : 49.5%, Test Loss: 1.6684147901833057\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 9 average loss: 1.683\n",
      "Test Accuracy : 50.9%, Test Loss: 1.63216383010149\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.83 seconds\n",
      "epoch: 10 average loss: 1.624\n",
      "Test Accuracy : 49.9%, Test Loss: 1.644652597606182\n",
      "Epoch Time (Training + Test) = 9.83 seconds\n",
      "epoch: 11 average loss: 1.610\n",
      "Test Accuracy : 53.0%, Test Loss: 1.5610126219689846\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.92 seconds\n",
      "epoch: 12 average loss: 1.572\n",
      "Test Accuracy : 52.2%, Test Loss: 1.5995800085365772\n",
      "Epoch Time (Training + Test) = 9.79 seconds\n",
      "epoch: 13 average loss: 1.550\n",
      "Test Accuracy : 53.1%, Test Loss: 1.5256382189691067\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.89 seconds\n",
      "epoch: 14 average loss: 1.514\n",
      "Test Accuracy : 55.0%, Test Loss: 1.4942468889057636\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.98 seconds\n",
      "epoch: 15 average loss: 1.484\n",
      "Test Accuracy : 55.0%, Test Loss: 1.4531530141830444\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.94 seconds\n",
      "epoch: 16 average loss: 1.464\n",
      "Test Accuracy : 55.6%, Test Loss: 1.4599743857979774\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.87 seconds\n",
      "epoch: 17 average loss: 1.430\n",
      "Test Accuracy : 56.3%, Test Loss: 1.4350563921034336\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.83 seconds\n",
      "epoch: 18 average loss: 1.437\n",
      "Test Accuracy : 56.9%, Test Loss: 1.42477872595191\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.80 seconds\n",
      "epoch: 19 average loss: 1.384\n",
      "Test Accuracy : 55.7%, Test Loss: 1.4110027104616165\n",
      "Epoch Time (Training + Test) = 9.85 seconds\n",
      "epoch: 20 average loss: 1.364\n",
      "Test Accuracy : 56.8%, Test Loss: 1.3792287483811378\n",
      "Epoch Time (Training + Test) = 9.88 seconds\n",
      "epoch: 21 average loss: 1.355\n",
      "Test Accuracy : 58.7%, Test Loss: 1.3562251552939415\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.90 seconds\n",
      "epoch: 22 average loss: 1.338\n",
      "Test Accuracy : 57.1%, Test Loss: 1.3546473011374474\n",
      "Epoch Time (Training + Test) = 9.81 seconds\n",
      "epoch: 23 average loss: 1.320\n",
      "Test Accuracy : 58.4%, Test Loss: 1.3376792818307877\n",
      "Epoch Time (Training + Test) = 9.90 seconds\n",
      "epoch: 24 average loss: 1.298\n",
      "Test Accuracy : 58.6%, Test Loss: 1.311178371310234\n",
      "Epoch Time (Training + Test) = 9.85 seconds\n",
      "epoch: 25 average loss: 1.282\n",
      "Test Accuracy : 59.5%, Test Loss: 1.3125247322022915\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.80 seconds\n",
      "epoch: 26 average loss: 1.265\n",
      "Test Accuracy : 59.8%, Test Loss: 1.282637471333146\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.89 seconds\n",
      "epoch: 27 average loss: 1.269\n",
      "Test Accuracy : 60.4%, Test Loss: 1.267262239009142\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.83 seconds\n",
      "epoch: 28 average loss: 1.233\n",
      "Test Accuracy : 59.6%, Test Loss: 1.2782558035105467\n",
      "Epoch Time (Training + Test) = 9.85 seconds\n",
      "epoch: 29 average loss: 1.231\n",
      "Test Accuracy : 60.2%, Test Loss: 1.2742319758981466\n",
      "Epoch Time (Training + Test) = 9.87 seconds\n",
      "epoch: 30 average loss: 1.198\n",
      "Test Accuracy : 61.2%, Test Loss: 1.2414916343986988\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.85 seconds\n",
      "epoch: 31 average loss: 1.190\n",
      "Test Accuracy : 62.1%, Test Loss: 1.2379048615694046\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.88 seconds\n",
      "epoch: 32 average loss: 1.176\n",
      "Test Accuracy : 62.7%, Test Loss: 1.2056324183940887\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.83 seconds\n",
      "epoch: 33 average loss: 1.164\n",
      "Test Accuracy : 61.5%, Test Loss: 1.256613727658987\n",
      "Epoch Time (Training + Test) = 9.85 seconds\n",
      "epoch: 34 average loss: 1.163\n",
      "Test Accuracy : 62.4%, Test Loss: 1.2012192904949188\n",
      "Epoch Time (Training + Test) = 9.84 seconds\n",
      "epoch: 35 average loss: 1.147\n",
      "Test Accuracy : 62.6%, Test Loss: 1.217030331492424\n",
      "Epoch Time (Training + Test) = 9.84 seconds\n",
      "epoch: 36 average loss: 1.137\n",
      "Test Accuracy : 61.0%, Test Loss: 1.2077908590435982\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 37 average loss: 1.137\n",
      "Test Accuracy : 61.9%, Test Loss: 1.2361756134778261\n",
      "Epoch Time (Training + Test) = 9.84 seconds\n",
      "conv2\n",
      "conv3\n",
      "conv2\n",
      "conv3\n",
      "conv2\n",
      "conv3\n",
      "Pruned [0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95]% of weights at epoch 37\n",
      "epoch: 38 average loss: 2.866\n",
      "Test Accuracy : 27.9%, Test Loss: 2.4552878364920616\n",
      "Epoch Time (Training + Test) = 9.83 seconds\n",
      "epoch: 39 average loss: 2.411\n",
      "Test Accuracy : 31.8%, Test Loss: 2.2692132368683815\n",
      "Epoch Time (Training + Test) = 9.84 seconds\n",
      "epoch: 40 average loss: 2.288\n",
      "Test Accuracy : 34.2%, Test Loss: 2.1860745064914227\n",
      "Epoch Time (Training + Test) = 9.78 seconds\n",
      "epoch: 41 average loss: 2.209\n",
      "Test Accuracy : 36.1%, Test Loss: 2.1310626193881035\n",
      "Epoch Time (Training + Test) = 9.79 seconds\n",
      "epoch: 42 average loss: 2.153\n",
      "Test Accuracy : 37.1%, Test Loss: 2.104559738188982\n",
      "Epoch Time (Training + Test) = 9.85 seconds\n",
      "epoch: 43 average loss: 2.144\n",
      "Test Accuracy : 38.1%, Test Loss: 2.0529339984059334\n",
      "Epoch Time (Training + Test) = 9.85 seconds\n",
      "epoch: 44 average loss: 2.101\n",
      "Test Accuracy : 39.7%, Test Loss: 2.0332209058105946\n",
      "Epoch Time (Training + Test) = 9.83 seconds\n",
      "epoch: 45 average loss: 2.091\n",
      "Test Accuracy : 39.2%, Test Loss: 2.004662625491619\n",
      "Epoch Time (Training + Test) = 9.85 seconds\n",
      "epoch: 46 average loss: 2.052\n",
      "Test Accuracy : 39.7%, Test Loss: 1.9935660101473331\n",
      "Epoch Time (Training + Test) = 9.82 seconds\n",
      "epoch: 47 average loss: 2.042\n",
      "Test Accuracy : 40.0%, Test Loss: 2.004978194832802\n",
      "Epoch Time (Training + Test) = 9.88 seconds\n",
      "epoch: 48 average loss: 2.046\n",
      "Test Accuracy : 40.3%, Test Loss: 1.955160990357399\n",
      "Epoch Time (Training + Test) = 9.87 seconds\n",
      "epoch: 49 average loss: 2.026\n",
      "Test Accuracy : 41.6%, Test Loss: 1.9228755198419094\n",
      "Epoch Time (Training + Test) = 9.81 seconds\n",
      "epoch: 50 average loss: 2.003\n",
      "Test Accuracy : 40.8%, Test Loss: 1.9462744072079659\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 51 average loss: 2.008\n",
      "Test Accuracy : 41.8%, Test Loss: 1.9357982277870178\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 52 average loss: 1.987\n",
      "Test Accuracy : 41.0%, Test Loss: 1.9270904213190079\n",
      "Epoch Time (Training + Test) = 9.88 seconds\n",
      "epoch: 53 average loss: 1.970\n",
      "Test Accuracy : 40.9%, Test Loss: 1.970328189432621\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 54 average loss: 1.957\n",
      "Test Accuracy : 41.8%, Test Loss: 1.8951186761260033\n",
      "Epoch Time (Training + Test) = 9.82 seconds\n",
      "epoch: 55 average loss: 1.956\n",
      "Test Accuracy : 42.9%, Test Loss: 1.8907866440713406\n",
      "Epoch Time (Training + Test) = 9.88 seconds\n",
      "epoch: 56 average loss: 1.949\n",
      "Test Accuracy : 41.6%, Test Loss: 1.9020938500761986\n",
      "Epoch Time (Training + Test) = 9.85 seconds\n",
      "epoch: 57 average loss: 1.942\n",
      "Test Accuracy : 42.6%, Test Loss: 1.879916489124298\n",
      "Epoch Time (Training + Test) = 9.84 seconds\n",
      "epoch: 58 average loss: 1.931\n",
      "Test Accuracy : 43.0%, Test Loss: 1.8704893216490746\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 59 average loss: 1.917\n",
      "Test Accuracy : 43.6%, Test Loss: 1.860454998910427\n",
      "Epoch Time (Training + Test) = 9.88 seconds\n",
      "epoch: 60 average loss: 1.926\n",
      "Test Accuracy : 42.9%, Test Loss: 1.8870384506881237\n",
      "Epoch Time (Training + Test) = 9.85 seconds\n",
      "epoch: 61 average loss: 1.912\n",
      "Test Accuracy : 42.1%, Test Loss: 1.8727265894412994\n",
      "Epoch Time (Training + Test) = 9.88 seconds\n",
      "epoch: 62 average loss: 1.894\n",
      "Test Accuracy : 43.2%, Test Loss: 1.8612765558063984\n",
      "Epoch Time (Training + Test) = 9.84 seconds\n",
      "epoch: 63 average loss: 1.900\n",
      "Test Accuracy : 44.2%, Test Loss: 1.8274588733911514\n",
      "Epoch Time (Training + Test) = 9.84 seconds\n",
      "epoch: 64 average loss: 1.892\n",
      "Test Accuracy : 43.2%, Test Loss: 1.8433948792517185\n",
      "Epoch Time (Training + Test) = 9.88 seconds\n",
      "epoch: 65 average loss: 1.879\n",
      "Test Accuracy : 43.5%, Test Loss: 1.8467497788369656\n",
      "Epoch Time (Training + Test) = 9.82 seconds\n",
      "epoch: 66 average loss: 1.881\n",
      "Test Accuracy : 44.4%, Test Loss: 1.831525132060051\n",
      "Epoch Time (Training + Test) = 9.87 seconds\n",
      "epoch: 67 average loss: 1.883\n",
      "Test Accuracy : 44.8%, Test Loss: 1.8197779692709446\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 68 average loss: 1.875\n",
      "Test Accuracy : 44.7%, Test Loss: 1.8202894255518913\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 69 average loss: 1.864\n",
      "Test Accuracy : 44.7%, Test Loss: 1.8353286758065224\n",
      "Epoch Time (Training + Test) = 9.82 seconds\n",
      "epoch: 70 average loss: 1.856\n",
      "Test Accuracy : 44.9%, Test Loss: 1.7834421284496784\n",
      "Epoch Time (Training + Test) = 9.85 seconds\n",
      "epoch: 71 average loss: 1.859\n",
      "Test Accuracy : 45.4%, Test Loss: 1.7815243303775787\n",
      "Epoch Time (Training + Test) = 9.85 seconds\n",
      "epoch: 72 average loss: 1.852\n",
      "Test Accuracy : 44.0%, Test Loss: 1.8175041116774082\n",
      "Epoch Time (Training + Test) = 9.87 seconds\n",
      "epoch: 73 average loss: 1.838\n",
      "Test Accuracy : 45.6%, Test Loss: 1.8103579208254814\n",
      "Epoch Time (Training + Test) = 9.87 seconds\n",
      "epoch: 74 average loss: 1.852\n",
      "Test Accuracy : 44.5%, Test Loss: 1.8070701211690903\n",
      "Epoch Time (Training + Test) = 9.87 seconds\n",
      "epoch: 75 average loss: 1.843\n",
      "Test Accuracy : 44.5%, Test Loss: 1.8211590237915516\n",
      "Epoch Time (Training + Test) = 9.92 seconds\n",
      "Data Saved to Pruned_Conv2_80%_Conv3_95.00%.csv\n",
      "Finished Training: \n",
      "Total Time 0.410588 hours\n",
      " Average Time Per Epoch 19.71 seconds\n",
      "BNN_Resnet_UniAdapt(\n",
      "  (conv1): BinarizeConv2d(3, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (bn1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "  (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(80, 160, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): BinarizeConv2d(80, 160, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(160, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): BinarizeConv2d(160, 320, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)\n",
      "  (bn2): BatchNorm1d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (bn3): BatchNorm1d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (logsoftmax): LogSoftmax(dim=-1)\n",
      "  (fc): BinarizeLinear(in_features=320, out_features=80, bias=True)\n",
      "  (adapter_dict): ModuleDict()\n",
      "  (head_bn): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (head): BinarizeLinear(in_features=640, out_features=20, bias=True)\n",
      "  (encoder_net): placeholder()\n",
      "  (uniAdaptNet): Sequential(\n",
      "    (0): uniAdapt_Net(\n",
      "      (layer1): thinBlock3(\n",
      "        (conv2): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=80, bias=False)\n",
      "        (bn2): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(80, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (layer2): thinBlock3(\n",
      "        (conv2): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=160, bias=False)\n",
      "        (bn2): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(160, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (layer3): thinBlock3(\n",
      "        (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=320, bias=False)\n",
      "        (bn2): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (pool1): AdaptiveAvgPool2d(output_size=16)\n",
      "      (pool2): AdaptiveAvgPool2d(output_size=8)\n",
      "      (pool3): AdaptiveAvgPool2d(output_size=1)\n",
      "    )\n",
      "    (1): Flatten(start_dim=1, end_dim=-1)\n",
      "  )\n",
      ")\n",
      "No lr scheduler\n",
      "Optimizer Set to New: {'state': {}, 'param_groups': [{'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': False, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]}]}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:36rkt0v3) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Current Best Acc</td><td>▁▅▅▆▆▇▇▇▇▇▇▇▇███████████████████████████</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>epoch time (s)</td><td>▁▇███████████▇███████▇██▇█████████▇█▇███</td></tr><tr><td>lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_accuracy</td><td>▁▅▅▆▆▇▇▇▇▇▇▇▇███████▄▅▅▅▅▅▅▅▅▆▆▆▅▆▆▆▆▆▆▆</td></tr><tr><td>test_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>training_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Best Accuracy</td><td>0.6275</td></tr><tr><td>Current Best Acc</td><td>0.6275</td></tr><tr><td>Total Time (hours)</td><td>0.41059</td></tr><tr><td>epoch</td><td>75</td></tr><tr><td>epoch time (s)</td><td>9.91689</td></tr><tr><td>lr</td><td>0.001</td></tr><tr><td>test_accuracy</td><td>0.4455</td></tr><tr><td>test_loss</td><td>1.82116</td></tr><tr><td>training_loss</td><td>1.8433</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">tusken-speeder-11</strong>: <a href=\"https://wandb.ai/johnny_suu/Cifar100_UniAdapt_Pruning/runs/36rkt0v3\" target=\"_blank\">https://wandb.ai/johnny_suu/Cifar100_UniAdapt_Pruning/runs/36rkt0v3</a><br/>Synced 5 W&B file(s), 1 media file(s), 1 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230505_111135-36rkt0v3\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:36rkt0v3). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bc876aaa08c4f3fa66f1730f13e3881",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01691666666883975, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\John Su\\Downloads\\SydneyUni\\2022\\thesis\\Thesis\\wandb\\run-20230505_112413-zp1cfzvc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/johnny_suu/Cifar100_UniAdapt_Pruning/runs/zp1cfzvc\" target=\"_blank\">carbonite-shuttle-12</a></strong> to <a href=\"https://wandb.ai/johnny_suu/Cifar100_UniAdapt_Pruning\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "Project Name: Cifar100_UniAdapt_Pruning, Run Name Pruned_Conv2_80%_Conv3_99.00% \n",
      "\n",
      "\n",
      "Run Start : 2023-05-05 11-24-13\n",
      "start_epoch : 0\n",
      "initial_lr : 0.001\n",
      "batch_size : 64\n",
      "epochs : 75\n",
      "epoch_chkpts : []\n",
      "device : cuda:0\n",
      "criterion : CrossEntropyLoss()\n",
      "optimizer : Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: False\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "seed : None\n",
      "model_architecture : <class 'NN_Thesis.nn_classes.BNN_Resnet_UniAdapt'>\n",
      "binerised_training : True\n",
      "Number of Elements : 4547660\n",
      "Initial accuracy:\n",
      "Test Accuracy : 4.9%, Test Loss: 55.81345058854218\n",
      "best_acc.pth saved!\n",
      "Test Accuracy : 3.9%, Test Loss: 53.92922103404999\n",
      "epoch: 1 average loss: 2.449\n",
      "Test Accuracy : 33.7%, Test Loss: 2.234038934111595\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 2 average loss: 2.159\n",
      "Test Accuracy : 40.2%, Test Loss: 2.0001944713294506\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.84 seconds\n",
      "epoch: 3 average loss: 2.048\n",
      "Test Accuracy : 41.4%, Test Loss: 1.9774995669722557\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.85 seconds\n",
      "epoch: 4 average loss: 1.957\n",
      "Test Accuracy : 46.5%, Test Loss: 1.825952298939228\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 5 average loss: 1.890\n",
      "Test Accuracy : 46.2%, Test Loss: 1.8205648139119148\n",
      "Epoch Time (Training + Test) = 9.82 seconds\n",
      "epoch: 6 average loss: 1.820\n",
      "Test Accuracy : 48.4%, Test Loss: 1.7368245087563992\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 7 average loss: 1.760\n",
      "Test Accuracy : 49.1%, Test Loss: 1.7030721232295036\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 8 average loss: 1.720\n",
      "Test Accuracy : 49.3%, Test Loss: 1.6836066879332066\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.88 seconds\n",
      "epoch: 9 average loss: 1.673\n",
      "Test Accuracy : 51.5%, Test Loss: 1.5956913977861404\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.82 seconds\n",
      "epoch: 10 average loss: 1.643\n",
      "Test Accuracy : 50.2%, Test Loss: 1.66000110283494\n",
      "Epoch Time (Training + Test) = 9.85 seconds\n",
      "epoch: 11 average loss: 1.616\n",
      "Test Accuracy : 51.5%, Test Loss: 1.5842880718410015\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 12 average loss: 1.560\n",
      "Test Accuracy : 54.4%, Test Loss: 1.5018991753458977\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.87 seconds\n",
      "epoch: 13 average loss: 1.527\n",
      "Test Accuracy : 54.4%, Test Loss: 1.4890443123877048\n",
      "Epoch Time (Training + Test) = 9.81 seconds\n",
      "epoch: 14 average loss: 1.497\n",
      "Test Accuracy : 53.3%, Test Loss: 1.498936489224434\n",
      "Epoch Time (Training + Test) = 9.85 seconds\n",
      "epoch: 15 average loss: 1.468\n",
      "Test Accuracy : 54.9%, Test Loss: 1.4674703814089298\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.88 seconds\n",
      "epoch: 16 average loss: 1.457\n",
      "Test Accuracy : 56.3%, Test Loss: 1.4354334808886051\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.89 seconds\n",
      "epoch: 17 average loss: 1.430\n",
      "Test Accuracy : 56.2%, Test Loss: 1.4134017452597618\n",
      "Epoch Time (Training + Test) = 9.83 seconds\n",
      "epoch: 18 average loss: 1.393\n",
      "Test Accuracy : 56.5%, Test Loss: 1.4025818668305874\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.88 seconds\n",
      "epoch: 19 average loss: 1.393\n",
      "Test Accuracy : 57.1%, Test Loss: 1.3867088742554188\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.83 seconds\n",
      "epoch: 20 average loss: 1.356\n",
      "Test Accuracy : 57.5%, Test Loss: 1.3491306826472282\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.90 seconds\n",
      "epoch: 21 average loss: 1.349\n",
      "Test Accuracy : 58.4%, Test Loss: 1.3266290314495564\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.84 seconds\n",
      "epoch: 22 average loss: 1.314\n",
      "Test Accuracy : 59.2%, Test Loss: 1.3129504062235355\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 23 average loss: 1.304\n",
      "Test Accuracy : 59.8%, Test Loss: 1.2916220165789127\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 24 average loss: 1.279\n",
      "Test Accuracy : 60.2%, Test Loss: 1.2845058515667915\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.84 seconds\n",
      "epoch: 25 average loss: 1.265\n",
      "Test Accuracy : 59.9%, Test Loss: 1.267961671575904\n",
      "Epoch Time (Training + Test) = 9.82 seconds\n",
      "epoch: 26 average loss: 1.256\n",
      "Test Accuracy : 60.0%, Test Loss: 1.2799797095358372\n",
      "Epoch Time (Training + Test) = 9.88 seconds\n",
      "epoch: 27 average loss: 1.245\n",
      "Test Accuracy : 58.5%, Test Loss: 1.3522018566727638\n",
      "Epoch Time (Training + Test) = 9.84 seconds\n",
      "epoch: 28 average loss: 1.231\n",
      "Test Accuracy : 59.9%, Test Loss: 1.3028235863894224\n",
      "Epoch Time (Training + Test) = 9.87 seconds\n",
      "epoch: 29 average loss: 1.216\n",
      "Test Accuracy : 61.1%, Test Loss: 1.2535315565764904\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.88 seconds\n",
      "epoch: 30 average loss: 1.197\n",
      "Test Accuracy : 60.0%, Test Loss: 1.272284310311079\n",
      "Epoch Time (Training + Test) = 9.83 seconds\n",
      "epoch: 31 average loss: 1.189\n",
      "Test Accuracy : 61.2%, Test Loss: 1.2620284892618656\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.90 seconds\n",
      "epoch: 32 average loss: 1.182\n",
      "Test Accuracy : 60.8%, Test Loss: 1.25612922757864\n",
      "Epoch Time (Training + Test) = 9.83 seconds\n",
      "epoch: 33 average loss: 1.158\n",
      "Test Accuracy : 62.1%, Test Loss: 1.2153029832988977\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.87 seconds\n",
      "epoch: 34 average loss: 1.154\n",
      "Test Accuracy : 62.1%, Test Loss: 1.208742557093501\n",
      "Epoch Time (Training + Test) = 9.82 seconds\n",
      "epoch: 35 average loss: 1.156\n",
      "Test Accuracy : 61.9%, Test Loss: 1.2316246330738068\n",
      "Epoch Time (Training + Test) = 9.74 seconds\n",
      "epoch: 36 average loss: 1.136\n",
      "Test Accuracy : 62.4%, Test Loss: 1.2175078298896551\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.87 seconds\n",
      "epoch: 37 average loss: 1.119\n",
      "Test Accuracy : 62.3%, Test Loss: 1.1930252369493246\n",
      "Epoch Time (Training + Test) = 9.84 seconds\n",
      "conv2\n",
      "conv3\n",
      "conv2\n",
      "conv3\n",
      "conv2\n",
      "conv3\n",
      "Pruned [0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99]% of weights at epoch 37\n",
      "epoch: 38 average loss: 2.893\n",
      "Test Accuracy : 25.6%, Test Loss: 2.5957368537783623\n",
      "Epoch Time (Training + Test) = 9.82 seconds\n",
      "epoch: 39 average loss: 2.580\n",
      "Test Accuracy : 28.1%, Test Loss: 2.472226433455944\n",
      "Epoch Time (Training + Test) = 9.78 seconds\n",
      "epoch: 40 average loss: 2.462\n",
      "Test Accuracy : 29.5%, Test Loss: 2.4099004045128822\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 41 average loss: 2.403\n",
      "Test Accuracy : 29.9%, Test Loss: 2.351879373192787\n",
      "Epoch Time (Training + Test) = 9.83 seconds\n",
      "epoch: 42 average loss: 2.346\n",
      "Test Accuracy : 31.0%, Test Loss: 2.2916769608855247\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 43 average loss: 2.309\n",
      "Test Accuracy : 32.6%, Test Loss: 2.2833639681339264\n",
      "Epoch Time (Training + Test) = 9.89 seconds\n",
      "epoch: 44 average loss: 2.281\n",
      "Test Accuracy : 33.8%, Test Loss: 2.247421309351921\n",
      "Epoch Time (Training + Test) = 9.87 seconds\n",
      "epoch: 45 average loss: 2.265\n",
      "Test Accuracy : 33.1%, Test Loss: 2.2209873124957085\n",
      "Epoch Time (Training + Test) = 9.83 seconds\n",
      "epoch: 46 average loss: 2.249\n",
      "Test Accuracy : 34.5%, Test Loss: 2.188268907368183\n",
      "Epoch Time (Training + Test) = 9.83 seconds\n",
      "epoch: 47 average loss: 2.230\n",
      "Test Accuracy : 34.1%, Test Loss: 2.189880181103945\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 48 average loss: 2.212\n",
      "Test Accuracy : 35.4%, Test Loss: 2.173832654953003\n",
      "Epoch Time (Training + Test) = 9.85 seconds\n",
      "epoch: 49 average loss: 2.214\n",
      "Test Accuracy : 35.1%, Test Loss: 2.182135973125696\n",
      "Epoch Time (Training + Test) = 9.84 seconds\n",
      "epoch: 50 average loss: 2.200\n",
      "Test Accuracy : 34.2%, Test Loss: 2.160299025475979\n",
      "Epoch Time (Training + Test) = 9.90 seconds\n",
      "epoch: 51 average loss: 2.199\n",
      "Test Accuracy : 35.6%, Test Loss: 2.131554838269949\n",
      "Epoch Time (Training + Test) = 9.83 seconds\n",
      "epoch: 52 average loss: 2.182\n",
      "Test Accuracy : 34.9%, Test Loss: 2.146817397326231\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 53 average loss: 2.189\n",
      "Test Accuracy : 36.7%, Test Loss: 2.105363357812166\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 54 average loss: 2.186\n",
      "Test Accuracy : 36.4%, Test Loss: 2.1536162942647934\n",
      "Epoch Time (Training + Test) = 9.82 seconds\n",
      "epoch: 55 average loss: 2.176\n",
      "Test Accuracy : 36.1%, Test Loss: 2.126647524535656\n",
      "Epoch Time (Training + Test) = 9.77 seconds\n",
      "epoch: 56 average loss: 2.176\n",
      "Test Accuracy : 35.6%, Test Loss: 2.1308124400675297\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 57 average loss: 2.170\n",
      "Test Accuracy : 37.1%, Test Loss: 2.120161134749651\n",
      "Epoch Time (Training + Test) = 9.82 seconds\n",
      "epoch: 58 average loss: 2.156\n",
      "Test Accuracy : 36.9%, Test Loss: 2.08598306030035\n",
      "Epoch Time (Training + Test) = 9.82 seconds\n",
      "epoch: 59 average loss: 2.162\n",
      "Test Accuracy : 36.8%, Test Loss: 2.086302787065506\n",
      "Epoch Time (Training + Test) = 10.01 seconds\n",
      "epoch: 60 average loss: 2.144\n",
      "Test Accuracy : 35.9%, Test Loss: 2.1189891509711742\n",
      "Epoch Time (Training + Test) = 9.91 seconds\n",
      "epoch: 61 average loss: 2.152\n",
      "Test Accuracy : 36.5%, Test Loss: 2.084589034318924\n",
      "Epoch Time (Training + Test) = 9.82 seconds\n",
      "epoch: 62 average loss: 2.148\n",
      "Test Accuracy : 37.2%, Test Loss: 2.0817497596144676\n",
      "Epoch Time (Training + Test) = 9.85 seconds\n",
      "epoch: 63 average loss: 2.145\n",
      "Test Accuracy : 37.7%, Test Loss: 2.087764650583267\n",
      "Epoch Time (Training + Test) = 9.90 seconds\n",
      "epoch: 64 average loss: 2.139\n",
      "Test Accuracy : 36.6%, Test Loss: 2.073085382580757\n",
      "Epoch Time (Training + Test) = 9.83 seconds\n",
      "epoch: 65 average loss: 2.145\n",
      "Test Accuracy : 37.4%, Test Loss: 2.065961666405201\n",
      "Epoch Time (Training + Test) = 9.85 seconds\n",
      "epoch: 66 average loss: 2.130\n",
      "Test Accuracy : 37.7%, Test Loss: 2.094550095498562\n",
      "Epoch Time (Training + Test) = 9.88 seconds\n",
      "epoch: 67 average loss: 2.130\n",
      "Test Accuracy : 36.3%, Test Loss: 2.0872351713478565\n",
      "Epoch Time (Training + Test) = 9.85 seconds\n",
      "epoch: 68 average loss: 2.135\n",
      "Test Accuracy : 37.4%, Test Loss: 2.069031234830618\n",
      "Epoch Time (Training + Test) = 9.82 seconds\n",
      "epoch: 69 average loss: 2.122\n",
      "Test Accuracy : 37.5%, Test Loss: 2.06321070343256\n",
      "Epoch Time (Training + Test) = 9.88 seconds\n",
      "epoch: 70 average loss: 2.132\n",
      "Test Accuracy : 37.6%, Test Loss: 2.0697111263871193\n",
      "Epoch Time (Training + Test) = 9.84 seconds\n",
      "epoch: 71 average loss: 2.126\n",
      "Test Accuracy : 36.4%, Test Loss: 2.0938598550856113\n",
      "Epoch Time (Training + Test) = 9.85 seconds\n",
      "epoch: 72 average loss: 2.119\n",
      "Test Accuracy : 36.4%, Test Loss: 2.0841469019651413\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 73 average loss: 2.115\n",
      "Test Accuracy : 39.3%, Test Loss: 2.0536224208772182\n",
      "Epoch Time (Training + Test) = 9.82 seconds\n",
      "epoch: 74 average loss: 2.108\n",
      "Test Accuracy : 35.6%, Test Loss: 2.105059139430523\n",
      "Epoch Time (Training + Test) = 9.87 seconds\n",
      "epoch: 75 average loss: 2.109\n",
      "Test Accuracy : 37.6%, Test Loss: 2.0799588784575462\n",
      "Epoch Time (Training + Test) = 9.88 seconds\n",
      "Data Saved to Pruned_Conv2_80%_Conv3_99.00%.csv\n",
      "Finished Training: \n",
      "Total Time 0.410468 hours\n",
      " Average Time Per Epoch 19.70 seconds\n"
     ]
    }
   ],
   "source": [
    "for prune_amount in [0,0.7,0.8,0.9,0.95,0.99]:\n",
    "    \n",
    "    BNN = BNN_Resnet_UniAdapt(80,freeze_pre_weights=True)\n",
    "    BNN.layer3[-1].do_bntan = True\n",
    "    BNN.load_state_dict(state,strict=False)\n",
    "    BNN.freeze()\n",
    "\n",
    "    BNN = BNN.to('cuda:0')\n",
    "    BNN.encoder_net = placeholder()\n",
    "\n",
    "    n1 = uniAdapt_Net([80,160,320],[80,160,320],block = thinBlock3)\n",
    "    n1 = n1.cuda()\n",
    "    n1.train()\n",
    "\n",
    "    adapt_net = nn.Sequential(n1,nn.Flatten())\n",
    "    \n",
    "    adapt_net.to('cuda:0')\n",
    "\n",
    "    BNN.head_bn =  nn.BatchNorm1d(20)\n",
    "    BNN.head =  BinarizeLinear(320 + 320,20)\n",
    "    BNN.add_UniAdapter(adapt_net)\n",
    "    BNN = BNN.to('cuda:0')\n",
    "    BNN.uniAdapt = True\n",
    "    \n",
    "    print(BNN)\n",
    "    trainer = Trainer(BNN,model_name=f'Pruned_Conv2_80%_Conv3_{prune_amount*100 :.2f}%',project_name='Cifar100_UniAdapt_Pruning',binarise=True,classes=train_20.classes)\n",
    "    trainer.lr = 1e-3\n",
    "    trainer.batch_size = 64\n",
    "    trainer.epochs =75\n",
    "    trainer.epoch_chkpts = []\n",
    "    trainer.start_epoch = 0\n",
    "    trainer.amount = [0.8,prune_amount]\n",
    "    # trainer.pruning_rounds = 1 # Only need this for iterative pruning\n",
    "    trainer.model_to_prune = trainer.model.uniAdaptNet[0]\n",
    "    params = nn.ModuleList([BNN.uniAdaptNet,BNN.head_bn,BNN.head]).parameters()\n",
    "    # params = nn.ModuleList([BNN.head_bn,BNN.head]).parameters()\n",
    "    trainer.set_scheduler(None)\n",
    "    trainer.set_optimizer(torch.optim.Adam,params,lr = trainer.lr)\n",
    "    trainer.train(train20_DL,test20_DL,prune_strat='oneshot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "88384693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BNN_Resnet_UniAdapt(\n",
      "  (conv1): BinarizeConv2d(3, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (bn1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "  (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(80, 160, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): BinarizeConv2d(80, 160, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(160, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): BinarizeConv2d(160, 320, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)\n",
      "  (bn2): BatchNorm1d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (bn3): BatchNorm1d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (logsoftmax): LogSoftmax(dim=-1)\n",
      "  (fc): BinarizeLinear(in_features=320, out_features=80, bias=True)\n",
      "  (adapter_dict): ModuleDict()\n",
      "  (head_bn): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (head): Linear(in_features=640, out_features=20, bias=True)\n",
      "  (encoder_net): placeholder()\n",
      "  (uniAdaptNet): Sequential(\n",
      "    (0): uniAdapt_Net(\n",
      "      (layer1): thinBlock3(\n",
      "        (conv2): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=80, bias=False)\n",
      "        (bn2): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(80, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (layer2): thinBlock3(\n",
      "        (conv2): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=160, bias=False)\n",
      "        (bn2): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(160, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (layer3): thinBlock3(\n",
      "        (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=320, bias=False)\n",
      "        (bn2): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (pool1): AdaptiveAvgPool2d(output_size=16)\n",
      "      (pool2): AdaptiveAvgPool2d(output_size=8)\n",
      "      (pool3): AdaptiveAvgPool2d(output_size=1)\n",
      "    )\n",
      "    (1): Flatten(start_dim=1, end_dim=-1)\n",
      "  )\n",
      ")\n",
      "No lr scheduler\n",
      "Optimizer Set to New: {'state': {}, 'param_groups': [{'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': False, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]}]}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:zp1cfzvc) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Current Best Acc</td><td>▁▅▅▆▆▇▇▇▇▇▇█████████████████████████████</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>epoch time (s)</td><td>▁▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇▇▇▇█▇█▇▇▇▇▇▇</td></tr><tr><td>lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_accuracy</td><td>▁▅▅▆▆▇▇▇▇▇▇█████████▄▄▄▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅</td></tr><tr><td>test_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>training_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Best Accuracy</td><td>0.624</td></tr><tr><td>Current Best Acc</td><td>0.624</td></tr><tr><td>Total Time (hours)</td><td>0.41047</td></tr><tr><td>epoch</td><td>75</td></tr><tr><td>epoch time (s)</td><td>9.88427</td></tr><tr><td>lr</td><td>0.001</td></tr><tr><td>test_accuracy</td><td>0.376</td></tr><tr><td>test_loss</td><td>2.07996</td></tr><tr><td>training_loss</td><td>2.10891</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">carbonite-shuttle-12</strong>: <a href=\"https://wandb.ai/johnny_suu/Cifar100_UniAdapt_Pruning/runs/zp1cfzvc\" target=\"_blank\">https://wandb.ai/johnny_suu/Cifar100_UniAdapt_Pruning/runs/zp1cfzvc</a><br/>Synced 5 W&B file(s), 1 media file(s), 1 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230505_112413-zp1cfzvc\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:zp1cfzvc). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e58b72e90244e35b7e776caa1d649a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01691666666883975, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\John Su\\Downloads\\SydneyUni\\2022\\thesis\\Thesis\\wandb\\run-20230505_113653-1mfhfzbz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/johnny_suu/Cifar100_UniAdapt_Pruning_LinearHead/runs/1mfhfzbz\" target=\"_blank\">forgotten-cantina-1</a></strong> to <a href=\"https://wandb.ai/johnny_suu/Cifar100_UniAdapt_Pruning_LinearHead\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "Project Name: Cifar100_UniAdapt_Pruning_LinearHead, Run Name Pruned_Conv2_80%_Conv3_0.00% \n",
      "\n",
      "\n",
      "Run Start : 2023-05-05 11-36-53\n",
      "start_epoch : 0\n",
      "initial_lr : 0.001\n",
      "batch_size : 64\n",
      "epochs : 75\n",
      "epoch_chkpts : []\n",
      "device : cuda:0\n",
      "criterion : CrossEntropyLoss()\n",
      "optimizer : Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: False\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "seed : None\n",
      "model_architecture : <class 'NN_Thesis.nn_classes.BNN_Resnet_UniAdapt'>\n",
      "binerised_training : True\n",
      "Number of Elements : 4547660\n",
      "Initial accuracy:\n",
      "Test Accuracy : 5.0%, Test Loss: 3.1094807166202814\n",
      "best_acc.pth saved!\n",
      "Test Accuracy : 4.5%, Test Loss: 3.1012175381183624\n",
      "epoch: 1 average loss: 2.420\n",
      "Test Accuracy : 37.8%, Test Loss: 2.1577258370816708\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 10.01 seconds\n",
      "epoch: 2 average loss: 2.149\n",
      "Test Accuracy : 41.7%, Test Loss: 1.9917388744652271\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.92 seconds\n",
      "epoch: 3 average loss: 2.016\n",
      "Test Accuracy : 44.0%, Test Loss: 1.8938222602009773\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.88 seconds\n",
      "epoch: 4 average loss: 1.915\n",
      "Test Accuracy : 46.1%, Test Loss: 1.8177323639392853\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.96 seconds\n",
      "epoch: 5 average loss: 1.843\n",
      "Test Accuracy : 50.4%, Test Loss: 1.6777581125497818\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.92 seconds\n",
      "epoch: 6 average loss: 1.766\n",
      "Test Accuracy : 51.4%, Test Loss: 1.650561485439539\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.98 seconds\n",
      "epoch: 7 average loss: 1.716\n",
      "Test Accuracy : 52.8%, Test Loss: 1.6030500456690788\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.94 seconds\n",
      "epoch: 8 average loss: 1.659\n",
      "Test Accuracy : 51.7%, Test Loss: 1.588821042329073\n",
      "Epoch Time (Training + Test) = 9.83 seconds\n",
      "epoch: 9 average loss: 1.618\n",
      "Test Accuracy : 53.1%, Test Loss: 1.5405013225972652\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.89 seconds\n",
      "epoch: 10 average loss: 1.578\n",
      "Test Accuracy : 52.4%, Test Loss: 1.536386676132679\n",
      "Epoch Time (Training + Test) = 9.87 seconds\n",
      "epoch: 11 average loss: 1.529\n",
      "Test Accuracy : 54.0%, Test Loss: 1.4668340906500816\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.89 seconds\n",
      "epoch: 12 average loss: 1.493\n",
      "Test Accuracy : 56.0%, Test Loss: 1.4173566587269306\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 10.10 seconds\n",
      "epoch: 13 average loss: 1.458\n",
      "Test Accuracy : 56.0%, Test Loss: 1.3942388966679573\n",
      "Epoch Time (Training + Test) = 9.96 seconds\n",
      "epoch: 14 average loss: 1.422\n",
      "Test Accuracy : 58.8%, Test Loss: 1.3996375761926174\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.88 seconds\n",
      "epoch: 15 average loss: 1.388\n",
      "Test Accuracy : 58.4%, Test Loss: 1.3652716465294361\n",
      "Epoch Time (Training + Test) = 9.99 seconds\n",
      "epoch: 16 average loss: 1.372\n",
      "Test Accuracy : 58.4%, Test Loss: 1.3484164401888847\n",
      "Epoch Time (Training + Test) = 9.96 seconds\n",
      "epoch: 17 average loss: 1.343\n",
      "Test Accuracy : 58.3%, Test Loss: 1.3219882622361183\n",
      "Epoch Time (Training + Test) = 9.92 seconds\n",
      "epoch: 18 average loss: 1.322\n",
      "Test Accuracy : 59.5%, Test Loss: 1.2911239825189114\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.97 seconds\n",
      "epoch: 19 average loss: 1.304\n",
      "Test Accuracy : 60.1%, Test Loss: 1.2835017666220665\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.98 seconds\n",
      "epoch: 20 average loss: 1.256\n",
      "Test Accuracy : 61.5%, Test Loss: 1.2510753311216831\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.93 seconds\n",
      "epoch: 21 average loss: 1.255\n",
      "Test Accuracy : 58.5%, Test Loss: 1.3169420510530472\n",
      "Epoch Time (Training + Test) = 9.84 seconds\n",
      "epoch: 22 average loss: 1.239\n",
      "Test Accuracy : 61.0%, Test Loss: 1.2545230649411678\n",
      "Epoch Time (Training + Test) = 9.89 seconds\n",
      "epoch: 23 average loss: 1.226\n",
      "Test Accuracy : 60.7%, Test Loss: 1.2037433385849\n",
      "Epoch Time (Training + Test) = 9.94 seconds\n",
      "epoch: 24 average loss: 1.196\n",
      "Test Accuracy : 61.6%, Test Loss: 1.2264892403036356\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.95 seconds\n",
      "epoch: 25 average loss: 1.184\n",
      "Test Accuracy : 62.3%, Test Loss: 1.21001598238945\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.91 seconds\n",
      "epoch: 26 average loss: 1.152\n",
      "Test Accuracy : 62.0%, Test Loss: 1.1768988501280546\n",
      "Epoch Time (Training + Test) = 9.95 seconds\n",
      "epoch: 27 average loss: 1.156\n",
      "Test Accuracy : 61.4%, Test Loss: 1.200021505355835\n",
      "Epoch Time (Training + Test) = 9.85 seconds\n",
      "epoch: 28 average loss: 1.133\n",
      "Test Accuracy : 63.3%, Test Loss: 1.1755825709551573\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.94 seconds\n",
      "epoch: 29 average loss: 1.112\n",
      "Test Accuracy : 63.1%, Test Loss: 1.1838923059403896\n",
      "Epoch Time (Training + Test) = 9.85 seconds\n",
      "epoch: 30 average loss: 1.110\n",
      "Test Accuracy : 62.3%, Test Loss: 1.1972210016101599\n",
      "Epoch Time (Training + Test) = 9.84 seconds\n",
      "epoch: 31 average loss: 1.092\n",
      "Test Accuracy : 63.4%, Test Loss: 1.1946635358035564\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.90 seconds\n",
      "epoch: 32 average loss: 1.091\n",
      "Test Accuracy : 63.5%, Test Loss: 1.155311681330204\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.97 seconds\n",
      "epoch: 33 average loss: 1.063\n",
      "Test Accuracy : 63.5%, Test Loss: 1.1704211067408323\n",
      "Epoch Time (Training + Test) = 9.88 seconds\n",
      "epoch: 34 average loss: 1.057\n",
      "Test Accuracy : 64.5%, Test Loss: 1.1384525131434202\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.92 seconds\n",
      "epoch: 35 average loss: 1.037\n",
      "Test Accuracy : 62.7%, Test Loss: 1.2071211114525795\n",
      "Epoch Time (Training + Test) = 9.93 seconds\n",
      "epoch: 36 average loss: 1.024\n",
      "Test Accuracy : 65.0%, Test Loss: 1.1262424923479557\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.96 seconds\n",
      "epoch: 37 average loss: 1.028\n",
      "Test Accuracy : 64.1%, Test Loss: 1.137381136417389\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "conv2\n",
      "conv3\n",
      "conv2\n",
      "conv3\n",
      "conv2\n",
      "conv3\n",
      "Pruned [0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0]% of weights at epoch 37\n",
      "epoch: 38 average loss: 2.037\n",
      "Test Accuracy : 48.4%, Test Loss: 1.7281583175063133\n",
      "Epoch Time (Training + Test) = 9.84 seconds\n",
      "epoch: 39 average loss: 1.703\n",
      "Test Accuracy : 50.4%, Test Loss: 1.653587430715561\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 40 average loss: 1.616\n",
      "Test Accuracy : 51.9%, Test Loss: 1.5677526071667671\n",
      "Epoch Time (Training + Test) = 9.85 seconds\n",
      "epoch: 41 average loss: 1.548\n",
      "Test Accuracy : 54.0%, Test Loss: 1.505071971565485\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 42 average loss: 1.502\n",
      "Test Accuracy : 53.1%, Test Loss: 1.4892678931355476\n",
      "Epoch Time (Training + Test) = 9.91 seconds\n",
      "epoch: 43 average loss: 1.479\n",
      "Test Accuracy : 53.9%, Test Loss: 1.4954357482492924\n",
      "Epoch Time (Training + Test) = 9.98 seconds\n",
      "epoch: 44 average loss: 1.449\n",
      "Test Accuracy : 55.5%, Test Loss: 1.4927132092416286\n",
      "Epoch Time (Training + Test) = 9.88 seconds\n",
      "epoch: 45 average loss: 1.425\n",
      "Test Accuracy : 55.8%, Test Loss: 1.4377594776451588\n",
      "Epoch Time (Training + Test) = 9.87 seconds\n",
      "epoch: 46 average loss: 1.389\n",
      "Test Accuracy : 56.1%, Test Loss: 1.3908976912498474\n",
      "Epoch Time (Training + Test) = 9.85 seconds\n",
      "epoch: 47 average loss: 1.392\n",
      "Test Accuracy : 57.0%, Test Loss: 1.4022468999028206\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 48 average loss: 1.379\n",
      "Test Accuracy : 57.8%, Test Loss: 1.368026502430439\n",
      "Epoch Time (Training + Test) = 9.87 seconds\n",
      "epoch: 49 average loss: 1.363\n",
      "Test Accuracy : 57.3%, Test Loss: 1.3993609994649887\n",
      "Epoch Time (Training + Test) = 9.98 seconds\n",
      "epoch: 50 average loss: 1.344\n",
      "Test Accuracy : 59.0%, Test Loss: 1.3354355562478304\n",
      "Epoch Time (Training + Test) = 9.91 seconds\n",
      "epoch: 51 average loss: 1.329\n",
      "Test Accuracy : 57.3%, Test Loss: 1.377055261284113\n",
      "Epoch Time (Training + Test) = 9.87 seconds\n",
      "epoch: 52 average loss: 1.329\n",
      "Test Accuracy : 58.4%, Test Loss: 1.3748150579631329\n",
      "Epoch Time (Training + Test) = 9.85 seconds\n",
      "epoch: 53 average loss: 1.308\n",
      "Test Accuracy : 59.0%, Test Loss: 1.3415028247982264\n",
      "Epoch Time (Training + Test) = 9.89 seconds\n",
      "epoch: 54 average loss: 1.297\n",
      "Test Accuracy : 58.4%, Test Loss: 1.3425130620598793\n",
      "Epoch Time (Training + Test) = 9.85 seconds\n",
      "epoch: 55 average loss: 1.303\n",
      "Test Accuracy : 59.8%, Test Loss: 1.3227995745837688\n",
      "Epoch Time (Training + Test) = 9.94 seconds\n",
      "epoch: 56 average loss: 1.289\n",
      "Test Accuracy : 58.7%, Test Loss: 1.304634965956211\n",
      "Epoch Time (Training + Test) = 9.91 seconds\n",
      "epoch: 57 average loss: 1.278\n",
      "Test Accuracy : 59.6%, Test Loss: 1.3038285449147224\n",
      "Epoch Time (Training + Test) = 9.92 seconds\n",
      "epoch: 58 average loss: 1.257\n",
      "Test Accuracy : 60.4%, Test Loss: 1.2910882234573364\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 59 average loss: 1.256\n",
      "Test Accuracy : 60.5%, Test Loss: 1.3086480163037777\n",
      "Epoch Time (Training + Test) = 9.84 seconds\n",
      "epoch: 60 average loss: 1.258\n",
      "Test Accuracy : 59.7%, Test Loss: 1.2804317325353622\n",
      "Epoch Time (Training + Test) = 9.88 seconds\n",
      "epoch: 61 average loss: 1.246\n",
      "Test Accuracy : 60.2%, Test Loss: 1.2712919395416975\n",
      "Epoch Time (Training + Test) = 9.85 seconds\n",
      "epoch: 62 average loss: 1.243\n",
      "Test Accuracy : 60.2%, Test Loss: 1.2838749028742313\n",
      "Epoch Time (Training + Test) = 9.85 seconds\n",
      "epoch: 63 average loss: 1.237\n",
      "Test Accuracy : 60.3%, Test Loss: 1.2751974873244762\n",
      "Epoch Time (Training + Test) = 9.85 seconds\n",
      "epoch: 64 average loss: 1.234\n",
      "Test Accuracy : 60.6%, Test Loss: 1.2904744818806648\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 65 average loss: 1.213\n",
      "Test Accuracy : 59.6%, Test Loss: 1.3154618870466948\n",
      "Epoch Time (Training + Test) = 9.87 seconds\n",
      "epoch: 66 average loss: 1.228\n",
      "Test Accuracy : 61.3%, Test Loss: 1.2580859828740358\n",
      "Epoch Time (Training + Test) = 9.84 seconds\n",
      "epoch: 67 average loss: 1.201\n",
      "Test Accuracy : 60.2%, Test Loss: 1.2671014908701181\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 68 average loss: 1.209\n",
      "Test Accuracy : 60.4%, Test Loss: 1.2604680638760328\n",
      "Epoch Time (Training + Test) = 9.85 seconds\n",
      "epoch: 69 average loss: 1.204\n",
      "Test Accuracy : 61.7%, Test Loss: 1.2641384471207857\n",
      "Epoch Time (Training + Test) = 9.88 seconds\n",
      "epoch: 70 average loss: 1.195\n",
      "Test Accuracy : 61.4%, Test Loss: 1.2640045620501041\n",
      "Epoch Time (Training + Test) = 9.84 seconds\n",
      "epoch: 71 average loss: 1.195\n",
      "Test Accuracy : 60.2%, Test Loss: 1.26015386544168\n",
      "Epoch Time (Training + Test) = 9.89 seconds\n",
      "epoch: 72 average loss: 1.196\n",
      "Test Accuracy : 60.9%, Test Loss: 1.2583805676549673\n",
      "Epoch Time (Training + Test) = 9.87 seconds\n",
      "epoch: 73 average loss: 1.193\n",
      "Test Accuracy : 62.7%, Test Loss: 1.22784579731524\n",
      "Epoch Time (Training + Test) = 9.94 seconds\n",
      "epoch: 74 average loss: 1.183\n",
      "Test Accuracy : 61.6%, Test Loss: 1.2587508019059896\n",
      "Epoch Time (Training + Test) = 9.92 seconds\n",
      "epoch: 75 average loss: 1.172\n",
      "Test Accuracy : 62.3%, Test Loss: 1.2125561498105526\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "Data Saved to Pruned_Conv2_80%_Conv3_0.00%.csv\n",
      "Finished Training: \n",
      "Total Time 0.412521 hours\n",
      " Average Time Per Epoch 19.80 seconds\n",
      "BNN_Resnet_UniAdapt(\n",
      "  (conv1): BinarizeConv2d(3, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (bn1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "  (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(80, 160, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): BinarizeConv2d(80, 160, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(160, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): BinarizeConv2d(160, 320, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)\n",
      "  (bn2): BatchNorm1d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (bn3): BatchNorm1d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (logsoftmax): LogSoftmax(dim=-1)\n",
      "  (fc): BinarizeLinear(in_features=320, out_features=80, bias=True)\n",
      "  (adapter_dict): ModuleDict()\n",
      "  (head_bn): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (head): Linear(in_features=640, out_features=20, bias=True)\n",
      "  (encoder_net): placeholder()\n",
      "  (uniAdaptNet): Sequential(\n",
      "    (0): uniAdapt_Net(\n",
      "      (layer1): thinBlock3(\n",
      "        (conv2): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=80, bias=False)\n",
      "        (bn2): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(80, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (layer2): thinBlock3(\n",
      "        (conv2): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=160, bias=False)\n",
      "        (bn2): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(160, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (layer3): thinBlock3(\n",
      "        (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=320, bias=False)\n",
      "        (bn2): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (pool1): AdaptiveAvgPool2d(output_size=16)\n",
      "      (pool2): AdaptiveAvgPool2d(output_size=8)\n",
      "      (pool3): AdaptiveAvgPool2d(output_size=1)\n",
      "    )\n",
      "    (1): Flatten(start_dim=1, end_dim=-1)\n",
      "  )\n",
      ")\n",
      "No lr scheduler\n",
      "Optimizer Set to New: {'state': {}, 'param_groups': [{'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': False, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]}]}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1mfhfzbz) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Current Best Acc</td><td>▁▅▆▆▇▇▇▇▇▇▇█████████████████████████████</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>epoch time (s)</td><td>▁█▇██▇▇████▇████▇███▇▇█▇▇▇█▇███▇▇▇▇▇▇▇█▇</td></tr><tr><td>lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_accuracy</td><td>▁▅▆▆▇▇▇▇▇▇▇▇████████▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇██</td></tr><tr><td>test_loss</td><td>█▅▄▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>training_loss</td><td>█▆▄▄▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▄▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Best Accuracy</td><td>0.6495</td></tr><tr><td>Current Best Acc</td><td>0.6495</td></tr><tr><td>Total Time (hours)</td><td>0.41252</td></tr><tr><td>epoch</td><td>75</td></tr><tr><td>epoch time (s)</td><td>9.86308</td></tr><tr><td>lr</td><td>0.001</td></tr><tr><td>test_accuracy</td><td>0.623</td></tr><tr><td>test_loss</td><td>1.21256</td></tr><tr><td>training_loss</td><td>1.17242</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">forgotten-cantina-1</strong>: <a href=\"https://wandb.ai/johnny_suu/Cifar100_UniAdapt_Pruning_LinearHead/runs/1mfhfzbz\" target=\"_blank\">https://wandb.ai/johnny_suu/Cifar100_UniAdapt_Pruning_LinearHead/runs/1mfhfzbz</a><br/>Synced 5 W&B file(s), 1 media file(s), 1 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230505_113653-1mfhfzbz\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1mfhfzbz). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fec5ff35e29647ddaea042e4938c9c8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016933333330477276, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\John Su\\Downloads\\SydneyUni\\2022\\thesis\\Thesis\\wandb\\run-20230505_114937-62l9ybhc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/johnny_suu/Cifar100_UniAdapt_Pruning_LinearHead/runs/62l9ybhc\" target=\"_blank\">tusken-fighter-2</a></strong> to <a href=\"https://wandb.ai/johnny_suu/Cifar100_UniAdapt_Pruning_LinearHead\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "Project Name: Cifar100_UniAdapt_Pruning_LinearHead, Run Name Pruned_Conv2_80%_Conv3_70.00% \n",
      "\n",
      "\n",
      "Run Start : 2023-05-05 11-49-37\n",
      "start_epoch : 0\n",
      "initial_lr : 0.001\n",
      "batch_size : 64\n",
      "epochs : 75\n",
      "epoch_chkpts : []\n",
      "device : cuda:0\n",
      "criterion : CrossEntropyLoss()\n",
      "optimizer : Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: False\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "seed : None\n",
      "model_architecture : <class 'NN_Thesis.nn_classes.BNN_Resnet_UniAdapt'>\n",
      "binerised_training : True\n",
      "Number of Elements : 4547660\n",
      "Initial accuracy:\n",
      "Test Accuracy : 5.4%, Test Loss: 3.0653868845313976\n",
      "best_acc.pth saved!\n",
      "Test Accuracy : 5.2%, Test Loss: 3.064706929028034\n",
      "epoch: 1 average loss: 2.417\n",
      "Test Accuracy : 34.5%, Test Loss: 2.1904855482280254\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.96 seconds\n",
      "epoch: 2 average loss: 2.133\n",
      "Test Accuracy : 40.3%, Test Loss: 1.9907712563872337\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.96 seconds\n",
      "epoch: 3 average loss: 2.023\n",
      "Test Accuracy : 44.1%, Test Loss: 1.9139453694224358\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.88 seconds\n",
      "epoch: 4 average loss: 1.924\n",
      "Test Accuracy : 46.9%, Test Loss: 1.8035865314304829\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.97 seconds\n",
      "epoch: 5 average loss: 1.857\n",
      "Test Accuracy : 48.9%, Test Loss: 1.7220654673874378\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.92 seconds\n",
      "epoch: 6 average loss: 1.786\n",
      "Test Accuracy : 48.6%, Test Loss: 1.7011327221989632\n",
      "Epoch Time (Training + Test) = 9.84 seconds\n",
      "epoch: 7 average loss: 1.741\n",
      "Test Accuracy : 49.0%, Test Loss: 1.6531990319490433\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.88 seconds\n",
      "epoch: 8 average loss: 1.691\n",
      "Test Accuracy : 50.8%, Test Loss: 1.6263336576521397\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.97 seconds\n",
      "epoch: 9 average loss: 1.627\n",
      "Test Accuracy : 53.9%, Test Loss: 1.5423159189522266\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.90 seconds\n",
      "epoch: 10 average loss: 1.597\n",
      "Test Accuracy : 54.4%, Test Loss: 1.4933884181082249\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.91 seconds\n",
      "epoch: 11 average loss: 1.564\n",
      "Test Accuracy : 54.0%, Test Loss: 1.4958166778087616\n",
      "Epoch Time (Training + Test) = 9.84 seconds\n",
      "epoch: 12 average loss: 1.519\n",
      "Test Accuracy : 54.9%, Test Loss: 1.4415036737918854\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.88 seconds\n",
      "epoch: 13 average loss: 1.501\n",
      "Test Accuracy : 56.1%, Test Loss: 1.407579392194748\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.99 seconds\n",
      "epoch: 14 average loss: 1.455\n",
      "Test Accuracy : 57.6%, Test Loss: 1.3770603202283382\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.90 seconds\n",
      "epoch: 15 average loss: 1.428\n",
      "Test Accuracy : 58.1%, Test Loss: 1.3664938993752003\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 10.04 seconds\n",
      "epoch: 16 average loss: 1.399\n",
      "Test Accuracy : 59.3%, Test Loss: 1.3313802145421505\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.91 seconds\n",
      "epoch: 17 average loss: 1.379\n",
      "Test Accuracy : 58.8%, Test Loss: 1.3296837508678436\n",
      "Epoch Time (Training + Test) = 9.94 seconds\n",
      "epoch: 18 average loss: 1.354\n",
      "Test Accuracy : 59.3%, Test Loss: 1.293016493320465\n",
      "Epoch Time (Training + Test) = 9.88 seconds\n",
      "epoch: 19 average loss: 1.323\n",
      "Test Accuracy : 60.1%, Test Loss: 1.289308425039053\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.90 seconds\n",
      "epoch: 20 average loss: 1.304\n",
      "Test Accuracy : 60.6%, Test Loss: 1.2513903081417084\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.89 seconds\n",
      "epoch: 21 average loss: 1.278\n",
      "Test Accuracy : 60.5%, Test Loss: 1.2699474170804024\n",
      "Epoch Time (Training + Test) = 9.82 seconds\n",
      "epoch: 22 average loss: 1.269\n",
      "Test Accuracy : 61.0%, Test Loss: 1.2476895693689585\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.88 seconds\n",
      "epoch: 23 average loss: 1.241\n",
      "Test Accuracy : 61.9%, Test Loss: 1.2273527700453997\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 10.03 seconds\n",
      "epoch: 24 average loss: 1.222\n",
      "Test Accuracy : 62.9%, Test Loss: 1.2127880863845348\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.90 seconds\n",
      "epoch: 25 average loss: 1.217\n",
      "Test Accuracy : 60.8%, Test Loss: 1.2326061930507421\n",
      "Epoch Time (Training + Test) = 9.95 seconds\n",
      "epoch: 26 average loss: 1.187\n",
      "Test Accuracy : 62.5%, Test Loss: 1.2050461154431105\n",
      "Epoch Time (Training + Test) = 9.96 seconds\n",
      "epoch: 27 average loss: 1.175\n",
      "Test Accuracy : 63.0%, Test Loss: 1.1730860471725464\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.98 seconds\n",
      "epoch: 28 average loss: 1.149\n",
      "Test Accuracy : 63.2%, Test Loss: 1.169276786968112\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.93 seconds\n",
      "epoch: 29 average loss: 1.149\n",
      "Test Accuracy : 63.5%, Test Loss: 1.1568026058375835\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 10.05 seconds\n",
      "epoch: 30 average loss: 1.130\n",
      "Test Accuracy : 63.3%, Test Loss: 1.1680426262319088\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 31 average loss: 1.118\n",
      "Test Accuracy : 63.2%, Test Loss: 1.156574446707964\n",
      "Epoch Time (Training + Test) = 9.85 seconds\n",
      "epoch: 32 average loss: 1.105\n",
      "Test Accuracy : 62.3%, Test Loss: 1.1783270947635174\n",
      "Epoch Time (Training + Test) = 9.87 seconds\n",
      "epoch: 33 average loss: 1.089\n",
      "Test Accuracy : 62.9%, Test Loss: 1.1782121863216162\n",
      "Epoch Time (Training + Test) = 9.85 seconds\n",
      "epoch: 34 average loss: 1.079\n",
      "Test Accuracy : 63.7%, Test Loss: 1.1520144511014223\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.89 seconds\n",
      "epoch: 35 average loss: 1.058\n",
      "Test Accuracy : 64.5%, Test Loss: 1.1484098676592112\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 36 average loss: 1.043\n",
      "Test Accuracy : 63.6%, Test Loss: 1.1148629281669855\n",
      "Epoch Time (Training + Test) = 9.93 seconds\n",
      "epoch: 37 average loss: 1.039\n",
      "Test Accuracy : 64.8%, Test Loss: 1.1035099644213915\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.97 seconds\n",
      "conv2\n",
      "conv3\n",
      "conv2\n",
      "conv3\n",
      "conv2\n",
      "conv3\n",
      "Pruned [0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7, 0.8, 0.7]% of weights at epoch 37\n",
      "epoch: 38 average loss: 2.261\n",
      "Test Accuracy : 43.1%, Test Loss: 1.9113690555095673\n",
      "Epoch Time (Training + Test) = 9.84 seconds\n",
      "epoch: 39 average loss: 1.853\n",
      "Test Accuracy : 47.3%, Test Loss: 1.7275206483900547\n",
      "Epoch Time (Training + Test) = 9.87 seconds\n",
      "epoch: 40 average loss: 1.730\n",
      "Test Accuracy : 47.3%, Test Loss: 1.703273307532072\n",
      "Epoch Time (Training + Test) = 9.83 seconds\n",
      "epoch: 41 average loss: 1.670\n",
      "Test Accuracy : 50.9%, Test Loss: 1.6101074889302254\n",
      "Epoch Time (Training + Test) = 9.85 seconds\n",
      "epoch: 42 average loss: 1.623\n",
      "Test Accuracy : 49.8%, Test Loss: 1.5675670206546783\n",
      "Epoch Time (Training + Test) = 9.87 seconds\n",
      "epoch: 43 average loss: 1.591\n",
      "Test Accuracy : 52.1%, Test Loss: 1.5687538236379623\n",
      "Epoch Time (Training + Test) = 9.87 seconds\n",
      "epoch: 44 average loss: 1.564\n",
      "Test Accuracy : 53.3%, Test Loss: 1.51106508821249\n",
      "Epoch Time (Training + Test) = 9.85 seconds\n",
      "epoch: 45 average loss: 1.537\n",
      "Test Accuracy : 51.9%, Test Loss: 1.5322653874754906\n",
      "Epoch Time (Training + Test) = 9.85 seconds\n",
      "epoch: 46 average loss: 1.515\n",
      "Test Accuracy : 54.2%, Test Loss: 1.478730957955122\n",
      "Epoch Time (Training + Test) = 9.87 seconds\n",
      "epoch: 47 average loss: 1.506\n",
      "Test Accuracy : 53.8%, Test Loss: 1.487045656889677\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 48 average loss: 1.484\n",
      "Test Accuracy : 54.1%, Test Loss: 1.4848760105669498\n",
      "Epoch Time (Training + Test) = 9.84 seconds\n",
      "epoch: 49 average loss: 1.476\n",
      "Test Accuracy : 53.4%, Test Loss: 1.4677862077951431\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 50 average loss: 1.459\n",
      "Test Accuracy : 54.6%, Test Loss: 1.467889528721571\n",
      "Epoch Time (Training + Test) = 9.87 seconds\n",
      "epoch: 51 average loss: 1.455\n",
      "Test Accuracy : 54.0%, Test Loss: 1.466868843883276\n",
      "Epoch Time (Training + Test) = 9.85 seconds\n",
      "epoch: 52 average loss: 1.431\n",
      "Test Accuracy : 55.6%, Test Loss: 1.410725373774767\n",
      "Epoch Time (Training + Test) = 9.93 seconds\n",
      "epoch: 53 average loss: 1.432\n",
      "Test Accuracy : 54.6%, Test Loss: 1.4574357271194458\n",
      "Epoch Time (Training + Test) = 9.82 seconds\n",
      "epoch: 54 average loss: 1.421\n",
      "Test Accuracy : 55.5%, Test Loss: 1.4288279190659523\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 55 average loss: 1.410\n",
      "Test Accuracy : 55.4%, Test Loss: 1.4197256155312061\n",
      "Epoch Time (Training + Test) = 9.84 seconds\n",
      "epoch: 56 average loss: 1.419\n",
      "Test Accuracy : 55.6%, Test Loss: 1.4346520453691483\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 57 average loss: 1.406\n",
      "Test Accuracy : 56.4%, Test Loss: 1.4093637354671955\n",
      "Epoch Time (Training + Test) = 9.85 seconds\n",
      "epoch: 58 average loss: 1.384\n",
      "Test Accuracy : 55.1%, Test Loss: 1.3843162469565868\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 59 average loss: 1.378\n",
      "Test Accuracy : 55.7%, Test Loss: 1.417741235345602\n",
      "Epoch Time (Training + Test) = 9.87 seconds\n",
      "epoch: 60 average loss: 1.395\n",
      "Test Accuracy : 55.3%, Test Loss: 1.4039312154054642\n",
      "Epoch Time (Training + Test) = 9.84 seconds\n",
      "epoch: 61 average loss: 1.381\n",
      "Test Accuracy : 56.9%, Test Loss: 1.391823135316372\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 62 average loss: 1.378\n",
      "Test Accuracy : 56.5%, Test Loss: 1.3834557570517063\n",
      "Epoch Time (Training + Test) = 9.85 seconds\n",
      "epoch: 63 average loss: 1.364\n",
      "Test Accuracy : 56.6%, Test Loss: 1.3645130805671215\n",
      "Epoch Time (Training + Test) = 9.88 seconds\n",
      "epoch: 64 average loss: 1.365\n",
      "Test Accuracy : 56.0%, Test Loss: 1.3733555227518082\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 65 average loss: 1.361\n",
      "Test Accuracy : 58.1%, Test Loss: 1.3582462407648563\n",
      "Epoch Time (Training + Test) = 9.87 seconds\n",
      "epoch: 66 average loss: 1.351\n",
      "Test Accuracy : 56.9%, Test Loss: 1.3626175336539745\n",
      "Epoch Time (Training + Test) = 9.84 seconds\n",
      "epoch: 67 average loss: 1.351\n",
      "Test Accuracy : 58.7%, Test Loss: 1.3393215350806713\n",
      "Epoch Time (Training + Test) = 9.87 seconds\n",
      "epoch: 68 average loss: 1.355\n",
      "Test Accuracy : 57.0%, Test Loss: 1.3456598371267319\n",
      "Epoch Time (Training + Test) = 9.85 seconds\n",
      "epoch: 69 average loss: 1.323\n",
      "Test Accuracy : 58.6%, Test Loss: 1.3336846865713596\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 70 average loss: 1.332\n",
      "Test Accuracy : 57.4%, Test Loss: 1.339034017175436\n",
      "Epoch Time (Training + Test) = 9.84 seconds\n",
      "epoch: 71 average loss: 1.315\n",
      "Test Accuracy : 58.1%, Test Loss: 1.341566026210785\n",
      "Epoch Time (Training + Test) = 9.84 seconds\n",
      "epoch: 72 average loss: 1.332\n",
      "Test Accuracy : 58.3%, Test Loss: 1.3359698876738548\n",
      "Epoch Time (Training + Test) = 9.88 seconds\n",
      "epoch: 73 average loss: 1.317\n",
      "Test Accuracy : 59.0%, Test Loss: 1.3284785449504852\n",
      "Epoch Time (Training + Test) = 9.84 seconds\n",
      "epoch: 74 average loss: 1.313\n",
      "Test Accuracy : 58.1%, Test Loss: 1.3546202033758163\n",
      "Epoch Time (Training + Test) = 9.88 seconds\n",
      "epoch: 75 average loss: 1.326\n",
      "Test Accuracy : 58.1%, Test Loss: 1.3436529003083706\n",
      "Epoch Time (Training + Test) = 9.85 seconds\n",
      "Data Saved to Pruned_Conv2_80%_Conv3_70.00%.csv\n",
      "Finished Training: \n",
      "Total Time 0.411971 hours\n",
      " Average Time Per Epoch 19.77 seconds\n",
      "BNN_Resnet_UniAdapt(\n",
      "  (conv1): BinarizeConv2d(3, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (bn1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "  (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(80, 160, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): BinarizeConv2d(80, 160, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(160, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): BinarizeConv2d(160, 320, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)\n",
      "  (bn2): BatchNorm1d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (bn3): BatchNorm1d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (logsoftmax): LogSoftmax(dim=-1)\n",
      "  (fc): BinarizeLinear(in_features=320, out_features=80, bias=True)\n",
      "  (adapter_dict): ModuleDict()\n",
      "  (head_bn): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (head): Linear(in_features=640, out_features=20, bias=True)\n",
      "  (encoder_net): placeholder()\n",
      "  (uniAdaptNet): Sequential(\n",
      "    (0): uniAdapt_Net(\n",
      "      (layer1): thinBlock3(\n",
      "        (conv2): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=80, bias=False)\n",
      "        (bn2): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(80, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (layer2): thinBlock3(\n",
      "        (conv2): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=160, bias=False)\n",
      "        (bn2): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(160, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (layer3): thinBlock3(\n",
      "        (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=320, bias=False)\n",
      "        (bn2): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (pool1): AdaptiveAvgPool2d(output_size=16)\n",
      "      (pool2): AdaptiveAvgPool2d(output_size=8)\n",
      "      (pool3): AdaptiveAvgPool2d(output_size=1)\n",
      "    )\n",
      "    (1): Flatten(start_dim=1, end_dim=-1)\n",
      "  )\n",
      ")\n",
      "No lr scheduler\n",
      "Optimizer Set to New: {'state': {}, 'param_groups': [{'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': False, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]}]}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:62l9ybhc) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Current Best Acc</td><td>▁▄▆▆▆▇▇▇▇▇▇█████████████████████████████</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>epoch time (s)</td><td>▁█▇█▇▇▇███▇▇████▇▇▇█▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_accuracy</td><td>▁▅▆▆▆▇▇▇▇▇██████████▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>test_loss</td><td>█▅▄▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▄▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂</td></tr><tr><td>training_loss</td><td>█▆▄▄▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▅▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Best Accuracy</td><td>0.6475</td></tr><tr><td>Current Best Acc</td><td>0.6475</td></tr><tr><td>Total Time (hours)</td><td>0.41197</td></tr><tr><td>epoch</td><td>75</td></tr><tr><td>epoch time (s)</td><td>9.84891</td></tr><tr><td>lr</td><td>0.001</td></tr><tr><td>test_accuracy</td><td>0.5805</td></tr><tr><td>test_loss</td><td>1.34365</td></tr><tr><td>training_loss</td><td>1.32643</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">tusken-fighter-2</strong>: <a href=\"https://wandb.ai/johnny_suu/Cifar100_UniAdapt_Pruning_LinearHead/runs/62l9ybhc\" target=\"_blank\">https://wandb.ai/johnny_suu/Cifar100_UniAdapt_Pruning_LinearHead/runs/62l9ybhc</a><br/>Synced 5 W&B file(s), 1 media file(s), 1 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230505_114937-62l9ybhc\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:62l9ybhc). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9652bd0d307438eb84b4aa4f9c05767",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016933333330477276, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\John Su\\Downloads\\SydneyUni\\2022\\thesis\\Thesis\\wandb\\run-20230505_120218-884usiel</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/johnny_suu/Cifar100_UniAdapt_Pruning_LinearHead/runs/884usiel\" target=\"_blank\">grievous-wookie-3</a></strong> to <a href=\"https://wandb.ai/johnny_suu/Cifar100_UniAdapt_Pruning_LinearHead\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "Project Name: Cifar100_UniAdapt_Pruning_LinearHead, Run Name Pruned_Conv2_80%_Conv3_80.00% \n",
      "\n",
      "\n",
      "Run Start : 2023-05-05 12-02-18\n",
      "start_epoch : 0\n",
      "initial_lr : 0.001\n",
      "batch_size : 64\n",
      "epochs : 75\n",
      "epoch_chkpts : []\n",
      "device : cuda:0\n",
      "criterion : CrossEntropyLoss()\n",
      "optimizer : Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: False\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "seed : None\n",
      "model_architecture : <class 'NN_Thesis.nn_classes.BNN_Resnet_UniAdapt'>\n",
      "binerised_training : True\n",
      "Number of Elements : 4547660\n",
      "Initial accuracy:\n",
      "Test Accuracy : 5.5%, Test Loss: 3.117642000222662\n",
      "best_acc.pth saved!\n",
      "Test Accuracy : 5.9%, Test Loss: 3.1104999408125877\n",
      "best_acc.pth saved!\n",
      "epoch: 1 average loss: 2.424\n",
      "Test Accuracy : 36.4%, Test Loss: 2.1320014633238316\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.97 seconds\n",
      "epoch: 2 average loss: 2.136\n",
      "Test Accuracy : 40.6%, Test Loss: 2.0071670040488243\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.95 seconds\n",
      "epoch: 3 average loss: 2.020\n",
      "Test Accuracy : 43.9%, Test Loss: 1.8878327570855618\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.94 seconds\n",
      "epoch: 4 average loss: 1.928\n",
      "Test Accuracy : 44.8%, Test Loss: 1.8085118979215622\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 10.09 seconds\n",
      "epoch: 5 average loss: 1.845\n",
      "Test Accuracy : 48.3%, Test Loss: 1.7512902356684208\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.94 seconds\n",
      "epoch: 6 average loss: 1.796\n",
      "Test Accuracy : 50.4%, Test Loss: 1.6657195910811424\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 10.05 seconds\n",
      "epoch: 7 average loss: 1.724\n",
      "Test Accuracy : 51.6%, Test Loss: 1.5875807739794254\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.88 seconds\n",
      "epoch: 8 average loss: 1.675\n",
      "Test Accuracy : 52.5%, Test Loss: 1.578497290611267\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.96 seconds\n",
      "epoch: 9 average loss: 1.630\n",
      "Test Accuracy : 53.0%, Test Loss: 1.4906507432460785\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.92 seconds\n",
      "epoch: 10 average loss: 1.581\n",
      "Test Accuracy : 54.2%, Test Loss: 1.4851751886308193\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 10.18 seconds\n",
      "epoch: 11 average loss: 1.545\n",
      "Test Accuracy : 54.5%, Test Loss: 1.453400608152151\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.95 seconds\n",
      "epoch: 12 average loss: 1.500\n",
      "Test Accuracy : 56.1%, Test Loss: 1.4038998112082481\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.97 seconds\n",
      "epoch: 13 average loss: 1.467\n",
      "Test Accuracy : 57.4%, Test Loss: 1.4103603772819042\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.97 seconds\n",
      "epoch: 14 average loss: 1.439\n",
      "Test Accuracy : 59.4%, Test Loss: 1.3447556123137474\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.97 seconds\n",
      "epoch: 15 average loss: 1.414\n",
      "Test Accuracy : 57.2%, Test Loss: 1.3659012466669083\n",
      "Epoch Time (Training + Test) = 9.92 seconds\n",
      "epoch: 16 average loss: 1.376\n",
      "Test Accuracy : 60.7%, Test Loss: 1.2932516317814589\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.99 seconds\n",
      "epoch: 17 average loss: 1.357\n",
      "Test Accuracy : 59.8%, Test Loss: 1.292608991265297\n",
      "Epoch Time (Training + Test) = 9.92 seconds\n",
      "epoch: 18 average loss: 1.333\n",
      "Test Accuracy : 60.0%, Test Loss: 1.2537143863737583\n",
      "Epoch Time (Training + Test) = 10.04 seconds\n",
      "epoch: 19 average loss: 1.313\n",
      "Test Accuracy : 59.8%, Test Loss: 1.2776572313159704\n",
      "Epoch Time (Training + Test) = 9.89 seconds\n",
      "epoch: 20 average loss: 1.277\n",
      "Test Accuracy : 60.2%, Test Loss: 1.2644839361310005\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 21 average loss: 1.272\n",
      "Test Accuracy : 60.2%, Test Loss: 1.2493336535990238\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 22 average loss: 1.236\n",
      "Test Accuracy : 63.1%, Test Loss: 1.1945565063506365\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.87 seconds\n",
      "epoch: 23 average loss: 1.223\n",
      "Test Accuracy : 61.2%, Test Loss: 1.2309749834239483\n",
      "Epoch Time (Training + Test) = 9.84 seconds\n",
      "epoch: 24 average loss: 1.204\n",
      "Test Accuracy : 62.9%, Test Loss: 1.1830528769642115\n",
      "Epoch Time (Training + Test) = 9.85 seconds\n",
      "epoch: 25 average loss: 1.186\n",
      "Test Accuracy : 62.2%, Test Loss: 1.207997191697359\n",
      "Epoch Time (Training + Test) = 9.97 seconds\n",
      "epoch: 26 average loss: 1.181\n",
      "Test Accuracy : 62.8%, Test Loss: 1.1676198411732912\n",
      "Epoch Time (Training + Test) = 9.93 seconds\n",
      "epoch: 27 average loss: 1.158\n",
      "Test Accuracy : 63.2%, Test Loss: 1.1560814678668976\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.89 seconds\n",
      "epoch: 28 average loss: 1.132\n",
      "Test Accuracy : 63.4%, Test Loss: 1.1607599463313818\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.98 seconds\n",
      "epoch: 29 average loss: 1.129\n",
      "Test Accuracy : 63.1%, Test Loss: 1.1622626781463623\n",
      "Epoch Time (Training + Test) = 9.92 seconds\n",
      "epoch: 30 average loss: 1.119\n",
      "Test Accuracy : 63.1%, Test Loss: 1.1583442371338606\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 31 average loss: 1.097\n",
      "Test Accuracy : 62.8%, Test Loss: 1.161279646679759\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 32 average loss: 1.096\n",
      "Test Accuracy : 64.3%, Test Loss: 1.146924113854766\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.88 seconds\n",
      "epoch: 33 average loss: 1.081\n",
      "Test Accuracy : 63.8%, Test Loss: 1.171232121065259\n",
      "Epoch Time (Training + Test) = 9.83 seconds\n",
      "epoch: 34 average loss: 1.065\n",
      "Test Accuracy : 64.7%, Test Loss: 1.1333673279732466\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.99 seconds\n",
      "epoch: 35 average loss: 1.048\n",
      "Test Accuracy : 63.9%, Test Loss: 1.1240723486989737\n",
      "Epoch Time (Training + Test) = 9.91 seconds\n",
      "epoch: 36 average loss: 1.050\n",
      "Test Accuracy : 65.5%, Test Loss: 1.0995674207806587\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.90 seconds\n",
      "epoch: 37 average loss: 1.035\n",
      "Test Accuracy : 64.3%, Test Loss: 1.12903100438416\n",
      "Epoch Time (Training + Test) = 9.96 seconds\n",
      "conv2\n",
      "conv3\n",
      "conv2\n",
      "conv3\n",
      "conv2\n",
      "conv3\n",
      "Pruned [0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8]% of weights at epoch 37\n",
      "epoch: 38 average loss: 2.414\n",
      "Test Accuracy : 42.1%, Test Loss: 1.9633092992007732\n",
      "Epoch Time (Training + Test) = 9.92 seconds\n",
      "epoch: 39 average loss: 1.927\n",
      "Test Accuracy : 42.6%, Test Loss: 1.8690381832420826\n",
      "Epoch Time (Training + Test) = 9.88 seconds\n",
      "epoch: 40 average loss: 1.792\n",
      "Test Accuracy : 46.9%, Test Loss: 1.7023346647620201\n",
      "Epoch Time (Training + Test) = 9.96 seconds\n",
      "epoch: 41 average loss: 1.736\n",
      "Test Accuracy : 47.8%, Test Loss: 1.6741750314831734\n",
      "Epoch Time (Training + Test) = 9.94 seconds\n",
      "epoch: 42 average loss: 1.677\n",
      "Test Accuracy : 49.8%, Test Loss: 1.63212263956666\n",
      "Epoch Time (Training + Test) = 9.87 seconds\n",
      "epoch: 43 average loss: 1.651\n",
      "Test Accuracy : 50.8%, Test Loss: 1.5844789370894432\n",
      "Epoch Time (Training + Test) = 10.03 seconds\n",
      "epoch: 44 average loss: 1.625\n",
      "Test Accuracy : 51.4%, Test Loss: 1.5710560642182827\n",
      "Epoch Time (Training + Test) = 9.87 seconds\n",
      "epoch: 45 average loss: 1.603\n",
      "Test Accuracy : 52.8%, Test Loss: 1.5389072597026825\n",
      "Epoch Time (Training + Test) = 9.88 seconds\n",
      "epoch: 46 average loss: 1.571\n",
      "Test Accuracy : 52.6%, Test Loss: 1.5156884491443634\n",
      "Epoch Time (Training + Test) = 9.85 seconds\n",
      "epoch: 47 average loss: 1.563\n",
      "Test Accuracy : 53.0%, Test Loss: 1.5337344594299793\n",
      "Epoch Time (Training + Test) = 9.88 seconds\n",
      "epoch: 48 average loss: 1.556\n",
      "Test Accuracy : 54.0%, Test Loss: 1.507349368184805\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 49 average loss: 1.538\n",
      "Test Accuracy : 55.1%, Test Loss: 1.4659851975739002\n",
      "Epoch Time (Training + Test) = 9.84 seconds\n",
      "epoch: 50 average loss: 1.525\n",
      "Test Accuracy : 55.4%, Test Loss: 1.4651478379964828\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 51 average loss: 1.496\n",
      "Test Accuracy : 55.4%, Test Loss: 1.4656410664319992\n",
      "Epoch Time (Training + Test) = 9.84 seconds\n",
      "epoch: 52 average loss: 1.489\n",
      "Test Accuracy : 53.9%, Test Loss: 1.4848743118345737\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 53 average loss: 1.497\n",
      "Test Accuracy : 55.5%, Test Loss: 1.4636992178857327\n",
      "Epoch Time (Training + Test) = 9.85 seconds\n",
      "epoch: 54 average loss: 1.465\n",
      "Test Accuracy : 55.0%, Test Loss: 1.4641116596758366\n",
      "Epoch Time (Training + Test) = 9.85 seconds\n",
      "epoch: 55 average loss: 1.463\n",
      "Test Accuracy : 55.8%, Test Loss: 1.4250529818236828\n",
      "Epoch Time (Training + Test) = 9.89 seconds\n",
      "epoch: 56 average loss: 1.469\n",
      "Test Accuracy : 57.1%, Test Loss: 1.4058250449597836\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 57 average loss: 1.470\n",
      "Test Accuracy : 56.1%, Test Loss: 1.41581492125988\n",
      "Epoch Time (Training + Test) = 9.84 seconds\n",
      "epoch: 58 average loss: 1.447\n",
      "Test Accuracy : 57.6%, Test Loss: 1.3780377954244614\n",
      "Epoch Time (Training + Test) = 9.87 seconds\n",
      "epoch: 59 average loss: 1.442\n",
      "Test Accuracy : 56.5%, Test Loss: 1.391955491155386\n",
      "Epoch Time (Training + Test) = 9.85 seconds\n",
      "epoch: 60 average loss: 1.431\n",
      "Test Accuracy : 56.0%, Test Loss: 1.3909651599824429\n",
      "Epoch Time (Training + Test) = 9.84 seconds\n",
      "epoch: 61 average loss: 1.420\n",
      "Test Accuracy : 57.9%, Test Loss: 1.382601335644722\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 62 average loss: 1.418\n",
      "Test Accuracy : 56.5%, Test Loss: 1.398271705955267\n",
      "Epoch Time (Training + Test) = 9.84 seconds\n",
      "epoch: 63 average loss: 1.414\n",
      "Test Accuracy : 56.8%, Test Loss: 1.3677539937198162\n",
      "Epoch Time (Training + Test) = 9.87 seconds\n",
      "epoch: 64 average loss: 1.418\n",
      "Test Accuracy : 57.1%, Test Loss: 1.3800796046853065\n",
      "Epoch Time (Training + Test) = 9.88 seconds\n",
      "epoch: 65 average loss: 1.406\n",
      "Test Accuracy : 58.1%, Test Loss: 1.3791078887879848\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 66 average loss: 1.392\n",
      "Test Accuracy : 57.3%, Test Loss: 1.3570616766810417\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 67 average loss: 1.402\n",
      "Test Accuracy : 57.6%, Test Loss: 1.345199454575777\n",
      "Epoch Time (Training + Test) = 9.94 seconds\n",
      "epoch: 68 average loss: 1.373\n",
      "Test Accuracy : 58.3%, Test Loss: 1.360687430948019\n",
      "Epoch Time (Training + Test) = 9.89 seconds\n",
      "epoch: 69 average loss: 1.391\n",
      "Test Accuracy : 58.5%, Test Loss: 1.3592340722680092\n",
      "Epoch Time (Training + Test) = 9.90 seconds\n",
      "epoch: 70 average loss: 1.385\n",
      "Test Accuracy : 57.5%, Test Loss: 1.3715047091245651\n",
      "Epoch Time (Training + Test) = 9.93 seconds\n",
      "epoch: 71 average loss: 1.379\n",
      "Test Accuracy : 59.2%, Test Loss: 1.3615193143486977\n",
      "Epoch Time (Training + Test) = 9.98 seconds\n",
      "epoch: 72 average loss: 1.376\n",
      "Test Accuracy : 59.3%, Test Loss: 1.3365917094051838\n",
      "Epoch Time (Training + Test) = 9.89 seconds\n",
      "epoch: 73 average loss: 1.379\n",
      "Test Accuracy : 58.8%, Test Loss: 1.3584935292601585\n",
      "Epoch Time (Training + Test) = 9.85 seconds\n",
      "epoch: 74 average loss: 1.370\n",
      "Test Accuracy : 58.0%, Test Loss: 1.3563967943191528\n",
      "Epoch Time (Training + Test) = 9.88 seconds\n",
      "epoch: 75 average loss: 1.353\n",
      "Test Accuracy : 59.3%, Test Loss: 1.322364415973425\n",
      "Epoch Time (Training + Test) = 9.90 seconds\n",
      "Data Saved to Pruned_Conv2_80%_Conv3_80.00%.csv\n",
      "Finished Training: \n",
      "Total Time 0.412867 hours\n",
      " Average Time Per Epoch 19.82 seconds\n",
      "BNN_Resnet_UniAdapt(\n",
      "  (conv1): BinarizeConv2d(3, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (bn1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "  (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(80, 160, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): BinarizeConv2d(80, 160, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(160, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): BinarizeConv2d(160, 320, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)\n",
      "  (bn2): BatchNorm1d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (bn3): BatchNorm1d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (logsoftmax): LogSoftmax(dim=-1)\n",
      "  (fc): BinarizeLinear(in_features=320, out_features=80, bias=True)\n",
      "  (adapter_dict): ModuleDict()\n",
      "  (head_bn): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (head): Linear(in_features=640, out_features=20, bias=True)\n",
      "  (encoder_net): placeholder()\n",
      "  (uniAdaptNet): Sequential(\n",
      "    (0): uniAdapt_Net(\n",
      "      (layer1): thinBlock3(\n",
      "        (conv2): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=80, bias=False)\n",
      "        (bn2): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(80, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (layer2): thinBlock3(\n",
      "        (conv2): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=160, bias=False)\n",
      "        (bn2): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(160, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (layer3): thinBlock3(\n",
      "        (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=320, bias=False)\n",
      "        (bn2): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (pool1): AdaptiveAvgPool2d(output_size=16)\n",
      "      (pool2): AdaptiveAvgPool2d(output_size=8)\n",
      "      (pool3): AdaptiveAvgPool2d(output_size=1)\n",
      "    )\n",
      "    (1): Flatten(start_dim=1, end_dim=-1)\n",
      "  )\n",
      ")\n",
      "No lr scheduler\n",
      "Optimizer Set to New: {'state': {}, 'param_groups': [{'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': False, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]}]}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:884usiel) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Current Best Acc</td><td>▁▅▅▆▆▇▇▇▇▇▇▇████████████████████████████</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>epoch time (s)</td><td>▁███▇██████▇▇███▇█████▇▇▇▇▇▇▇█▇▇▇▇▇███▇█</td></tr><tr><td>lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_accuracy</td><td>▁▅▅▆▆▇▇▇▇▇▇▇████████▅▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>test_loss</td><td>█▅▄▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂</td></tr><tr><td>training_loss</td><td>█▆▄▄▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▆▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Best Accuracy</td><td>0.6545</td></tr><tr><td>Current Best Acc</td><td>0.6545</td></tr><tr><td>Total Time (hours)</td><td>0.41287</td></tr><tr><td>epoch</td><td>75</td></tr><tr><td>epoch time (s)</td><td>9.89691</td></tr><tr><td>lr</td><td>0.001</td></tr><tr><td>test_accuracy</td><td>0.593</td></tr><tr><td>test_loss</td><td>1.32236</td></tr><tr><td>training_loss</td><td>1.35342</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">grievous-wookie-3</strong>: <a href=\"https://wandb.ai/johnny_suu/Cifar100_UniAdapt_Pruning_LinearHead/runs/884usiel\" target=\"_blank\">https://wandb.ai/johnny_suu/Cifar100_UniAdapt_Pruning_LinearHead/runs/884usiel</a><br/>Synced 5 W&B file(s), 1 media file(s), 1 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230505_120218-884usiel\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:884usiel). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21139f9a279e4cd9bf31072a39064d79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\John Su\\Downloads\\SydneyUni\\2022\\thesis\\Thesis\\wandb\\run-20230505_121502-2em6g0hg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/johnny_suu/Cifar100_UniAdapt_Pruning_LinearHead/runs/2em6g0hg\" target=\"_blank\">imperial-fleet-4</a></strong> to <a href=\"https://wandb.ai/johnny_suu/Cifar100_UniAdapt_Pruning_LinearHead\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "Project Name: Cifar100_UniAdapt_Pruning_LinearHead, Run Name Pruned_Conv2_80%_Conv3_90.00% \n",
      "\n",
      "\n",
      "Run Start : 2023-05-05 12-15-02\n",
      "start_epoch : 0\n",
      "initial_lr : 0.001\n",
      "batch_size : 64\n",
      "epochs : 75\n",
      "epoch_chkpts : []\n",
      "device : cuda:0\n",
      "criterion : CrossEntropyLoss()\n",
      "optimizer : Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: False\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "seed : None\n",
      "model_architecture : <class 'NN_Thesis.nn_classes.BNN_Resnet_UniAdapt'>\n",
      "binerised_training : True\n",
      "Number of Elements : 4547660\n",
      "Initial accuracy:\n",
      "Test Accuracy : 4.7%, Test Loss: 3.0630624901716876\n",
      "best_acc.pth saved!\n",
      "Test Accuracy : 5.3%, Test Loss: 3.0574233382940292\n",
      "best_acc.pth saved!\n",
      "epoch: 1 average loss: 2.462\n",
      "Test Accuracy : 35.9%, Test Loss: 2.183311391621828\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.97 seconds\n",
      "epoch: 2 average loss: 2.147\n",
      "Test Accuracy : 39.2%, Test Loss: 2.018593981862068\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.91 seconds\n",
      "epoch: 3 average loss: 2.033\n",
      "Test Accuracy : 44.1%, Test Loss: 1.9269725419580936\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.87 seconds\n",
      "epoch: 4 average loss: 1.944\n",
      "Test Accuracy : 45.2%, Test Loss: 1.828707531094551\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 10.00 seconds\n",
      "epoch: 5 average loss: 1.872\n",
      "Test Accuracy : 47.8%, Test Loss: 1.7256816886365414\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.90 seconds\n",
      "epoch: 6 average loss: 1.803\n",
      "Test Accuracy : 48.8%, Test Loss: 1.7058145590126514\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.98 seconds\n",
      "epoch: 7 average loss: 1.750\n",
      "Test Accuracy : 48.5%, Test Loss: 1.689399614930153\n",
      "Epoch Time (Training + Test) = 9.92 seconds\n",
      "epoch: 8 average loss: 1.695\n",
      "Test Accuracy : 50.3%, Test Loss: 1.6175283044576645\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.90 seconds\n",
      "epoch: 9 average loss: 1.644\n",
      "Test Accuracy : 52.1%, Test Loss: 1.5679418109357357\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 10.02 seconds\n",
      "epoch: 10 average loss: 1.602\n",
      "Test Accuracy : 53.4%, Test Loss: 1.5173416137695312\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.91 seconds\n",
      "epoch: 11 average loss: 1.562\n",
      "Test Accuracy : 54.0%, Test Loss: 1.478774394840002\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.87 seconds\n",
      "epoch: 12 average loss: 1.525\n",
      "Test Accuracy : 53.1%, Test Loss: 1.511208936572075\n",
      "Epoch Time (Training + Test) = 9.94 seconds\n",
      "epoch: 13 average loss: 1.493\n",
      "Test Accuracy : 56.9%, Test Loss: 1.4017911069095135\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.93 seconds\n",
      "epoch: 14 average loss: 1.446\n",
      "Test Accuracy : 56.2%, Test Loss: 1.4013096876442432\n",
      "Epoch Time (Training + Test) = 9.84 seconds\n",
      "epoch: 15 average loss: 1.429\n",
      "Test Accuracy : 58.1%, Test Loss: 1.3475326411426067\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.89 seconds\n",
      "epoch: 16 average loss: 1.380\n",
      "Test Accuracy : 56.8%, Test Loss: 1.3873975649476051\n",
      "Epoch Time (Training + Test) = 9.94 seconds\n",
      "epoch: 17 average loss: 1.361\n",
      "Test Accuracy : 58.1%, Test Loss: 1.3563253432512283\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.95 seconds\n",
      "epoch: 18 average loss: 1.332\n",
      "Test Accuracy : 58.7%, Test Loss: 1.3135902471840382\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.89 seconds\n",
      "epoch: 19 average loss: 1.323\n",
      "Test Accuracy : 60.2%, Test Loss: 1.3006335590034723\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.97 seconds\n",
      "epoch: 20 average loss: 1.295\n",
      "Test Accuracy : 61.2%, Test Loss: 1.2549694292247295\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.95 seconds\n",
      "epoch: 21 average loss: 1.272\n",
      "Test Accuracy : 60.8%, Test Loss: 1.257908372208476\n",
      "Epoch Time (Training + Test) = 9.84 seconds\n",
      "epoch: 22 average loss: 1.251\n",
      "Test Accuracy : 62.7%, Test Loss: 1.2284083887934685\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.91 seconds\n",
      "epoch: 23 average loss: 1.231\n",
      "Test Accuracy : 60.7%, Test Loss: 1.247930858284235\n",
      "Epoch Time (Training + Test) = 9.94 seconds\n",
      "epoch: 24 average loss: 1.209\n",
      "Test Accuracy : 62.3%, Test Loss: 1.234558641910553\n",
      "Epoch Time (Training + Test) = 9.91 seconds\n",
      "epoch: 25 average loss: 1.192\n",
      "Test Accuracy : 62.5%, Test Loss: 1.2163035590201616\n",
      "Epoch Time (Training + Test) = 9.85 seconds\n",
      "epoch: 26 average loss: 1.181\n",
      "Test Accuracy : 62.9%, Test Loss: 1.1959503572434187\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.92 seconds\n",
      "epoch: 27 average loss: 1.154\n",
      "Test Accuracy : 63.0%, Test Loss: 1.1942973993718624\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.84 seconds\n",
      "epoch: 28 average loss: 1.146\n",
      "Test Accuracy : 62.8%, Test Loss: 1.169516360387206\n",
      "Epoch Time (Training + Test) = 9.84 seconds\n",
      "epoch: 29 average loss: 1.131\n",
      "Test Accuracy : 62.8%, Test Loss: 1.173378236591816\n",
      "Epoch Time (Training + Test) = 9.96 seconds\n",
      "epoch: 30 average loss: 1.121\n",
      "Test Accuracy : 65.0%, Test Loss: 1.152668746188283\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.96 seconds\n",
      "epoch: 31 average loss: 1.105\n",
      "Test Accuracy : 64.2%, Test Loss: 1.1290047504007816\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 32 average loss: 1.108\n",
      "Test Accuracy : 62.9%, Test Loss: 1.1443992033600807\n",
      "Epoch Time (Training + Test) = 9.83 seconds\n",
      "epoch: 33 average loss: 1.069\n",
      "Test Accuracy : 64.3%, Test Loss: 1.1384961903095245\n",
      "Epoch Time (Training + Test) = 9.88 seconds\n",
      "epoch: 34 average loss: 1.059\n",
      "Test Accuracy : 65.1%, Test Loss: 1.1059721745550632\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.96 seconds\n",
      "epoch: 35 average loss: 1.060\n",
      "Test Accuracy : 63.0%, Test Loss: 1.1578152067959309\n",
      "Epoch Time (Training + Test) = 9.88 seconds\n",
      "epoch: 36 average loss: 1.052\n",
      "Test Accuracy : 65.6%, Test Loss: 1.1133058611303568\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.96 seconds\n",
      "epoch: 37 average loss: 1.021\n",
      "Test Accuracy : 66.0%, Test Loss: 1.0944251362234354\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.88 seconds\n",
      "conv2\n",
      "conv3\n",
      "conv2\n",
      "conv3\n",
      "conv2\n",
      "conv3\n",
      "Pruned [0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.9]% of weights at epoch 37\n",
      "epoch: 38 average loss: 2.656\n",
      "Test Accuracy : 33.2%, Test Loss: 2.2343051061034203\n",
      "Epoch Time (Training + Test) = 9.85 seconds\n",
      "epoch: 39 average loss: 2.102\n",
      "Test Accuracy : 43.0%, Test Loss: 1.9500151574611664\n",
      "Epoch Time (Training + Test) = 10.00 seconds\n",
      "epoch: 40 average loss: 1.971\n",
      "Test Accuracy : 43.6%, Test Loss: 1.884196873754263\n",
      "Epoch Time (Training + Test) = 9.88 seconds\n",
      "epoch: 41 average loss: 1.888\n",
      "Test Accuracy : 45.0%, Test Loss: 1.819395799189806\n",
      "Epoch Time (Training + Test) = 9.84 seconds\n",
      "epoch: 42 average loss: 1.829\n",
      "Test Accuracy : 46.2%, Test Loss: 1.784803844988346\n",
      "Epoch Time (Training + Test) = 9.89 seconds\n",
      "epoch: 43 average loss: 1.799\n",
      "Test Accuracy : 47.9%, Test Loss: 1.7369052842259407\n",
      "Epoch Time (Training + Test) = 9.85 seconds\n",
      "epoch: 44 average loss: 1.750\n",
      "Test Accuracy : 48.2%, Test Loss: 1.6952912621200085\n",
      "Epoch Time (Training + Test) = 9.89 seconds\n",
      "epoch: 45 average loss: 1.732\n",
      "Test Accuracy : 50.0%, Test Loss: 1.6702481545507908\n",
      "Epoch Time (Training + Test) = 9.94 seconds\n",
      "epoch: 46 average loss: 1.719\n",
      "Test Accuracy : 50.3%, Test Loss: 1.650296039879322\n",
      "Epoch Time (Training + Test) = 9.88 seconds\n",
      "epoch: 47 average loss: 1.677\n",
      "Test Accuracy : 49.2%, Test Loss: 1.6622227504849434\n",
      "Epoch Time (Training + Test) = 9.91 seconds\n",
      "epoch: 48 average loss: 1.686\n",
      "Test Accuracy : 49.1%, Test Loss: 1.6706646867096424\n",
      "Epoch Time (Training + Test) = 9.88 seconds\n",
      "epoch: 49 average loss: 1.667\n",
      "Test Accuracy : 49.6%, Test Loss: 1.6229904405772686\n",
      "Epoch Time (Training + Test) = 9.82 seconds\n",
      "epoch: 50 average loss: 1.638\n",
      "Test Accuracy : 51.0%, Test Loss: 1.6219742149114609\n",
      "Epoch Time (Training + Test) = 9.88 seconds\n",
      "epoch: 51 average loss: 1.643\n",
      "Test Accuracy : 50.9%, Test Loss: 1.5984147302806377\n",
      "Epoch Time (Training + Test) = 9.89 seconds\n",
      "epoch: 52 average loss: 1.641\n",
      "Test Accuracy : 50.8%, Test Loss: 1.606361385434866\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 53 average loss: 1.636\n",
      "Test Accuracy : 51.4%, Test Loss: 1.6100514084100723\n",
      "Epoch Time (Training + Test) = 9.84 seconds\n",
      "epoch: 54 average loss: 1.611\n",
      "Test Accuracy : 51.7%, Test Loss: 1.5747445411980152\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 55 average loss: 1.595\n",
      "Test Accuracy : 52.3%, Test Loss: 1.5567559264600277\n",
      "Epoch Time (Training + Test) = 9.88 seconds\n",
      "epoch: 56 average loss: 1.609\n",
      "Test Accuracy : 52.3%, Test Loss: 1.557641126215458\n",
      "Epoch Time (Training + Test) = 9.87 seconds\n",
      "epoch: 57 average loss: 1.587\n",
      "Test Accuracy : 51.5%, Test Loss: 1.5533473528921604\n",
      "Epoch Time (Training + Test) = 9.92 seconds\n",
      "epoch: 58 average loss: 1.578\n",
      "Test Accuracy : 52.5%, Test Loss: 1.5540761910378933\n",
      "Epoch Time (Training + Test) = 9.94 seconds\n",
      "epoch: 59 average loss: 1.572\n",
      "Test Accuracy : 52.9%, Test Loss: 1.5192516893148422\n",
      "Epoch Time (Training + Test) = 9.87 seconds\n",
      "epoch: 60 average loss: 1.570\n",
      "Test Accuracy : 53.9%, Test Loss: 1.5161746516823769\n",
      "Epoch Time (Training + Test) = 9.85 seconds\n",
      "epoch: 61 average loss: 1.565\n",
      "Test Accuracy : 53.3%, Test Loss: 1.5212153568863869\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 62 average loss: 1.561\n",
      "Test Accuracy : 53.6%, Test Loss: 1.5191513411700726\n",
      "Epoch Time (Training + Test) = 9.84 seconds\n",
      "epoch: 63 average loss: 1.545\n",
      "Test Accuracy : 53.1%, Test Loss: 1.5454201400279999\n",
      "Epoch Time (Training + Test) = 9.89 seconds\n",
      "epoch: 64 average loss: 1.538\n",
      "Test Accuracy : 52.5%, Test Loss: 1.5297776497900486\n",
      "Epoch Time (Training + Test) = 9.87 seconds\n",
      "epoch: 65 average loss: 1.547\n",
      "Test Accuracy : 53.4%, Test Loss: 1.5034929439425468\n",
      "Epoch Time (Training + Test) = 9.93 seconds\n",
      "epoch: 66 average loss: 1.521\n",
      "Test Accuracy : 53.4%, Test Loss: 1.5071774385869503\n",
      "Epoch Time (Training + Test) = 9.92 seconds\n",
      "epoch: 67 average loss: 1.534\n",
      "Test Accuracy : 53.8%, Test Loss: 1.5045490749180317\n",
      "Epoch Time (Training + Test) = 9.89 seconds\n",
      "epoch: 68 average loss: 1.529\n",
      "Test Accuracy : 54.6%, Test Loss: 1.4993670247495174\n",
      "Epoch Time (Training + Test) = 9.85 seconds\n",
      "epoch: 69 average loss: 1.507\n",
      "Test Accuracy : 54.4%, Test Loss: 1.4928301945328712\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 70 average loss: 1.510\n",
      "Test Accuracy : 53.6%, Test Loss: 1.4880362302064896\n",
      "Epoch Time (Training + Test) = 9.88 seconds\n",
      "epoch: 71 average loss: 1.515\n",
      "Test Accuracy : 54.8%, Test Loss: 1.4747797884047031\n",
      "Epoch Time (Training + Test) = 9.84 seconds\n",
      "epoch: 72 average loss: 1.505\n",
      "Test Accuracy : 55.4%, Test Loss: 1.474656142294407\n",
      "Epoch Time (Training + Test) = 9.87 seconds\n",
      "epoch: 73 average loss: 1.518\n",
      "Test Accuracy : 54.9%, Test Loss: 1.470108613371849\n",
      "Epoch Time (Training + Test) = 9.84 seconds\n",
      "epoch: 74 average loss: 1.501\n",
      "Test Accuracy : 55.4%, Test Loss: 1.4662172868847847\n",
      "Epoch Time (Training + Test) = 9.89 seconds\n",
      "epoch: 75 average loss: 1.487\n",
      "Test Accuracy : 54.9%, Test Loss: 1.473265178501606\n",
      "Epoch Time (Training + Test) = 9.88 seconds\n",
      "Data Saved to Pruned_Conv2_80%_Conv3_90.00%.csv\n",
      "Finished Training: \n",
      "Total Time 0.412327 hours\n",
      " Average Time Per Epoch 19.79 seconds\n",
      "BNN_Resnet_UniAdapt(\n",
      "  (conv1): BinarizeConv2d(3, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (bn1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "  (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(80, 160, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): BinarizeConv2d(80, 160, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(160, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): BinarizeConv2d(160, 320, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)\n",
      "  (bn2): BatchNorm1d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (bn3): BatchNorm1d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (logsoftmax): LogSoftmax(dim=-1)\n",
      "  (fc): BinarizeLinear(in_features=320, out_features=80, bias=True)\n",
      "  (adapter_dict): ModuleDict()\n",
      "  (head_bn): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (head): Linear(in_features=640, out_features=20, bias=True)\n",
      "  (encoder_net): placeholder()\n",
      "  (uniAdaptNet): Sequential(\n",
      "    (0): uniAdapt_Net(\n",
      "      (layer1): thinBlock3(\n",
      "        (conv2): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=80, bias=False)\n",
      "        (bn2): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(80, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (layer2): thinBlock3(\n",
      "        (conv2): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=160, bias=False)\n",
      "        (bn2): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(160, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (layer3): thinBlock3(\n",
      "        (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=320, bias=False)\n",
      "        (bn2): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (pool1): AdaptiveAvgPool2d(output_size=16)\n",
      "      (pool2): AdaptiveAvgPool2d(output_size=8)\n",
      "      (pool3): AdaptiveAvgPool2d(output_size=1)\n",
      "    )\n",
      "    (1): Flatten(start_dim=1, end_dim=-1)\n",
      "  )\n",
      ")\n",
      "No lr scheduler\n",
      "Optimizer Set to New: {'state': {}, 'param_groups': [{'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': False, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]}]}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2em6g0hg) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Current Best Acc</td><td>▁▅▅▆▆▆▇▇▇▇▇▇████████████████████████████</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>epoch time (s)</td><td>▁█▇███▇█▇██▇█▇█▇█▇██▇▇▇▇▇▇▇▇▇▇█▇▇▇█▇▇▇▇▇</td></tr><tr><td>lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_accuracy</td><td>▁▅▆▆▆▆▇▇▇▇▇▇▇███████▄▅▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇</td></tr><tr><td>test_loss</td><td>█▅▄▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▅▄▃▃▃▃▃▃▃▃▃▂▂▃▂▂▂▂▂▂</td></tr><tr><td>training_loss</td><td>█▆▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁▇▄▄▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Best Accuracy</td><td>0.66</td></tr><tr><td>Current Best Acc</td><td>0.66</td></tr><tr><td>Total Time (hours)</td><td>0.41233</td></tr><tr><td>epoch</td><td>75</td></tr><tr><td>epoch time (s)</td><td>9.87709</td></tr><tr><td>lr</td><td>0.001</td></tr><tr><td>test_accuracy</td><td>0.5495</td></tr><tr><td>test_loss</td><td>1.47327</td></tr><tr><td>training_loss</td><td>1.48723</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">imperial-fleet-4</strong>: <a href=\"https://wandb.ai/johnny_suu/Cifar100_UniAdapt_Pruning_LinearHead/runs/2em6g0hg\" target=\"_blank\">https://wandb.ai/johnny_suu/Cifar100_UniAdapt_Pruning_LinearHead/runs/2em6g0hg</a><br/>Synced 5 W&B file(s), 1 media file(s), 1 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230505_121502-2em6g0hg\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2em6g0hg). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7c13154488d4e0bb5bf8d4e2e7a49d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01691666666495924, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\John Su\\Downloads\\SydneyUni\\2022\\thesis\\Thesis\\wandb\\run-20230505_122744-2snr15pt</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/johnny_suu/Cifar100_UniAdapt_Pruning_LinearHead/runs/2snr15pt\" target=\"_blank\">clone-master-5</a></strong> to <a href=\"https://wandb.ai/johnny_suu/Cifar100_UniAdapt_Pruning_LinearHead\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "Project Name: Cifar100_UniAdapt_Pruning_LinearHead, Run Name Pruned_Conv2_80%_Conv3_95.00% \n",
      "\n",
      "\n",
      "Run Start : 2023-05-05 12-27-44\n",
      "start_epoch : 0\n",
      "initial_lr : 0.001\n",
      "batch_size : 64\n",
      "epochs : 75\n",
      "epoch_chkpts : []\n",
      "device : cuda:0\n",
      "criterion : CrossEntropyLoss()\n",
      "optimizer : Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: False\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "seed : None\n",
      "model_architecture : <class 'NN_Thesis.nn_classes.BNN_Resnet_UniAdapt'>\n",
      "binerised_training : True\n",
      "Number of Elements : 4547660\n",
      "Initial accuracy:\n",
      "Test Accuracy : 5.5%, Test Loss: 3.126349543310275\n",
      "best_acc.pth saved!\n",
      "Test Accuracy : 5.9%, Test Loss: 3.1320628225803375\n",
      "best_acc.pth saved!\n",
      "epoch: 1 average loss: 2.432\n",
      "Test Accuracy : 35.4%, Test Loss: 2.1761609613895416\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.93 seconds\n",
      "epoch: 2 average loss: 2.166\n",
      "Test Accuracy : 40.9%, Test Loss: 2.0109386816620827\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 10.01 seconds\n",
      "epoch: 3 average loss: 2.034\n",
      "Test Accuracy : 43.2%, Test Loss: 1.9108281135559082\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.91 seconds\n",
      "epoch: 4 average loss: 1.929\n",
      "Test Accuracy : 46.0%, Test Loss: 1.8149328529834747\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 10.02 seconds\n",
      "epoch: 5 average loss: 1.862\n",
      "Test Accuracy : 46.6%, Test Loss: 1.7831531092524529\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.94 seconds\n",
      "epoch: 6 average loss: 1.785\n",
      "Test Accuracy : 48.1%, Test Loss: 1.6859264969825745\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.96 seconds\n",
      "epoch: 7 average loss: 1.746\n",
      "Test Accuracy : 52.0%, Test Loss: 1.595992773771286\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.95 seconds\n",
      "epoch: 8 average loss: 1.693\n",
      "Test Accuracy : 51.5%, Test Loss: 1.5710940584540367\n",
      "Epoch Time (Training + Test) = 9.93 seconds\n",
      "epoch: 9 average loss: 1.641\n",
      "Test Accuracy : 51.9%, Test Loss: 1.5536855086684227\n",
      "Epoch Time (Training + Test) = 9.92 seconds\n",
      "epoch: 10 average loss: 1.602\n",
      "Test Accuracy : 53.8%, Test Loss: 1.4832155220210552\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.93 seconds\n",
      "epoch: 11 average loss: 1.562\n",
      "Test Accuracy : 55.8%, Test Loss: 1.4232102409005165\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.88 seconds\n",
      "epoch: 12 average loss: 1.501\n",
      "Test Accuracy : 56.5%, Test Loss: 1.417420133948326\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.96 seconds\n",
      "epoch: 13 average loss: 1.481\n",
      "Test Accuracy : 56.7%, Test Loss: 1.376274161040783\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.93 seconds\n",
      "epoch: 14 average loss: 1.447\n",
      "Test Accuracy : 56.2%, Test Loss: 1.3921346738934517\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 15 average loss: 1.407\n",
      "Test Accuracy : 57.8%, Test Loss: 1.3432458937168121\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.98 seconds\n",
      "epoch: 16 average loss: 1.382\n",
      "Test Accuracy : 58.3%, Test Loss: 1.319819089025259\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.93 seconds\n",
      "epoch: 17 average loss: 1.351\n",
      "Test Accuracy : 59.6%, Test Loss: 1.3131700195372105\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.98 seconds\n",
      "epoch: 18 average loss: 1.334\n",
      "Test Accuracy : 61.4%, Test Loss: 1.2779334597289562\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.90 seconds\n",
      "epoch: 19 average loss: 1.310\n",
      "Test Accuracy : 59.0%, Test Loss: 1.3086663372814655\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 20 average loss: 1.282\n",
      "Test Accuracy : 58.3%, Test Loss: 1.3090856224298477\n",
      "Epoch Time (Training + Test) = 9.87 seconds\n",
      "epoch: 21 average loss: 1.271\n",
      "Test Accuracy : 59.8%, Test Loss: 1.254687750712037\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 22 average loss: 1.246\n",
      "Test Accuracy : 61.5%, Test Loss: 1.1958897653967142\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.89 seconds\n",
      "epoch: 23 average loss: 1.206\n",
      "Test Accuracy : 62.7%, Test Loss: 1.1986412536352873\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 10.02 seconds\n",
      "epoch: 24 average loss: 1.203\n",
      "Test Accuracy : 61.7%, Test Loss: 1.203995294868946\n",
      "Epoch Time (Training + Test) = 9.88 seconds\n",
      "epoch: 25 average loss: 1.201\n",
      "Test Accuracy : 60.8%, Test Loss: 1.2277839966118336\n",
      "Epoch Time (Training + Test) = 9.84 seconds\n",
      "epoch: 26 average loss: 1.156\n",
      "Test Accuracy : 61.8%, Test Loss: 1.2004592791199684\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 27 average loss: 1.147\n",
      "Test Accuracy : 64.2%, Test Loss: 1.1432479117065668\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.91 seconds\n",
      "epoch: 28 average loss: 1.129\n",
      "Test Accuracy : 63.6%, Test Loss: 1.1484520565718412\n",
      "Epoch Time (Training + Test) = 9.97 seconds\n",
      "epoch: 29 average loss: 1.120\n",
      "Test Accuracy : 62.8%, Test Loss: 1.1426554657518864\n",
      "Epoch Time (Training + Test) = 9.89 seconds\n",
      "epoch: 30 average loss: 1.113\n",
      "Test Accuracy : 62.1%, Test Loss: 1.1692057847976685\n",
      "Epoch Time (Training + Test) = 9.90 seconds\n",
      "epoch: 31 average loss: 1.087\n",
      "Test Accuracy : 63.0%, Test Loss: 1.1541206799447536\n",
      "Epoch Time (Training + Test) = 9.99 seconds\n",
      "epoch: 32 average loss: 1.090\n",
      "Test Accuracy : 64.9%, Test Loss: 1.1152094956487417\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.89 seconds\n",
      "epoch: 33 average loss: 1.067\n",
      "Test Accuracy : 64.8%, Test Loss: 1.109361233189702\n",
      "Epoch Time (Training + Test) = 9.96 seconds\n",
      "epoch: 34 average loss: 1.047\n",
      "Test Accuracy : 65.1%, Test Loss: 1.089999569579959\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.95 seconds\n",
      "epoch: 35 average loss: 1.047\n",
      "Test Accuracy : 62.1%, Test Loss: 1.1763676051050425\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 36 average loss: 1.026\n",
      "Test Accuracy : 64.4%, Test Loss: 1.1024933904409409\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 37 average loss: 1.029\n",
      "Test Accuracy : 65.0%, Test Loss: 1.0960945133119822\n",
      "Epoch Time (Training + Test) = 10.18 seconds\n",
      "conv2\n",
      "conv3\n",
      "conv2\n",
      "conv3\n",
      "conv2\n",
      "conv3\n",
      "Pruned [0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95, 0.8, 0.95]% of weights at epoch 37\n",
      "epoch: 38 average loss: 2.638\n",
      "Test Accuracy : 33.2%, Test Loss: 2.248834140598774\n",
      "Epoch Time (Training + Test) = 10.05 seconds\n",
      "epoch: 39 average loss: 2.185\n",
      "Test Accuracy : 37.0%, Test Loss: 2.0604810975492\n",
      "Epoch Time (Training + Test) = 9.92 seconds\n",
      "epoch: 40 average loss: 2.052\n",
      "Test Accuracy : 41.2%, Test Loss: 1.923754721879959\n",
      "Epoch Time (Training + Test) = 9.88 seconds\n",
      "epoch: 41 average loss: 1.970\n",
      "Test Accuracy : 42.1%, Test Loss: 1.8956022262573242\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 42 average loss: 1.919\n",
      "Test Accuracy : 43.8%, Test Loss: 1.8398107960820198\n",
      "Epoch Time (Training + Test) = 9.85 seconds\n",
      "epoch: 43 average loss: 1.874\n",
      "Test Accuracy : 45.1%, Test Loss: 1.7815317027270794\n",
      "Epoch Time (Training + Test) = 9.99 seconds\n",
      "epoch: 44 average loss: 1.848\n",
      "Test Accuracy : 46.2%, Test Loss: 1.769480261951685\n",
      "Epoch Time (Training + Test) = 9.91 seconds\n",
      "epoch: 45 average loss: 1.822\n",
      "Test Accuracy : 44.4%, Test Loss: 1.7952584847807884\n",
      "Epoch Time (Training + Test) = 9.89 seconds\n",
      "epoch: 46 average loss: 1.813\n",
      "Test Accuracy : 44.3%, Test Loss: 1.7595278583467007\n",
      "Epoch Time (Training + Test) = 9.85 seconds\n",
      "epoch: 47 average loss: 1.797\n",
      "Test Accuracy : 48.2%, Test Loss: 1.6818800196051598\n",
      "Epoch Time (Training + Test) = 9.84 seconds\n",
      "epoch: 48 average loss: 1.772\n",
      "Test Accuracy : 47.6%, Test Loss: 1.7132025361061096\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 49 average loss: 1.769\n",
      "Test Accuracy : 48.4%, Test Loss: 1.682080939412117\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 50 average loss: 1.749\n",
      "Test Accuracy : 47.9%, Test Loss: 1.6766174770891666\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 51 average loss: 1.738\n",
      "Test Accuracy : 48.2%, Test Loss: 1.6945839747786522\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 52 average loss: 1.734\n",
      "Test Accuracy : 48.5%, Test Loss: 1.639445185661316\n",
      "Epoch Time (Training + Test) = 9.85 seconds\n",
      "epoch: 53 average loss: 1.725\n",
      "Test Accuracy : 49.8%, Test Loss: 1.6508492976427078\n",
      "Epoch Time (Training + Test) = 9.87 seconds\n",
      "epoch: 54 average loss: 1.718\n",
      "Test Accuracy : 48.4%, Test Loss: 1.6438193693757057\n",
      "Epoch Time (Training + Test) = 9.83 seconds\n",
      "epoch: 55 average loss: 1.701\n",
      "Test Accuracy : 50.0%, Test Loss: 1.6409849785268307\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 56 average loss: 1.694\n",
      "Test Accuracy : 49.0%, Test Loss: 1.624467372894287\n",
      "Epoch Time (Training + Test) = 9.87 seconds\n",
      "epoch: 57 average loss: 1.692\n",
      "Test Accuracy : 49.5%, Test Loss: 1.6044845394790173\n",
      "Epoch Time (Training + Test) = 9.85 seconds\n",
      "epoch: 58 average loss: 1.697\n",
      "Test Accuracy : 48.2%, Test Loss: 1.6220966801047325\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 59 average loss: 1.682\n",
      "Test Accuracy : 51.4%, Test Loss: 1.5798072963953018\n",
      "Epoch Time (Training + Test) = 9.87 seconds\n",
      "epoch: 60 average loss: 1.676\n",
      "Test Accuracy : 51.1%, Test Loss: 1.5914659015834332\n",
      "Epoch Time (Training + Test) = 9.83 seconds\n",
      "epoch: 61 average loss: 1.669\n",
      "Test Accuracy : 50.1%, Test Loss: 1.594662170857191\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 62 average loss: 1.656\n",
      "Test Accuracy : 49.9%, Test Loss: 1.594472985714674\n",
      "Epoch Time (Training + Test) = 9.85 seconds\n",
      "epoch: 63 average loss: 1.644\n",
      "Test Accuracy : 50.5%, Test Loss: 1.5969391018152237\n",
      "Epoch Time (Training + Test) = 9.87 seconds\n",
      "epoch: 64 average loss: 1.644\n",
      "Test Accuracy : 51.4%, Test Loss: 1.565678432583809\n",
      "Epoch Time (Training + Test) = 9.89 seconds\n",
      "epoch: 65 average loss: 1.635\n",
      "Test Accuracy : 50.7%, Test Loss: 1.5550165064632893\n",
      "Epoch Time (Training + Test) = 9.87 seconds\n",
      "epoch: 66 average loss: 1.650\n",
      "Test Accuracy : 50.6%, Test Loss: 1.5995298027992249\n",
      "Epoch Time (Training + Test) = 9.95 seconds\n",
      "epoch: 67 average loss: 1.624\n",
      "Test Accuracy : 50.7%, Test Loss: 1.5615785829722881\n",
      "Epoch Time (Training + Test) = 9.93 seconds\n",
      "epoch: 68 average loss: 1.630\n",
      "Test Accuracy : 52.9%, Test Loss: 1.5411249697208405\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 69 average loss: 1.613\n",
      "Test Accuracy : 51.6%, Test Loss: 1.5680951997637749\n",
      "Epoch Time (Training + Test) = 9.84 seconds\n",
      "epoch: 70 average loss: 1.614\n",
      "Test Accuracy : 50.3%, Test Loss: 1.5678357481956482\n",
      "Epoch Time (Training + Test) = 9.87 seconds\n",
      "epoch: 71 average loss: 1.610\n",
      "Test Accuracy : 52.1%, Test Loss: 1.562715407460928\n",
      "Epoch Time (Training + Test) = 9.87 seconds\n",
      "epoch: 72 average loss: 1.611\n",
      "Test Accuracy : 51.6%, Test Loss: 1.5435067266225815\n",
      "Epoch Time (Training + Test) = 9.85 seconds\n",
      "epoch: 73 average loss: 1.615\n",
      "Test Accuracy : 51.8%, Test Loss: 1.5569363683462143\n",
      "Epoch Time (Training + Test) = 9.85 seconds\n",
      "epoch: 74 average loss: 1.625\n",
      "Test Accuracy : 53.6%, Test Loss: 1.5366393700242043\n",
      "Epoch Time (Training + Test) = 10.00 seconds\n",
      "epoch: 75 average loss: 1.605\n",
      "Test Accuracy : 53.3%, Test Loss: 1.5146815814077854\n",
      "Epoch Time (Training + Test) = 9.93 seconds\n",
      "Data Saved to Pruned_Conv2_80%_Conv3_95.00%.csv\n",
      "Finished Training: \n",
      "Total Time 0.412676 hours\n",
      " Average Time Per Epoch 19.81 seconds\n",
      "BNN_Resnet_UniAdapt(\n",
      "  (conv1): BinarizeConv2d(3, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (bn1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "  (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(80, 160, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): BinarizeConv2d(80, 160, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(160, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): BinarizeConv2d(160, 320, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): BinarizeConv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (tanh1): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (conv2): BinarizeConv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (tanh2): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "      (bn2): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)\n",
      "  (bn2): BatchNorm1d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (bn3): BatchNorm1d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (logsoftmax): LogSoftmax(dim=-1)\n",
      "  (fc): BinarizeLinear(in_features=320, out_features=80, bias=True)\n",
      "  (adapter_dict): ModuleDict()\n",
      "  (head_bn): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (head): Linear(in_features=640, out_features=20, bias=True)\n",
      "  (encoder_net): placeholder()\n",
      "  (uniAdaptNet): Sequential(\n",
      "    (0): uniAdapt_Net(\n",
      "      (layer1): thinBlock3(\n",
      "        (conv2): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=80, bias=False)\n",
      "        (bn2): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(80, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (layer2): thinBlock3(\n",
      "        (conv2): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=160, bias=False)\n",
      "        (bn2): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(160, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (layer3): thinBlock3(\n",
      "        (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=320, bias=False)\n",
      "        (bn2): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (pool1): AdaptiveAvgPool2d(output_size=16)\n",
      "      (pool2): AdaptiveAvgPool2d(output_size=8)\n",
      "      (pool3): AdaptiveAvgPool2d(output_size=1)\n",
      "    )\n",
      "    (1): Flatten(start_dim=1, end_dim=-1)\n",
      "  )\n",
      ")\n",
      "No lr scheduler\n",
      "Optimizer Set to New: {'state': {}, 'param_groups': [{'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': False, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]}]}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2snr15pt) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Current Best Acc</td><td>▁▄▅▆▆▆▇▇▇▇██████████████████████████████</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>epoch time (s)</td><td>▁█▇██▇▇▇██▇▇█▇▇█▇▇█▇█▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇▇▇█</td></tr><tr><td>lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_accuracy</td><td>▁▄▅▆▆▆▇▇▇▇▇▇█▇██████▄▅▅▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▇</td></tr><tr><td>test_loss</td><td>█▅▄▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▅▄▄▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▂</td></tr><tr><td>training_loss</td><td>█▆▄▄▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▆▄▄▄▄▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Best Accuracy</td><td>0.6515</td></tr><tr><td>Current Best Acc</td><td>0.6515</td></tr><tr><td>Total Time (hours)</td><td>0.41268</td></tr><tr><td>epoch</td><td>75</td></tr><tr><td>epoch time (s)</td><td>9.93129</td></tr><tr><td>lr</td><td>0.001</td></tr><tr><td>test_accuracy</td><td>0.5335</td></tr><tr><td>test_loss</td><td>1.51468</td></tr><tr><td>training_loss</td><td>1.60462</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">clone-master-5</strong>: <a href=\"https://wandb.ai/johnny_suu/Cifar100_UniAdapt_Pruning_LinearHead/runs/2snr15pt\" target=\"_blank\">https://wandb.ai/johnny_suu/Cifar100_UniAdapt_Pruning_LinearHead/runs/2snr15pt</a><br/>Synced 5 W&B file(s), 1 media file(s), 1 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230505_122744-2snr15pt\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2snr15pt). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "717826718e1443dda74ec65c732d4549",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\John Su\\Downloads\\SydneyUni\\2022\\thesis\\Thesis\\wandb\\run-20230505_124028-1t87egtc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/johnny_suu/Cifar100_UniAdapt_Pruning_LinearHead/runs/1t87egtc\" target=\"_blank\">clone-nerf-herder-6</a></strong> to <a href=\"https://wandb.ai/johnny_suu/Cifar100_UniAdapt_Pruning_LinearHead\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "Project Name: Cifar100_UniAdapt_Pruning_LinearHead, Run Name Pruned_Conv2_80%_Conv3_99.00% \n",
      "\n",
      "\n",
      "Run Start : 2023-05-05 12-40-28\n",
      "start_epoch : 0\n",
      "initial_lr : 0.001\n",
      "batch_size : 64\n",
      "epochs : 75\n",
      "epoch_chkpts : []\n",
      "device : cuda:0\n",
      "criterion : CrossEntropyLoss()\n",
      "optimizer : Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: False\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "seed : None\n",
      "model_architecture : <class 'NN_Thesis.nn_classes.BNN_Resnet_UniAdapt'>\n",
      "binerised_training : True\n",
      "Number of Elements : 4547660\n",
      "Initial accuracy:\n",
      "Test Accuracy : 5.1%, Test Loss: 3.1008763708126774\n",
      "best_acc.pth saved!\n",
      "Test Accuracy : 5.6%, Test Loss: 3.0966564938426018\n",
      "best_acc.pth saved!\n",
      "epoch: 1 average loss: 2.420\n",
      "Test Accuracy : 37.4%, Test Loss: 2.1197276152670383\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 10.00 seconds\n",
      "epoch: 2 average loss: 2.126\n",
      "Test Accuracy : 42.0%, Test Loss: 1.9785063602030277\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.90 seconds\n",
      "epoch: 3 average loss: 1.999\n",
      "Test Accuracy : 42.9%, Test Loss: 1.9278485886752605\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.96 seconds\n",
      "epoch: 4 average loss: 1.918\n",
      "Test Accuracy : 47.2%, Test Loss: 1.7664010785520077\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.94 seconds\n",
      "epoch: 5 average loss: 1.837\n",
      "Test Accuracy : 46.9%, Test Loss: 1.733359131962061\n",
      "Epoch Time (Training + Test) = 9.84 seconds\n",
      "epoch: 6 average loss: 1.790\n",
      "Test Accuracy : 49.6%, Test Loss: 1.6929370872676373\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 10.00 seconds\n",
      "epoch: 7 average loss: 1.730\n",
      "Test Accuracy : 52.6%, Test Loss: 1.6023823283612728\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.94 seconds\n",
      "epoch: 8 average loss: 1.684\n",
      "Test Accuracy : 52.6%, Test Loss: 1.5702363066375256\n",
      "Epoch Time (Training + Test) = 9.87 seconds\n",
      "epoch: 9 average loss: 1.633\n",
      "Test Accuracy : 52.8%, Test Loss: 1.5280574150383472\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.89 seconds\n",
      "epoch: 10 average loss: 1.574\n",
      "Test Accuracy : 54.2%, Test Loss: 1.5009188391268253\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.98 seconds\n",
      "epoch: 11 average loss: 1.546\n",
      "Test Accuracy : 55.0%, Test Loss: 1.4515146054327488\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.94 seconds\n",
      "epoch: 12 average loss: 1.516\n",
      "Test Accuracy : 55.8%, Test Loss: 1.4425045475363731\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.98 seconds\n",
      "epoch: 13 average loss: 1.481\n",
      "Test Accuracy : 55.5%, Test Loss: 1.4322737492620945\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 14 average loss: 1.445\n",
      "Test Accuracy : 56.5%, Test Loss: 1.423114400357008\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.89 seconds\n",
      "epoch: 15 average loss: 1.407\n",
      "Test Accuracy : 58.1%, Test Loss: 1.3682143837213516\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.87 seconds\n",
      "epoch: 16 average loss: 1.392\n",
      "Test Accuracy : 59.0%, Test Loss: 1.3155220579355955\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.98 seconds\n",
      "epoch: 17 average loss: 1.357\n",
      "Test Accuracy : 58.6%, Test Loss: 1.3545544408261776\n",
      "Epoch Time (Training + Test) = 9.91 seconds\n",
      "epoch: 18 average loss: 1.322\n",
      "Test Accuracy : 58.8%, Test Loss: 1.3227480929344893\n",
      "Epoch Time (Training + Test) = 10.00 seconds\n",
      "epoch: 19 average loss: 1.313\n",
      "Test Accuracy : 60.5%, Test Loss: 1.2943980805575848\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.93 seconds\n",
      "epoch: 20 average loss: 1.278\n",
      "Test Accuracy : 59.6%, Test Loss: 1.2908404394984245\n",
      "Epoch Time (Training + Test) = 9.98 seconds\n",
      "epoch: 21 average loss: 1.271\n",
      "Test Accuracy : 60.7%, Test Loss: 1.2354387491941452\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.96 seconds\n",
      "epoch: 22 average loss: 1.245\n",
      "Test Accuracy : 61.6%, Test Loss: 1.2341643273830414\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.97 seconds\n",
      "epoch: 23 average loss: 1.221\n",
      "Test Accuracy : 61.1%, Test Loss: 1.2633346505463123\n",
      "Epoch Time (Training + Test) = 9.92 seconds\n",
      "epoch: 24 average loss: 1.201\n",
      "Test Accuracy : 62.3%, Test Loss: 1.2430168110877275\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.97 seconds\n",
      "epoch: 25 average loss: 1.190\n",
      "Test Accuracy : 61.2%, Test Loss: 1.2725037075579166\n",
      "Epoch Time (Training + Test) = 9.93 seconds\n",
      "epoch: 26 average loss: 1.182\n",
      "Test Accuracy : 63.7%, Test Loss: 1.2061632853001356\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.89 seconds\n",
      "epoch: 27 average loss: 1.168\n",
      "Test Accuracy : 62.1%, Test Loss: 1.2279356233775616\n",
      "Epoch Time (Training + Test) = 9.96 seconds\n",
      "epoch: 28 average loss: 1.147\n",
      "Test Accuracy : 63.1%, Test Loss: 1.1897056177258492\n",
      "Epoch Time (Training + Test) = 9.93 seconds\n",
      "epoch: 29 average loss: 1.142\n",
      "Test Accuracy : 62.3%, Test Loss: 1.190898459404707\n",
      "Epoch Time (Training + Test) = 9.97 seconds\n",
      "epoch: 30 average loss: 1.113\n",
      "Test Accuracy : 63.1%, Test Loss: 1.1703542191535234\n",
      "Epoch Time (Training + Test) = 9.94 seconds\n",
      "epoch: 31 average loss: 1.093\n",
      "Test Accuracy : 63.6%, Test Loss: 1.176782412454486\n",
      "Epoch Time (Training + Test) = 9.88 seconds\n",
      "epoch: 32 average loss: 1.097\n",
      "Test Accuracy : 63.0%, Test Loss: 1.1502431314438581\n",
      "Epoch Time (Training + Test) = 9.85 seconds\n",
      "epoch: 33 average loss: 1.082\n",
      "Test Accuracy : 64.8%, Test Loss: 1.1415198631584644\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.89 seconds\n",
      "epoch: 34 average loss: 1.062\n",
      "Test Accuracy : 65.8%, Test Loss: 1.1301602739840746\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.99 seconds\n",
      "epoch: 35 average loss: 1.060\n",
      "Test Accuracy : 64.1%, Test Loss: 1.131732190027833\n",
      "Epoch Time (Training + Test) = 9.89 seconds\n",
      "epoch: 36 average loss: 1.037\n",
      "Test Accuracy : 64.6%, Test Loss: 1.1231729574501514\n",
      "Epoch Time (Training + Test) = 9.88 seconds\n",
      "epoch: 37 average loss: 1.027\n",
      "Test Accuracy : 64.3%, Test Loss: 1.1470515299588442\n",
      "Epoch Time (Training + Test) = 9.88 seconds\n",
      "conv2\n",
      "conv3\n",
      "conv2\n",
      "conv3\n",
      "conv2\n",
      "conv3\n",
      "Pruned [0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99]% of weights at epoch 37\n",
      "epoch: 38 average loss: 2.835\n",
      "Test Accuracy : 29.6%, Test Loss: 2.404949076473713\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 39 average loss: 2.333\n",
      "Test Accuracy : 33.6%, Test Loss: 2.180178288370371\n",
      "Epoch Time (Training + Test) = 9.84 seconds\n",
      "epoch: 40 average loss: 2.209\n",
      "Test Accuracy : 36.7%, Test Loss: 2.0921567529439926\n",
      "Epoch Time (Training + Test) = 9.83 seconds\n",
      "epoch: 41 average loss: 2.142\n",
      "Test Accuracy : 37.0%, Test Loss: 2.0539979562163353\n",
      "Epoch Time (Training + Test) = 9.85 seconds\n",
      "epoch: 42 average loss: 2.109\n",
      "Test Accuracy : 38.5%, Test Loss: 2.03100523725152\n",
      "Epoch Time (Training + Test) = 9.88 seconds\n",
      "epoch: 43 average loss: 2.083\n",
      "Test Accuracy : 38.5%, Test Loss: 1.9967981688678265\n",
      "Epoch Time (Training + Test) = 9.87 seconds\n",
      "epoch: 44 average loss: 2.055\n",
      "Test Accuracy : 40.7%, Test Loss: 1.9655541740357876\n",
      "Epoch Time (Training + Test) = 9.83 seconds\n",
      "epoch: 45 average loss: 2.039\n",
      "Test Accuracy : 41.1%, Test Loss: 1.9548113979399204\n",
      "Epoch Time (Training + Test) = 9.87 seconds\n",
      "epoch: 46 average loss: 2.032\n",
      "Test Accuracy : 40.4%, Test Loss: 1.961384154856205\n",
      "Epoch Time (Training + Test) = 9.85 seconds\n",
      "epoch: 47 average loss: 2.010\n",
      "Test Accuracy : 40.2%, Test Loss: 1.9505638182163239\n",
      "Epoch Time (Training + Test) = 9.85 seconds\n",
      "epoch: 48 average loss: 1.995\n",
      "Test Accuracy : 42.7%, Test Loss: 1.9018480442464352\n",
      "Epoch Time (Training + Test) = 9.87 seconds\n",
      "epoch: 49 average loss: 1.988\n",
      "Test Accuracy : 41.8%, Test Loss: 1.9102588146924973\n",
      "Epoch Time (Training + Test) = 9.87 seconds\n",
      "epoch: 50 average loss: 1.993\n",
      "Test Accuracy : 41.5%, Test Loss: 1.9261460155248642\n",
      "Epoch Time (Training + Test) = 9.87 seconds\n",
      "epoch: 51 average loss: 1.970\n",
      "Test Accuracy : 42.6%, Test Loss: 1.8983097188174725\n",
      "Epoch Time (Training + Test) = 9.87 seconds\n",
      "epoch: 52 average loss: 1.965\n",
      "Test Accuracy : 42.2%, Test Loss: 1.8760179728269577\n",
      "Epoch Time (Training + Test) = 9.83 seconds\n",
      "epoch: 53 average loss: 1.952\n",
      "Test Accuracy : 41.8%, Test Loss: 1.9172778390347958\n",
      "Epoch Time (Training + Test) = 9.87 seconds\n",
      "epoch: 54 average loss: 1.966\n",
      "Test Accuracy : 41.9%, Test Loss: 1.8636607229709625\n",
      "Epoch Time (Training + Test) = 9.85 seconds\n",
      "epoch: 55 average loss: 1.942\n",
      "Test Accuracy : 43.0%, Test Loss: 1.8905464112758636\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 56 average loss: 1.938\n",
      "Test Accuracy : 42.3%, Test Loss: 1.8893645480275154\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 57 average loss: 1.919\n",
      "Test Accuracy : 44.6%, Test Loss: 1.8426216766238213\n",
      "Epoch Time (Training + Test) = 9.84 seconds\n",
      "epoch: 58 average loss: 1.941\n",
      "Test Accuracy : 42.5%, Test Loss: 1.8748071417212486\n",
      "Epoch Time (Training + Test) = 9.87 seconds\n",
      "epoch: 59 average loss: 1.904\n",
      "Test Accuracy : 41.8%, Test Loss: 1.8735974840819836\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 60 average loss: 1.912\n",
      "Test Accuracy : 42.0%, Test Loss: 1.863450862467289\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 61 average loss: 1.908\n",
      "Test Accuracy : 44.0%, Test Loss: 1.8566932193934917\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 62 average loss: 1.913\n",
      "Test Accuracy : 43.0%, Test Loss: 1.8545168302953243\n",
      "Epoch Time (Training + Test) = 9.85 seconds\n",
      "epoch: 63 average loss: 1.906\n",
      "Test Accuracy : 44.0%, Test Loss: 1.8502116911113262\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 64 average loss: 1.907\n",
      "Test Accuracy : 43.0%, Test Loss: 1.8398845456540585\n",
      "Epoch Time (Training + Test) = 9.87 seconds\n",
      "epoch: 65 average loss: 1.898\n",
      "Test Accuracy : 43.5%, Test Loss: 1.8424220830202103\n",
      "Epoch Time (Training + Test) = 9.85 seconds\n",
      "epoch: 66 average loss: 1.898\n",
      "Test Accuracy : 43.8%, Test Loss: 1.809613648802042\n",
      "Epoch Time (Training + Test) = 9.87 seconds\n",
      "epoch: 67 average loss: 1.887\n",
      "Test Accuracy : 43.9%, Test Loss: 1.8055663518607616\n",
      "Epoch Time (Training + Test) = 9.85 seconds\n",
      "epoch: 68 average loss: 1.891\n",
      "Test Accuracy : 42.8%, Test Loss: 1.84022793546319\n",
      "Epoch Time (Training + Test) = 9.85 seconds\n",
      "epoch: 69 average loss: 1.899\n",
      "Test Accuracy : 42.8%, Test Loss: 1.845329374074936\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 70 average loss: 1.879\n",
      "Test Accuracy : 44.0%, Test Loss: 1.8119866140186787\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 71 average loss: 1.890\n",
      "Test Accuracy : 44.2%, Test Loss: 1.8191355802118778\n",
      "Epoch Time (Training + Test) = 9.88 seconds\n",
      "epoch: 72 average loss: 1.872\n",
      "Test Accuracy : 43.0%, Test Loss: 1.8334437943995\n",
      "Epoch Time (Training + Test) = 9.84 seconds\n",
      "epoch: 73 average loss: 1.871\n",
      "Test Accuracy : 44.3%, Test Loss: 1.809409610927105\n",
      "Epoch Time (Training + Test) = 9.85 seconds\n",
      "epoch: 74 average loss: 1.871\n",
      "Test Accuracy : 45.0%, Test Loss: 1.7987626306712627\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 75 average loss: 1.868\n",
      "Test Accuracy : 44.9%, Test Loss: 1.823228094726801\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "Data Saved to Pruned_Conv2_80%_Conv3_99.00%.csv\n",
      "Finished Training: \n",
      "Total Time 0.412177 hours\n",
      " Average Time Per Epoch 19.78 seconds\n"
     ]
    }
   ],
   "source": [
    "for prune_amount in [0,0.7,0.8,0.9,0.95,0.99]:\n",
    "    \n",
    "    BNN = BNN_Resnet_UniAdapt(80,freeze_pre_weights=True)\n",
    "    BNN.layer3[-1].do_bntan = True\n",
    "    BNN.load_state_dict(state,strict=False)\n",
    "    BNN.freeze()\n",
    "\n",
    "    BNN = BNN.to('cuda:0')\n",
    "    BNN.encoder_net = placeholder()\n",
    "\n",
    "    n1 = uniAdapt_Net([80,160,320],[80,160,320],block = thinBlock3)\n",
    "    n1 = n1.cuda()\n",
    "    n1.train()\n",
    "\n",
    "    adapt_net = nn.Sequential(n1,nn.Flatten())\n",
    "    \n",
    "    adapt_net.to('cuda:0')\n",
    "\n",
    "    BNN.head_bn =  nn.BatchNorm1d(20)\n",
    "    BNN.head =  nn.Linear(320 + 320,20)\n",
    "    BNN.add_UniAdapter(adapt_net)\n",
    "    BNN = BNN.to('cuda:0')\n",
    "    BNN.uniAdapt = True\n",
    "    \n",
    "    print(BNN)\n",
    "    trainer = Trainer(BNN,model_name=f'Pruned_Conv2_80%_Conv3_{prune_amount*100 :.2f}%',project_name='Cifar100_UniAdapt_Pruning_LinearHead',binarise=True,classes=train_20.classes)\n",
    "    trainer.lr = 1e-3\n",
    "    trainer.batch_size = 64\n",
    "    trainer.epochs =75\n",
    "    trainer.epoch_chkpts = []\n",
    "    trainer.start_epoch = 0\n",
    "    trainer.amount = [0.8,prune_amount]\n",
    "    # trainer.pruning_rounds = 1 # Only need this for iterative pruning\n",
    "    trainer.model_to_prune = trainer.model.uniAdaptNet[0]\n",
    "    params = nn.ModuleList([BNN.uniAdaptNet,BNN.head_bn,BNN.head]).parameters()\n",
    "    # params = nn.ModuleList([BNN.head_bn,BNN.head]).parameters()\n",
    "    trainer.set_scheduler(None)\n",
    "    trainer.set_optimizer(torch.optim.Adam,params,lr = trainer.lr)\n",
    "    trainer.train(train20_DL,test20_DL,prune_strat='oneshot')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8bdce884",
   "metadata": {},
   "source": [
    "## Grouping Conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "018123fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None 1\n",
      "No lr scheduler\n",
      "Optimizer Set to New: {'state': {}, 'param_groups': [{'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': False, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]}]}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:13xs5m62) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Current Best Acc</td><td>▁▄▆▇▇▇▇███</td></tr><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>epoch time (s)</td><td>▁▇▇▇▇█████</td></tr><tr><td>lr</td><td>▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_accuracy</td><td>▁▄▆▇▇▇▇███</td></tr><tr><td>test_loss</td><td>█▁▁▁▁▁▁▁▁▁</td></tr><tr><td>training_loss</td><td>█▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Current Best Acc</td><td>0.354</td></tr><tr><td>epoch</td><td>9</td></tr><tr><td>epoch time (s)</td><td>20.10508</td></tr><tr><td>lr</td><td>0.0001</td></tr><tr><td>test_accuracy</td><td>0.354</td></tr><tr><td>test_loss</td><td>2.1867</td></tr><tr><td>training_loss</td><td>2.23972</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">smart-wildflower-2</strong>: <a href=\"https://wandb.ai/johnny_suu/UniAdapt_Grouped_Convolution/runs/13xs5m62\" target=\"_blank\">https://wandb.ai/johnny_suu/UniAdapt_Grouped_Convolution/runs/13xs5m62</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230503_161910-13xs5m62\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:13xs5m62). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4d288398b094d12b33b3f42d210ee27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01693333333435779, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\John Su\\Downloads\\SydneyUni\\2022\\thesis\\Thesis\\wandb\\run-20230503_162249-c4xeryez</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/johnny_suu/UniAdapt_Grouped_Convolution/runs/c4xeryez\" target=\"_blank\">splendid-haze-3</a></strong> to <a href=\"https://wandb.ai/johnny_suu/UniAdapt_Grouped_Convolution\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "Project Name: UniAdapt_Grouped_Convolution, Run Name Grouped_Convolution_PW_Group_1 \n",
      "\n",
      "\n",
      "Run Start : 2023-05-03 16-22-49\n",
      "start_epoch : 0\n",
      "initial_lr : 0.001\n",
      "batch_size : 64\n",
      "epochs : 75\n",
      "epoch_chkpts : []\n",
      "device : cuda:0\n",
      "criterion : CrossEntropyLoss()\n",
      "optimizer : Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: False\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "seed : None\n",
      "model_architecture : <class 'NN_Thesis.nn_classes.BNN_Resnet_UniAdapt'>\n",
      "binerised_training : True\n",
      "Number of Elements : 4547660\n",
      "Initial accuracy:\n",
      "Test Accuracy : 5.2%, Test Loss: 55.6799902703352\n",
      "best_acc.pth saved!\n",
      "Test Accuracy : 5.3%, Test Loss: 57.43521463871002\n",
      "best_acc.pth saved!\n",
      "epoch: 1 average loss: 2.444\n",
      "Test Accuracy : 33.9%, Test Loss: 2.204070322215557\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 10.12 seconds\n",
      "epoch: 2 average loss: 2.164\n",
      "Test Accuracy : 38.5%, Test Loss: 2.0970653109252453\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 10.40 seconds\n",
      "epoch: 3 average loss: 2.042\n",
      "Test Accuracy : 42.2%, Test Loss: 1.9547796994447708\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 10.43 seconds\n",
      "epoch: 4 average loss: 1.947\n",
      "Test Accuracy : 44.9%, Test Loss: 1.8713484518229961\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 10.35 seconds\n",
      "epoch: 5 average loss: 1.885\n",
      "Test Accuracy : 44.0%, Test Loss: 1.845026295632124\n",
      "Epoch Time (Training + Test) = 10.19 seconds\n",
      "epoch: 6 average loss: 1.816\n",
      "Test Accuracy : 47.3%, Test Loss: 1.7567123398184776\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 10.06 seconds\n",
      "epoch: 7 average loss: 1.760\n",
      "Test Accuracy : 49.1%, Test Loss: 1.7133609615266323\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 10.10 seconds\n",
      "epoch: 8 average loss: 1.716\n",
      "Test Accuracy : 50.1%, Test Loss: 1.6672884784638882\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 10.29 seconds\n",
      "epoch: 9 average loss: 1.667\n",
      "Test Accuracy : 50.4%, Test Loss: 1.6000157669186592\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 10.33 seconds\n",
      "epoch: 10 average loss: 1.639\n",
      "Test Accuracy : 51.9%, Test Loss: 1.5756380409002304\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 10.22 seconds\n",
      "epoch: 11 average loss: 1.606\n",
      "Test Accuracy : 52.6%, Test Loss: 1.5551589839160442\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 10.38 seconds\n",
      "epoch: 12 average loss: 1.576\n",
      "Test Accuracy : 54.2%, Test Loss: 1.4847966767847538\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 10.30 seconds\n",
      "epoch: 13 average loss: 1.539\n",
      "Test Accuracy : 53.8%, Test Loss: 1.5033335126936436\n",
      "Epoch Time (Training + Test) = 10.63 seconds\n",
      "epoch: 14 average loss: 1.521\n",
      "Test Accuracy : 55.1%, Test Loss: 1.4607676453888416\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 10.38 seconds\n",
      "epoch: 15 average loss: 1.487\n",
      "Test Accuracy : 54.6%, Test Loss: 1.4724530950188637\n",
      "Epoch Time (Training + Test) = 10.12 seconds\n",
      "epoch: 16 average loss: 1.450\n",
      "Test Accuracy : 55.5%, Test Loss: 1.4295448064804077\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 10.48 seconds\n",
      "epoch: 17 average loss: 1.444\n",
      "Test Accuracy : 57.4%, Test Loss: 1.3918494880199432\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 10.26 seconds\n",
      "epoch: 18 average loss: 1.427\n",
      "Test Accuracy : 56.5%, Test Loss: 1.386968981474638\n",
      "Epoch Time (Training + Test) = 10.28 seconds\n",
      "epoch: 19 average loss: 1.384\n",
      "Test Accuracy : 56.8%, Test Loss: 1.3737864270806313\n",
      "Epoch Time (Training + Test) = 10.14 seconds\n",
      "epoch: 20 average loss: 1.379\n",
      "Test Accuracy : 58.0%, Test Loss: 1.3272007666528225\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 10.09 seconds\n",
      "epoch: 21 average loss: 1.356\n",
      "Test Accuracy : 58.7%, Test Loss: 1.3261624611914158\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 10.32 seconds\n",
      "epoch: 22 average loss: 1.325\n",
      "Test Accuracy : 57.5%, Test Loss: 1.324711300432682\n",
      "Epoch Time (Training + Test) = 10.21 seconds\n",
      "epoch: 23 average loss: 1.319\n",
      "Test Accuracy : 59.3%, Test Loss: 1.2902737818658352\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 10.27 seconds\n",
      "epoch: 24 average loss: 1.300\n",
      "Test Accuracy : 60.5%, Test Loss: 1.2843207605183125\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 10.07 seconds\n",
      "epoch: 25 average loss: 1.276\n",
      "Test Accuracy : 60.3%, Test Loss: 1.2851912137120962\n",
      "Epoch Time (Training + Test) = 10.08 seconds\n",
      "epoch: 26 average loss: 1.259\n",
      "Test Accuracy : 58.8%, Test Loss: 1.309850201010704\n",
      "Epoch Time (Training + Test) = 10.15 seconds\n",
      "epoch: 27 average loss: 1.252\n",
      "Test Accuracy : 58.5%, Test Loss: 1.2937250062823296\n",
      "Epoch Time (Training + Test) = 10.05 seconds\n",
      "epoch: 28 average loss: 1.242\n",
      "Test Accuracy : 61.0%, Test Loss: 1.2476147953420877\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 10.24 seconds\n",
      "epoch: 29 average loss: 1.245\n",
      "Test Accuracy : 61.0%, Test Loss: 1.2424035407602787\n",
      "Epoch Time (Training + Test) = 10.15 seconds\n",
      "epoch: 30 average loss: 1.227\n",
      "Test Accuracy : 61.1%, Test Loss: 1.2376265786588192\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 10.53 seconds\n",
      "epoch: 31 average loss: 1.204\n",
      "Test Accuracy : 60.9%, Test Loss: 1.2555445320904255\n",
      "Epoch Time (Training + Test) = 10.10 seconds\n",
      "epoch: 32 average loss: 1.192\n",
      "Test Accuracy : 62.1%, Test Loss: 1.2007126063108444\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 10.44 seconds\n",
      "epoch: 33 average loss: 1.184\n",
      "Test Accuracy : 60.6%, Test Loss: 1.2330558933317661\n",
      "Epoch Time (Training + Test) = 10.29 seconds\n",
      "epoch: 34 average loss: 1.170\n",
      "Test Accuracy : 60.6%, Test Loss: 1.2653279695659876\n",
      "Epoch Time (Training + Test) = 10.12 seconds\n",
      "epoch: 35 average loss: 1.159\n",
      "Test Accuracy : 61.3%, Test Loss: 1.2348737232387066\n",
      "Epoch Time (Training + Test) = 10.18 seconds\n",
      "epoch: 36 average loss: 1.151\n",
      "Test Accuracy : 62.1%, Test Loss: 1.199845528230071\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.96 seconds\n",
      "epoch: 37 average loss: 1.122\n",
      "Test Accuracy : 63.4%, Test Loss: 1.1805254556238651\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 10.00 seconds\n",
      "epoch: 38 average loss: 1.119\n",
      "Test Accuracy : 63.3%, Test Loss: 1.1577350497245789\n",
      "Epoch Time (Training + Test) = 9.88 seconds\n",
      "epoch: 39 average loss: 1.109\n",
      "Test Accuracy : 63.4%, Test Loss: 1.1726234406232834\n",
      "Epoch Time (Training + Test) = 10.02 seconds\n",
      "epoch: 40 average loss: 1.089\n",
      "Test Accuracy : 63.0%, Test Loss: 1.1553978994488716\n",
      "Epoch Time (Training + Test) = 9.99 seconds\n",
      "epoch: 41 average loss: 1.087\n",
      "Test Accuracy : 64.2%, Test Loss: 1.1730581987649202\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 10.09 seconds\n",
      "epoch: 42 average loss: 1.075\n",
      "Test Accuracy : 63.0%, Test Loss: 1.1733876448124647\n",
      "Epoch Time (Training + Test) = 9.90 seconds\n",
      "epoch: 43 average loss: 1.067\n",
      "Test Accuracy : 62.1%, Test Loss: 1.1959172878414392\n",
      "Epoch Time (Training + Test) = 10.03 seconds\n",
      "epoch: 44 average loss: 1.059\n",
      "Test Accuracy : 64.1%, Test Loss: 1.150236614048481\n",
      "Epoch Time (Training + Test) = 10.24 seconds\n",
      "epoch: 45 average loss: 1.057\n",
      "Test Accuracy : 63.8%, Test Loss: 1.1229942869395018\n",
      "Epoch Time (Training + Test) = 10.28 seconds\n",
      "epoch: 46 average loss: 1.038\n",
      "Test Accuracy : 64.1%, Test Loss: 1.1274383924901485\n",
      "Epoch Time (Training + Test) = 10.31 seconds\n",
      "epoch: 47 average loss: 1.040\n",
      "Test Accuracy : 64.1%, Test Loss: 1.1275992095470428\n",
      "Epoch Time (Training + Test) = 10.27 seconds\n",
      "epoch: 48 average loss: 1.024\n",
      "Test Accuracy : 66.9%, Test Loss: 1.0883275382220745\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 10.25 seconds\n",
      "epoch: 49 average loss: 1.023\n",
      "Test Accuracy : 64.5%, Test Loss: 1.1610888224095106\n",
      "Epoch Time (Training + Test) = 10.22 seconds\n",
      "epoch: 50 average loss: 1.011\n",
      "Test Accuracy : 65.0%, Test Loss: 1.117679813876748\n",
      "Epoch Time (Training + Test) = 10.34 seconds\n",
      "epoch: 51 average loss: 0.998\n",
      "Test Accuracy : 63.0%, Test Loss: 1.1507220976054668\n",
      "Epoch Time (Training + Test) = 10.44 seconds\n",
      "epoch: 52 average loss: 0.993\n",
      "Test Accuracy : 64.7%, Test Loss: 1.1161308996379375\n",
      "Epoch Time (Training + Test) = 10.42 seconds\n",
      "epoch: 53 average loss: 0.996\n",
      "Test Accuracy : 65.1%, Test Loss: 1.0991125293076038\n",
      "Epoch Time (Training + Test) = 10.22 seconds\n",
      "epoch: 54 average loss: 0.975\n",
      "Test Accuracy : 65.4%, Test Loss: 1.0971789117902517\n",
      "Epoch Time (Training + Test) = 10.33 seconds\n",
      "epoch: 55 average loss: 0.999\n",
      "Test Accuracy : 65.3%, Test Loss: 1.0948137119412422\n",
      "Epoch Time (Training + Test) = 10.35 seconds\n",
      "epoch: 56 average loss: 0.971\n",
      "Test Accuracy : 65.4%, Test Loss: 1.091238271445036\n",
      "Epoch Time (Training + Test) = 10.39 seconds\n",
      "epoch: 57 average loss: 0.976\n",
      "Test Accuracy : 65.4%, Test Loss: 1.1009787879884243\n",
      "Epoch Time (Training + Test) = 10.20 seconds\n",
      "epoch: 58 average loss: 0.961\n",
      "Test Accuracy : 66.1%, Test Loss: 1.0835423674434423\n",
      "Epoch Time (Training + Test) = 10.19 seconds\n",
      "epoch: 59 average loss: 0.961\n",
      "Test Accuracy : 65.8%, Test Loss: 1.1147921457886696\n",
      "Epoch Time (Training + Test) = 10.14 seconds\n",
      "epoch: 60 average loss: 0.948\n",
      "Test Accuracy : 65.1%, Test Loss: 1.0913863368332386\n",
      "Epoch Time (Training + Test) = 10.27 seconds\n",
      "epoch: 61 average loss: 0.948\n",
      "Test Accuracy : 66.3%, Test Loss: 1.0750485770404339\n",
      "Epoch Time (Training + Test) = 10.23 seconds\n",
      "epoch: 62 average loss: 0.929\n",
      "Test Accuracy : 66.2%, Test Loss: 1.0661676842719316\n",
      "Epoch Time (Training + Test) = 10.38 seconds\n",
      "epoch: 63 average loss: 0.916\n",
      "Test Accuracy : 67.2%, Test Loss: 1.0540850274264812\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.98 seconds\n",
      "epoch: 64 average loss: 0.921\n",
      "Test Accuracy : 66.1%, Test Loss: 1.0784496050328016\n",
      "Epoch Time (Training + Test) = 9.99 seconds\n",
      "epoch: 65 average loss: 0.897\n",
      "Test Accuracy : 66.1%, Test Loss: 1.0853331182152033\n",
      "Epoch Time (Training + Test) = 10.20 seconds\n",
      "epoch: 66 average loss: 0.903\n",
      "Test Accuracy : 67.0%, Test Loss: 1.0523900594562292\n",
      "Epoch Time (Training + Test) = 10.43 seconds\n",
      "epoch: 67 average loss: 0.896\n",
      "Test Accuracy : 67.7%, Test Loss: 1.0659050066024065\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 10.06 seconds\n",
      "epoch: 68 average loss: 0.883\n",
      "Test Accuracy : 66.8%, Test Loss: 1.0377587843686342\n",
      "Epoch Time (Training + Test) = 9.92 seconds\n",
      "epoch: 69 average loss: 0.886\n",
      "Test Accuracy : 65.0%, Test Loss: 1.0915611293166876\n",
      "Epoch Time (Training + Test) = 10.23 seconds\n",
      "epoch: 70 average loss: 0.880\n",
      "Test Accuracy : 65.3%, Test Loss: 1.0834832899272442\n",
      "Epoch Time (Training + Test) = 10.33 seconds\n",
      "epoch: 71 average loss: 0.889\n",
      "Test Accuracy : 67.2%, Test Loss: 1.0559401456266642\n",
      "Epoch Time (Training + Test) = 10.38 seconds\n",
      "epoch: 72 average loss: 0.872\n",
      "Test Accuracy : 65.6%, Test Loss: 1.0755124427378178\n",
      "Epoch Time (Training + Test) = 11.14 seconds\n",
      "epoch: 73 average loss: 0.874\n",
      "Test Accuracy : 66.2%, Test Loss: 1.0960043035447598\n",
      "Epoch Time (Training + Test) = 10.30 seconds\n",
      "epoch: 74 average loss: 0.873\n",
      "Test Accuracy : 65.9%, Test Loss: 1.069070553407073\n",
      "Epoch Time (Training + Test) = 10.38 seconds\n",
      "epoch: 75 average loss: 0.861\n",
      "Test Accuracy : 68.8%, Test Loss: 1.0213662646710873\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 10.27 seconds\n",
      "Data Saved to Grouped_Convolution_PW_Group_1.csv\n",
      "Finished Training: \n",
      "Total Time 0.426504 hours\n",
      " Average Time Per Epoch 20.47 seconds\n",
      "None 2\n",
      "No lr scheduler\n",
      "Optimizer Set to New: {'state': {}, 'param_groups': [{'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': False, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]}]}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:c4xeryez) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Current Best Acc</td><td>▁▄▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇███████████████</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>epoch time (s)</td><td>▁▆▇▆▆▇▇█▆▇▆▇▇▆▆▆█▇▆▅▅▅▅▆▇▆▇▇▆▇▆▆▆▅▆▆▆▇▇▇</td></tr><tr><td>lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_accuracy</td><td>▁▄▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇██▇████████████</td></tr><tr><td>test_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>training_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Best Accuracy</td><td>0.688</td></tr><tr><td>Current Best Acc</td><td>0.688</td></tr><tr><td>Total Time (hours)</td><td>0.4265</td></tr><tr><td>epoch</td><td>75</td></tr><tr><td>epoch time (s)</td><td>10.27193</td></tr><tr><td>lr</td><td>0.001</td></tr><tr><td>test_accuracy</td><td>0.688</td></tr><tr><td>test_loss</td><td>1.02137</td></tr><tr><td>training_loss</td><td>0.8614</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">splendid-haze-3</strong>: <a href=\"https://wandb.ai/johnny_suu/UniAdapt_Grouped_Convolution/runs/c4xeryez\" target=\"_blank\">https://wandb.ai/johnny_suu/UniAdapt_Grouped_Convolution/runs/c4xeryez</a><br/>Synced 5 W&B file(s), 1 media file(s), 1 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230503_162249-c4xeryez\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:c4xeryez). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf25b6fae250449db45b7adf094dc8d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01691666666495924, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\John Su\\Downloads\\SydneyUni\\2022\\thesis\\Thesis\\wandb\\run-20230503_163554-162qyvgo</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/johnny_suu/UniAdapt_Grouped_Convolution/runs/162qyvgo\" target=\"_blank\">fresh-tree-4</a></strong> to <a href=\"https://wandb.ai/johnny_suu/UniAdapt_Grouped_Convolution\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "Project Name: UniAdapt_Grouped_Convolution, Run Name Grouped_Convolution_PW_Group_2 \n",
      "\n",
      "\n",
      "Run Start : 2023-05-03 16-35-54\n",
      "start_epoch : 0\n",
      "initial_lr : 0.001\n",
      "batch_size : 64\n",
      "epochs : 75\n",
      "epoch_chkpts : []\n",
      "device : cuda:0\n",
      "criterion : CrossEntropyLoss()\n",
      "optimizer : Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: False\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "seed : None\n",
      "model_architecture : <class 'NN_Thesis.nn_classes.BNN_Resnet_UniAdapt'>\n",
      "binerised_training : True\n",
      "Number of Elements : 4464460\n",
      "Initial accuracy:\n",
      "Test Accuracy : 4.8%, Test Loss: 45.993561519938666\n",
      "best_acc.pth saved!\n",
      "Test Accuracy : 5.3%, Test Loss: 47.28043878078461\n",
      "best_acc.pth saved!\n",
      "epoch: 1 average loss: 2.499\n",
      "Test Accuracy : 31.9%, Test Loss: 2.266748145222664\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 18.92 seconds\n",
      "epoch: 2 average loss: 2.217\n",
      "Test Accuracy : 37.5%, Test Loss: 2.1014444157481194\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 18.75 seconds\n",
      "epoch: 3 average loss: 2.105\n",
      "Test Accuracy : 39.7%, Test Loss: 2.0120458230376244\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 18.89 seconds\n",
      "epoch: 4 average loss: 2.022\n",
      "Test Accuracy : 43.5%, Test Loss: 1.9312991797924042\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 18.65 seconds\n",
      "epoch: 5 average loss: 1.960\n",
      "Test Accuracy : 42.6%, Test Loss: 1.9171629808843136\n",
      "Epoch Time (Training + Test) = 18.37 seconds\n",
      "epoch: 6 average loss: 1.912\n",
      "Test Accuracy : 43.0%, Test Loss: 1.8635525554418564\n",
      "Epoch Time (Training + Test) = 18.29 seconds\n",
      "epoch: 7 average loss: 1.873\n",
      "Test Accuracy : 45.6%, Test Loss: 1.8331988751888275\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 18.32 seconds\n",
      "epoch: 8 average loss: 1.832\n",
      "Test Accuracy : 46.9%, Test Loss: 1.770682968199253\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 18.29 seconds\n",
      "epoch: 9 average loss: 1.791\n",
      "Test Accuracy : 48.0%, Test Loss: 1.765594381839037\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 18.28 seconds\n",
      "epoch: 10 average loss: 1.766\n",
      "Test Accuracy : 49.1%, Test Loss: 1.689714401960373\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 18.28 seconds\n",
      "epoch: 11 average loss: 1.726\n",
      "Test Accuracy : 48.9%, Test Loss: 1.679447252303362\n",
      "Epoch Time (Training + Test) = 18.25 seconds\n",
      "epoch: 12 average loss: 1.712\n",
      "Test Accuracy : 50.2%, Test Loss: 1.6428869515657425\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 18.30 seconds\n",
      "epoch: 13 average loss: 1.683\n",
      "Test Accuracy : 50.2%, Test Loss: 1.6348251849412918\n",
      "Epoch Time (Training + Test) = 18.29 seconds\n",
      "epoch: 14 average loss: 1.668\n",
      "Test Accuracy : 50.4%, Test Loss: 1.6110093407332897\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 18.27 seconds\n",
      "epoch: 15 average loss: 1.642\n",
      "Test Accuracy : 51.7%, Test Loss: 1.5696675218641758\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 18.30 seconds\n",
      "epoch: 16 average loss: 1.616\n",
      "Test Accuracy : 50.8%, Test Loss: 1.577077392488718\n",
      "Epoch Time (Training + Test) = 18.29 seconds\n",
      "epoch: 17 average loss: 1.599\n",
      "Test Accuracy : 52.0%, Test Loss: 1.5398744456470013\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 18.31 seconds\n",
      "epoch: 18 average loss: 1.584\n",
      "Test Accuracy : 53.0%, Test Loss: 1.533593788743019\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 18.41 seconds\n",
      "epoch: 19 average loss: 1.554\n",
      "Test Accuracy : 53.4%, Test Loss: 1.5255162604153156\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 18.34 seconds\n",
      "epoch: 20 average loss: 1.537\n",
      "Test Accuracy : 54.6%, Test Loss: 1.4834641925990582\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 18.47 seconds\n",
      "epoch: 21 average loss: 1.522\n",
      "Test Accuracy : 55.0%, Test Loss: 1.4645871482789516\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 18.45 seconds\n",
      "epoch: 22 average loss: 1.507\n",
      "Test Accuracy : 54.5%, Test Loss: 1.492464266717434\n",
      "Epoch Time (Training + Test) = 18.35 seconds\n",
      "epoch: 23 average loss: 1.505\n",
      "Test Accuracy : 53.8%, Test Loss: 1.4722845777869225\n",
      "Epoch Time (Training + Test) = 18.38 seconds\n",
      "epoch: 24 average loss: 1.485\n",
      "Test Accuracy : 55.4%, Test Loss: 1.4619869850575924\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 18.71 seconds\n",
      "epoch: 25 average loss: 1.471\n",
      "Test Accuracy : 55.2%, Test Loss: 1.4504491984844208\n",
      "Epoch Time (Training + Test) = 18.50 seconds\n",
      "epoch: 26 average loss: 1.456\n",
      "Test Accuracy : 56.6%, Test Loss: 1.424078080803156\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 18.41 seconds\n",
      "epoch: 27 average loss: 1.445\n",
      "Test Accuracy : 56.5%, Test Loss: 1.394454251974821\n",
      "Epoch Time (Training + Test) = 18.23 seconds\n",
      "epoch: 28 average loss: 1.425\n",
      "Test Accuracy : 56.3%, Test Loss: 1.4161502569913864\n",
      "Epoch Time (Training + Test) = 18.29 seconds\n",
      "epoch: 29 average loss: 1.419\n",
      "Test Accuracy : 55.2%, Test Loss: 1.4281001575291157\n",
      "Epoch Time (Training + Test) = 18.47 seconds\n",
      "epoch: 30 average loss: 1.413\n",
      "Test Accuracy : 56.4%, Test Loss: 1.3869554102420807\n",
      "Epoch Time (Training + Test) = 18.48 seconds\n",
      "epoch: 31 average loss: 1.401\n",
      "Test Accuracy : 57.6%, Test Loss: 1.4014746807515621\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 18.49 seconds\n",
      "epoch: 32 average loss: 1.392\n",
      "Test Accuracy : 57.6%, Test Loss: 1.366171456873417\n",
      "Epoch Time (Training + Test) = 18.43 seconds\n",
      "epoch: 33 average loss: 1.372\n",
      "Test Accuracy : 58.3%, Test Loss: 1.3512753508985043\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 18.49 seconds\n",
      "epoch: 34 average loss: 1.369\n",
      "Test Accuracy : 57.2%, Test Loss: 1.3631917126476765\n",
      "Epoch Time (Training + Test) = 18.50 seconds\n",
      "epoch: 35 average loss: 1.359\n",
      "Test Accuracy : 57.9%, Test Loss: 1.371943324804306\n",
      "Epoch Time (Training + Test) = 18.36 seconds\n",
      "epoch: 36 average loss: 1.359\n",
      "Test Accuracy : 57.3%, Test Loss: 1.368527665734291\n",
      "Epoch Time (Training + Test) = 18.48 seconds\n",
      "epoch: 37 average loss: 1.338\n",
      "Test Accuracy : 56.9%, Test Loss: 1.3686406463384628\n",
      "Epoch Time (Training + Test) = 18.40 seconds\n",
      "epoch: 38 average loss: 1.322\n",
      "Test Accuracy : 59.1%, Test Loss: 1.3332851622253656\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 18.41 seconds\n",
      "epoch: 39 average loss: 1.315\n",
      "Test Accuracy : 58.4%, Test Loss: 1.3370436951518059\n",
      "Epoch Time (Training + Test) = 18.43 seconds\n",
      "epoch: 40 average loss: 1.303\n",
      "Test Accuracy : 60.0%, Test Loss: 1.2973974794149399\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 18.37 seconds\n",
      "epoch: 41 average loss: 1.305\n",
      "Test Accuracy : 59.1%, Test Loss: 1.312958288937807\n",
      "Epoch Time (Training + Test) = 18.35 seconds\n",
      "epoch: 42 average loss: 1.303\n",
      "Test Accuracy : 57.6%, Test Loss: 1.3285693414509296\n",
      "Epoch Time (Training + Test) = 18.47 seconds\n",
      "epoch: 43 average loss: 1.298\n",
      "Test Accuracy : 59.3%, Test Loss: 1.3346950076520443\n",
      "Epoch Time (Training + Test) = 18.52 seconds\n",
      "epoch: 44 average loss: 1.290\n",
      "Test Accuracy : 58.9%, Test Loss: 1.314318660646677\n",
      "Epoch Time (Training + Test) = 18.47 seconds\n",
      "epoch: 45 average loss: 1.278\n",
      "Test Accuracy : 59.5%, Test Loss: 1.2942838668823242\n",
      "Epoch Time (Training + Test) = 18.40 seconds\n",
      "epoch: 46 average loss: 1.253\n",
      "Test Accuracy : 58.1%, Test Loss: 1.3011985644698143\n",
      "Epoch Time (Training + Test) = 18.36 seconds\n",
      "epoch: 47 average loss: 1.282\n",
      "Test Accuracy : 59.2%, Test Loss: 1.2802643440663815\n",
      "Epoch Time (Training + Test) = 18.33 seconds\n",
      "epoch: 48 average loss: 1.252\n",
      "Test Accuracy : 59.3%, Test Loss: 1.3012292962521315\n",
      "Epoch Time (Training + Test) = 18.33 seconds\n",
      "epoch: 49 average loss: 1.249\n",
      "Test Accuracy : 59.5%, Test Loss: 1.2920833956450224\n",
      "Epoch Time (Training + Test) = 18.26 seconds\n",
      "epoch: 50 average loss: 1.231\n",
      "Test Accuracy : 60.1%, Test Loss: 1.270536869764328\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 18.33 seconds\n",
      "epoch: 51 average loss: 1.233\n",
      "Test Accuracy : 60.4%, Test Loss: 1.2560817170888186\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 18.28 seconds\n",
      "epoch: 52 average loss: 1.227\n",
      "Test Accuracy : 60.7%, Test Loss: 1.2544299326837063\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 18.28 seconds\n",
      "epoch: 53 average loss: 1.224\n",
      "Test Accuracy : 60.1%, Test Loss: 1.233542961999774\n",
      "Epoch Time (Training + Test) = 18.29 seconds\n",
      "epoch: 54 average loss: 1.214\n",
      "Test Accuracy : 60.0%, Test Loss: 1.2526421919465065\n",
      "Epoch Time (Training + Test) = 18.35 seconds\n",
      "epoch: 55 average loss: 1.201\n",
      "Test Accuracy : 58.0%, Test Loss: 1.3110763281583786\n",
      "Epoch Time (Training + Test) = 18.20 seconds\n",
      "epoch: 56 average loss: 1.197\n",
      "Test Accuracy : 60.7%, Test Loss: 1.2577464766800404\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 18.37 seconds\n",
      "epoch: 57 average loss: 1.225\n",
      "Test Accuracy : 60.9%, Test Loss: 1.2476007100194693\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 18.26 seconds\n",
      "epoch: 58 average loss: 1.201\n",
      "Test Accuracy : 60.0%, Test Loss: 1.2359730508178473\n",
      "Epoch Time (Training + Test) = 18.21 seconds\n",
      "epoch: 59 average loss: 1.183\n",
      "Test Accuracy : 60.0%, Test Loss: 1.2565975971519947\n",
      "Epoch Time (Training + Test) = 18.32 seconds\n",
      "epoch: 60 average loss: 1.187\n",
      "Test Accuracy : 61.5%, Test Loss: 1.2481372021138668\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 18.28 seconds\n",
      "epoch: 61 average loss: 1.180\n",
      "Test Accuracy : 61.8%, Test Loss: 1.2178196795284748\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 18.29 seconds\n",
      "epoch: 62 average loss: 1.186\n",
      "Test Accuracy : 61.1%, Test Loss: 1.246302904561162\n",
      "Epoch Time (Training + Test) = 18.89 seconds\n",
      "epoch: 63 average loss: 1.166\n",
      "Test Accuracy : 60.7%, Test Loss: 1.237607093527913\n",
      "Epoch Time (Training + Test) = 19.08 seconds\n",
      "epoch: 64 average loss: 1.158\n",
      "Test Accuracy : 61.0%, Test Loss: 1.2237131129950285\n",
      "Epoch Time (Training + Test) = 18.67 seconds\n",
      "epoch: 65 average loss: 1.163\n",
      "Test Accuracy : 61.4%, Test Loss: 1.2172035463154316\n",
      "Epoch Time (Training + Test) = 18.23 seconds\n",
      "epoch: 66 average loss: 1.143\n",
      "Test Accuracy : 61.5%, Test Loss: 1.240912776440382\n",
      "Epoch Time (Training + Test) = 18.32 seconds\n",
      "epoch: 67 average loss: 1.162\n",
      "Test Accuracy : 62.4%, Test Loss: 1.2359596490859985\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 18.34 seconds\n",
      "epoch: 68 average loss: 1.156\n",
      "Test Accuracy : 61.9%, Test Loss: 1.216530928388238\n",
      "Epoch Time (Training + Test) = 18.24 seconds\n",
      "epoch: 69 average loss: 1.133\n",
      "Test Accuracy : 61.0%, Test Loss: 1.244387211278081\n",
      "Epoch Time (Training + Test) = 18.31 seconds\n",
      "epoch: 70 average loss: 1.141\n",
      "Test Accuracy : 60.5%, Test Loss: 1.247626779600978\n",
      "Epoch Time (Training + Test) = 18.26 seconds\n",
      "epoch: 71 average loss: 1.150\n",
      "Test Accuracy : 62.5%, Test Loss: 1.198814706876874\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 18.23 seconds\n",
      "epoch: 72 average loss: 1.127\n",
      "Test Accuracy : 60.1%, Test Loss: 1.2446243409067392\n",
      "Epoch Time (Training + Test) = 18.93 seconds\n",
      "epoch: 73 average loss: 1.145\n",
      "Test Accuracy : 62.3%, Test Loss: 1.211191389709711\n",
      "Epoch Time (Training + Test) = 19.81 seconds\n",
      "epoch: 74 average loss: 1.108\n",
      "Test Accuracy : 62.8%, Test Loss: 1.1828365586698055\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 19.85 seconds\n",
      "epoch: 75 average loss: 1.126\n",
      "Test Accuracy : 63.1%, Test Loss: 1.1709746234118938\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 19.65 seconds\n",
      "Data Saved to Grouped_Convolution_PW_Group_2.csv\n",
      "Finished Training: \n",
      "Total Time 0.769259 hours\n",
      " Average Time Per Epoch 36.92 seconds\n",
      "None 5\n",
      "No lr scheduler\n",
      "Optimizer Set to New: {'state': {}, 'param_groups': [{'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': False, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]}]}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:162qyvgo) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Current Best Acc</td><td>▁▄▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇████████████████████</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>epoch time (s)</td><td>▁▇▇▆▆▆▆▆▆▆▆▇▆▇▇▆▇▇▇▇▇▆▇▇▆▆▆▆▆▆▆▆▆▇▆▆▆▆██</td></tr><tr><td>lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_accuracy</td><td>▁▄▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇██▇▇▇████▇██████████</td></tr><tr><td>test_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>training_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Best Accuracy</td><td>0.631</td></tr><tr><td>Current Best Acc</td><td>0.631</td></tr><tr><td>Total Time (hours)</td><td>0.76926</td></tr><tr><td>epoch</td><td>75</td></tr><tr><td>epoch time (s)</td><td>19.65261</td></tr><tr><td>lr</td><td>0.001</td></tr><tr><td>test_accuracy</td><td>0.631</td></tr><tr><td>test_loss</td><td>1.17097</td></tr><tr><td>training_loss</td><td>1.12642</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">fresh-tree-4</strong>: <a href=\"https://wandb.ai/johnny_suu/UniAdapt_Grouped_Convolution/runs/162qyvgo\" target=\"_blank\">https://wandb.ai/johnny_suu/UniAdapt_Grouped_Convolution/runs/162qyvgo</a><br/>Synced 5 W&B file(s), 1 media file(s), 1 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230503_163554-162qyvgo\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:162qyvgo). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8526ef971bd34eb6863cdd3a13bd594b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01693333333435779, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\John Su\\Downloads\\SydneyUni\\2022\\thesis\\Thesis\\wandb\\run-20230503_165925-27x7scbu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/johnny_suu/UniAdapt_Grouped_Convolution/runs/27x7scbu\" target=\"_blank\">stellar-capybara-5</a></strong> to <a href=\"https://wandb.ai/johnny_suu/UniAdapt_Grouped_Convolution\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "Project Name: UniAdapt_Grouped_Convolution, Run Name Grouped_Convolution_PW_Group_5 \n",
      "\n",
      "\n",
      "Run Start : 2023-05-03 16-59-25\n",
      "start_epoch : 0\n",
      "initial_lr : 0.001\n",
      "batch_size : 64\n",
      "epochs : 75\n",
      "epoch_chkpts : []\n",
      "device : cuda:0\n",
      "criterion : CrossEntropyLoss()\n",
      "optimizer : Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: False\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "seed : None\n",
      "model_architecture : <class 'NN_Thesis.nn_classes.BNN_Resnet_UniAdapt'>\n",
      "binerised_training : True\n",
      "Number of Elements : 4414540\n",
      "Initial accuracy:\n",
      "Test Accuracy : 4.6%, Test Loss: 43.533083362943806\n",
      "best_acc.pth saved!\n",
      "Test Accuracy : 3.9%, Test Loss: 43.249693274497986\n",
      "epoch: 1 average loss: 2.556\n",
      "Test Accuracy : 31.5%, Test Loss: 2.2862957417964935\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.92 seconds\n",
      "epoch: 2 average loss: 2.282\n",
      "Test Accuracy : 34.1%, Test Loss: 2.1868048273026943\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.98 seconds\n",
      "epoch: 3 average loss: 2.187\n",
      "Test Accuracy : 38.2%, Test Loss: 2.0812700130045414\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.90 seconds\n",
      "epoch: 4 average loss: 2.113\n",
      "Test Accuracy : 39.7%, Test Loss: 2.0248186849057674\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.81 seconds\n",
      "epoch: 5 average loss: 2.050\n",
      "Test Accuracy : 43.2%, Test Loss: 1.940447922796011\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.76 seconds\n",
      "epoch: 6 average loss: 2.012\n",
      "Test Accuracy : 43.2%, Test Loss: 1.9263587519526482\n",
      "Epoch Time (Training + Test) = 9.82 seconds\n",
      "epoch: 7 average loss: 1.967\n",
      "Test Accuracy : 42.7%, Test Loss: 1.8761309944093227\n",
      "Epoch Time (Training + Test) = 9.85 seconds\n",
      "epoch: 8 average loss: 1.925\n",
      "Test Accuracy : 44.0%, Test Loss: 1.8574345707893372\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.95 seconds\n",
      "epoch: 9 average loss: 1.900\n",
      "Test Accuracy : 42.6%, Test Loss: 1.8447242341935635\n",
      "Epoch Time (Training + Test) = 9.94 seconds\n",
      "epoch: 10 average loss: 1.874\n",
      "Test Accuracy : 45.5%, Test Loss: 1.8066538088023663\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.89 seconds\n",
      "epoch: 11 average loss: 1.857\n",
      "Test Accuracy : 45.0%, Test Loss: 1.786437626928091\n",
      "Epoch Time (Training + Test) = 9.79 seconds\n",
      "epoch: 12 average loss: 1.826\n",
      "Test Accuracy : 46.1%, Test Loss: 1.7639116570353508\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.99 seconds\n",
      "epoch: 13 average loss: 1.814\n",
      "Test Accuracy : 44.8%, Test Loss: 1.7805306538939476\n",
      "Epoch Time (Training + Test) = 9.84 seconds\n",
      "epoch: 14 average loss: 1.785\n",
      "Test Accuracy : 46.2%, Test Loss: 1.733527671545744\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.96 seconds\n",
      "epoch: 15 average loss: 1.773\n",
      "Test Accuracy : 47.1%, Test Loss: 1.7263656184077263\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 10.04 seconds\n",
      "epoch: 16 average loss: 1.770\n",
      "Test Accuracy : 46.2%, Test Loss: 1.7334792576730251\n",
      "Epoch Time (Training + Test) = 10.08 seconds\n",
      "epoch: 17 average loss: 1.743\n",
      "Test Accuracy : 48.1%, Test Loss: 1.6722020991146564\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 10.29 seconds\n",
      "epoch: 18 average loss: 1.733\n",
      "Test Accuracy : 47.6%, Test Loss: 1.7186441384255886\n",
      "Epoch Time (Training + Test) = 10.02 seconds\n",
      "epoch: 19 average loss: 1.716\n",
      "Test Accuracy : 48.6%, Test Loss: 1.6495348922908306\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.87 seconds\n",
      "epoch: 20 average loss: 1.720\n",
      "Test Accuracy : 47.2%, Test Loss: 1.6831729039549828\n",
      "Epoch Time (Training + Test) = 10.06 seconds\n",
      "epoch: 21 average loss: 1.700\n",
      "Test Accuracy : 49.0%, Test Loss: 1.6528775468468666\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 10.24 seconds\n",
      "epoch: 22 average loss: 1.693\n",
      "Test Accuracy : 47.9%, Test Loss: 1.671846330165863\n",
      "Epoch Time (Training + Test) = 9.99 seconds\n",
      "epoch: 23 average loss: 1.677\n",
      "Test Accuracy : 49.7%, Test Loss: 1.6191663779318333\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 10.17 seconds\n",
      "epoch: 24 average loss: 1.674\n",
      "Test Accuracy : 49.4%, Test Loss: 1.63435922190547\n",
      "Epoch Time (Training + Test) = 10.01 seconds\n",
      "epoch: 25 average loss: 1.667\n",
      "Test Accuracy : 48.9%, Test Loss: 1.63242307305336\n",
      "Epoch Time (Training + Test) = 9.90 seconds\n",
      "epoch: 26 average loss: 1.648\n",
      "Test Accuracy : 49.8%, Test Loss: 1.5973967052996159\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.93 seconds\n",
      "epoch: 27 average loss: 1.641\n",
      "Test Accuracy : 51.0%, Test Loss: 1.5951699912548065\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 10.30 seconds\n",
      "epoch: 28 average loss: 1.622\n",
      "Test Accuracy : 50.1%, Test Loss: 1.598226685076952\n",
      "Epoch Time (Training + Test) = 9.97 seconds\n",
      "epoch: 29 average loss: 1.627\n",
      "Test Accuracy : 49.6%, Test Loss: 1.5977774299681187\n",
      "Epoch Time (Training + Test) = 9.83 seconds\n",
      "epoch: 30 average loss: 1.611\n",
      "Test Accuracy : 50.9%, Test Loss: 1.5735322423279285\n",
      "Epoch Time (Training + Test) = 9.97 seconds\n",
      "epoch: 31 average loss: 1.613\n",
      "Test Accuracy : 49.3%, Test Loss: 1.59646100923419\n",
      "Epoch Time (Training + Test) = 9.95 seconds\n",
      "epoch: 32 average loss: 1.600\n",
      "Test Accuracy : 49.9%, Test Loss: 1.594070702791214\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 33 average loss: 1.598\n",
      "Test Accuracy : 51.7%, Test Loss: 1.526291362941265\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 34 average loss: 1.574\n",
      "Test Accuracy : 50.5%, Test Loss: 1.5818460993468761\n",
      "Epoch Time (Training + Test) = 9.81 seconds\n",
      "epoch: 35 average loss: 1.586\n",
      "Test Accuracy : 52.3%, Test Loss: 1.539917517453432\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.71 seconds\n",
      "epoch: 36 average loss: 1.576\n",
      "Test Accuracy : 51.1%, Test Loss: 1.5615089796483517\n",
      "Epoch Time (Training + Test) = 9.80 seconds\n",
      "epoch: 37 average loss: 1.583\n",
      "Test Accuracy : 51.4%, Test Loss: 1.5467803999781609\n",
      "Epoch Time (Training + Test) = 9.71 seconds\n",
      "epoch: 38 average loss: 1.564\n",
      "Test Accuracy : 51.5%, Test Loss: 1.5567386262118816\n",
      "Epoch Time (Training + Test) = 9.83 seconds\n",
      "epoch: 39 average loss: 1.556\n",
      "Test Accuracy : 51.5%, Test Loss: 1.532213520258665\n",
      "Epoch Time (Training + Test) = 9.78 seconds\n",
      "epoch: 40 average loss: 1.546\n",
      "Test Accuracy : 52.3%, Test Loss: 1.5354868322610855\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 41 average loss: 1.545\n",
      "Test Accuracy : 52.2%, Test Loss: 1.516510784626007\n",
      "Epoch Time (Training + Test) = 9.74 seconds\n",
      "epoch: 42 average loss: 1.545\n",
      "Test Accuracy : 52.2%, Test Loss: 1.5110426433384418\n",
      "Epoch Time (Training + Test) = 9.82 seconds\n",
      "epoch: 43 average loss: 1.551\n",
      "Test Accuracy : 52.5%, Test Loss: 1.5360165238380432\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.87 seconds\n",
      "epoch: 44 average loss: 1.542\n",
      "Test Accuracy : 52.1%, Test Loss: 1.4874009191989899\n",
      "Epoch Time (Training + Test) = 9.79 seconds\n",
      "epoch: 45 average loss: 1.522\n",
      "Test Accuracy : 53.2%, Test Loss: 1.5193308182060719\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.77 seconds\n",
      "epoch: 46 average loss: 1.523\n",
      "Test Accuracy : 52.8%, Test Loss: 1.5050040632486343\n",
      "Epoch Time (Training + Test) = 9.76 seconds\n",
      "epoch: 47 average loss: 1.515\n",
      "Test Accuracy : 51.6%, Test Loss: 1.5530945025384426\n",
      "Epoch Time (Training + Test) = 9.88 seconds\n",
      "epoch: 48 average loss: 1.522\n",
      "Test Accuracy : 53.4%, Test Loss: 1.5034583173692226\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.81 seconds\n",
      "epoch: 49 average loss: 1.509\n",
      "Test Accuracy : 52.8%, Test Loss: 1.4984992407262325\n",
      "Epoch Time (Training + Test) = 9.76 seconds\n",
      "epoch: 50 average loss: 1.504\n",
      "Test Accuracy : 52.1%, Test Loss: 1.5022256784141064\n",
      "Epoch Time (Training + Test) = 9.84 seconds\n",
      "epoch: 51 average loss: 1.495\n",
      "Test Accuracy : 52.8%, Test Loss: 1.4911888651549816\n",
      "Epoch Time (Training + Test) = 9.83 seconds\n",
      "epoch: 52 average loss: 1.509\n",
      "Test Accuracy : 54.0%, Test Loss: 1.4623335525393486\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.83 seconds\n",
      "epoch: 53 average loss: 1.491\n",
      "Test Accuracy : 54.0%, Test Loss: 1.455608356744051\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 54 average loss: 1.503\n",
      "Test Accuracy : 52.6%, Test Loss: 1.490847546607256\n",
      "Epoch Time (Training + Test) = 9.69 seconds\n",
      "epoch: 55 average loss: 1.478\n",
      "Test Accuracy : 53.9%, Test Loss: 1.4633839204907417\n",
      "Epoch Time (Training + Test) = 9.65 seconds\n",
      "epoch: 56 average loss: 1.488\n",
      "Test Accuracy : 52.4%, Test Loss: 1.5175024457275867\n",
      "Epoch Time (Training + Test) = 9.76 seconds\n",
      "epoch: 57 average loss: 1.480\n",
      "Test Accuracy : 53.0%, Test Loss: 1.4801195487380028\n",
      "Epoch Time (Training + Test) = 9.73 seconds\n",
      "epoch: 58 average loss: 1.476\n",
      "Test Accuracy : 53.6%, Test Loss: 1.4647212103009224\n",
      "Epoch Time (Training + Test) = 9.85 seconds\n",
      "epoch: 59 average loss: 1.470\n",
      "Test Accuracy : 53.3%, Test Loss: 1.436231791973114\n",
      "Epoch Time (Training + Test) = 9.90 seconds\n",
      "epoch: 60 average loss: 1.473\n",
      "Test Accuracy : 54.3%, Test Loss: 1.4566208831965923\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.96 seconds\n",
      "epoch: 61 average loss: 1.466\n",
      "Test Accuracy : 55.8%, Test Loss: 1.4223187416791916\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.76 seconds\n",
      "epoch: 62 average loss: 1.475\n",
      "Test Accuracy : 54.8%, Test Loss: 1.452741127461195\n",
      "Epoch Time (Training + Test) = 9.81 seconds\n",
      "epoch: 63 average loss: 1.465\n",
      "Test Accuracy : 53.7%, Test Loss: 1.454086385667324\n",
      "Epoch Time (Training + Test) = 9.80 seconds\n",
      "epoch: 64 average loss: 1.457\n",
      "Test Accuracy : 55.5%, Test Loss: 1.4284557327628136\n",
      "Epoch Time (Training + Test) = 9.73 seconds\n",
      "epoch: 65 average loss: 1.446\n",
      "Test Accuracy : 53.6%, Test Loss: 1.4816593527793884\n",
      "Epoch Time (Training + Test) = 9.85 seconds\n",
      "epoch: 66 average loss: 1.450\n",
      "Test Accuracy : 54.6%, Test Loss: 1.4546192176640034\n",
      "Epoch Time (Training + Test) = 9.80 seconds\n",
      "epoch: 67 average loss: 1.432\n",
      "Test Accuracy : 54.2%, Test Loss: 1.4361925162374973\n",
      "Epoch Time (Training + Test) = 9.71 seconds\n",
      "epoch: 68 average loss: 1.443\n",
      "Test Accuracy : 54.1%, Test Loss: 1.4632525481283665\n",
      "Epoch Time (Training + Test) = 9.83 seconds\n",
      "epoch: 69 average loss: 1.425\n",
      "Test Accuracy : 55.7%, Test Loss: 1.4059694558382034\n",
      "Epoch Time (Training + Test) = 9.80 seconds\n",
      "epoch: 70 average loss: 1.439\n",
      "Test Accuracy : 54.5%, Test Loss: 1.4193337187170982\n",
      "Epoch Time (Training + Test) = 9.71 seconds\n",
      "epoch: 71 average loss: 1.431\n",
      "Test Accuracy : 55.3%, Test Loss: 1.4134563952684402\n",
      "Epoch Time (Training + Test) = 9.78 seconds\n",
      "epoch: 72 average loss: 1.428\n",
      "Test Accuracy : 54.6%, Test Loss: 1.440867431461811\n",
      "Epoch Time (Training + Test) = 9.81 seconds\n",
      "epoch: 73 average loss: 1.423\n",
      "Test Accuracy : 53.9%, Test Loss: 1.4454171061515808\n",
      "Epoch Time (Training + Test) = 9.76 seconds\n",
      "epoch: 74 average loss: 1.419\n",
      "Test Accuracy : 54.2%, Test Loss: 1.4372835233807564\n",
      "Epoch Time (Training + Test) = 9.85 seconds\n",
      "epoch: 75 average loss: 1.425\n",
      "Test Accuracy : 54.8%, Test Loss: 1.4321718700230122\n",
      "Epoch Time (Training + Test) = 9.80 seconds\n",
      "Data Saved to Grouped_Convolution_PW_Group_5.csv\n",
      "Finished Training: \n",
      "Total Time 0.411195 hours\n",
      " Average Time Per Epoch 19.74 seconds\n",
      "None 10\n",
      "No lr scheduler\n",
      "Optimizer Set to New: {'state': {}, 'param_groups': [{'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': False, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]}]}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:27x7scbu) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Current Best Acc</td><td>▁▅▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇█████████████████████</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>epoch time (s)</td><td>▁▇▇▆▆▇▆▆▇█▇██▇▇▇▇▇▆▆▆▇▆▆▆▆▆▆▇▆▆▇▆▆▆▆▆▆▆▆</td></tr><tr><td>lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_accuracy</td><td>▁▅▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇███████████████████</td></tr><tr><td>test_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>training_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Best Accuracy</td><td>0.558</td></tr><tr><td>Current Best Acc</td><td>0.558</td></tr><tr><td>Total Time (hours)</td><td>0.4112</td></tr><tr><td>epoch</td><td>75</td></tr><tr><td>epoch time (s)</td><td>9.79688</td></tr><tr><td>lr</td><td>0.001</td></tr><tr><td>test_accuracy</td><td>0.5475</td></tr><tr><td>test_loss</td><td>1.43217</td></tr><tr><td>training_loss</td><td>1.42503</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">stellar-capybara-5</strong>: <a href=\"https://wandb.ai/johnny_suu/UniAdapt_Grouped_Convolution/runs/27x7scbu\" target=\"_blank\">https://wandb.ai/johnny_suu/UniAdapt_Grouped_Convolution/runs/27x7scbu</a><br/>Synced 5 W&B file(s), 1 media file(s), 1 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230503_165925-27x7scbu\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:27x7scbu). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67eb03cf481c4177a63aaf05aad2f19e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\John Su\\Downloads\\SydneyUni\\2022\\thesis\\Thesis\\wandb\\run-20230503_171205-146snvt8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/johnny_suu/UniAdapt_Grouped_Convolution/runs/146snvt8\" target=\"_blank\">wobbly-wood-6</a></strong> to <a href=\"https://wandb.ai/johnny_suu/UniAdapt_Grouped_Convolution\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "Project Name: UniAdapt_Grouped_Convolution, Run Name Grouped_Convolution_PW_Group_10 \n",
      "\n",
      "\n",
      "Run Start : 2023-05-03 17-12-05\n",
      "start_epoch : 0\n",
      "initial_lr : 0.001\n",
      "batch_size : 64\n",
      "epochs : 75\n",
      "epoch_chkpts : []\n",
      "device : cuda:0\n",
      "criterion : CrossEntropyLoss()\n",
      "optimizer : Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: False\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "seed : None\n",
      "model_architecture : <class 'NN_Thesis.nn_classes.BNN_Resnet_UniAdapt'>\n",
      "binerised_training : True\n",
      "Number of Elements : 4397900\n",
      "Initial accuracy:\n",
      "Test Accuracy : 4.5%, Test Loss: 48.152670550498236\n",
      "best_acc.pth saved!\n",
      "Test Accuracy : 5.2%, Test Loss: 48.03066325187683\n",
      "best_acc.pth saved!\n",
      "epoch: 1 average loss: 2.610\n",
      "Test Accuracy : 28.9%, Test Loss: 2.3412688449025154\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.88 seconds\n",
      "epoch: 2 average loss: 2.332\n",
      "Test Accuracy : 34.5%, Test Loss: 2.2187106534838676\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.90 seconds\n",
      "epoch: 3 average loss: 2.226\n",
      "Test Accuracy : 37.3%, Test Loss: 2.150582257658243\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 4 average loss: 2.158\n",
      "Test Accuracy : 38.8%, Test Loss: 2.0532681196928024\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.98 seconds\n",
      "epoch: 5 average loss: 2.117\n",
      "Test Accuracy : 38.8%, Test Loss: 2.040620017796755\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 6 average loss: 2.078\n",
      "Test Accuracy : 38.6%, Test Loss: 2.0182420164346695\n",
      "Epoch Time (Training + Test) = 9.72 seconds\n",
      "epoch: 7 average loss: 2.050\n",
      "Test Accuracy : 40.2%, Test Loss: 1.9742181710898876\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.96 seconds\n",
      "epoch: 8 average loss: 2.016\n",
      "Test Accuracy : 42.4%, Test Loss: 1.9452107325196266\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 9 average loss: 1.998\n",
      "Test Accuracy : 43.0%, Test Loss: 1.9260083138942719\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.77 seconds\n",
      "epoch: 10 average loss: 1.972\n",
      "Test Accuracy : 44.1%, Test Loss: 1.8706869147717953\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.94 seconds\n",
      "epoch: 11 average loss: 1.953\n",
      "Test Accuracy : 42.6%, Test Loss: 1.8767678439617157\n",
      "Epoch Time (Training + Test) = 9.95 seconds\n",
      "epoch: 12 average loss: 1.932\n",
      "Test Accuracy : 43.4%, Test Loss: 1.8583288826048374\n",
      "Epoch Time (Training + Test) = 9.87 seconds\n",
      "epoch: 13 average loss: 1.916\n",
      "Test Accuracy : 42.1%, Test Loss: 1.8940539173781872\n",
      "Epoch Time (Training + Test) = 9.84 seconds\n",
      "epoch: 14 average loss: 1.905\n",
      "Test Accuracy : 44.6%, Test Loss: 1.8380498960614204\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.88 seconds\n",
      "epoch: 15 average loss: 1.884\n",
      "Test Accuracy : 44.5%, Test Loss: 1.8206029646098614\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 16 average loss: 1.865\n",
      "Test Accuracy : 43.2%, Test Loss: 1.8227112852036953\n",
      "Epoch Time (Training + Test) = 9.83 seconds\n",
      "epoch: 17 average loss: 1.850\n",
      "Test Accuracy : 45.3%, Test Loss: 1.7989695444703102\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.87 seconds\n",
      "epoch: 18 average loss: 1.845\n",
      "Test Accuracy : 44.2%, Test Loss: 1.7997612953186035\n",
      "Epoch Time (Training + Test) = 9.87 seconds\n",
      "epoch: 19 average loss: 1.847\n",
      "Test Accuracy : 46.2%, Test Loss: 1.7931916825473309\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.87 seconds\n",
      "epoch: 20 average loss: 1.809\n",
      "Test Accuracy : 46.5%, Test Loss: 1.7545820660889149\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.98 seconds\n",
      "epoch: 21 average loss: 1.809\n",
      "Test Accuracy : 45.0%, Test Loss: 1.78352265432477\n",
      "Epoch Time (Training + Test) = 9.89 seconds\n",
      "epoch: 22 average loss: 1.806\n",
      "Test Accuracy : 44.8%, Test Loss: 1.781449519097805\n",
      "Epoch Time (Training + Test) = 9.96 seconds\n",
      "epoch: 23 average loss: 1.791\n",
      "Test Accuracy : 47.2%, Test Loss: 1.739822767674923\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.93 seconds\n",
      "epoch: 24 average loss: 1.794\n",
      "Test Accuracy : 46.8%, Test Loss: 1.733998864889145\n",
      "Epoch Time (Training + Test) = 9.89 seconds\n",
      "epoch: 25 average loss: 1.783\n",
      "Test Accuracy : 45.5%, Test Loss: 1.7256973795592785\n",
      "Epoch Time (Training + Test) = 9.87 seconds\n",
      "epoch: 26 average loss: 1.778\n",
      "Test Accuracy : 47.7%, Test Loss: 1.686713483184576\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.87 seconds\n",
      "epoch: 27 average loss: 1.758\n",
      "Test Accuracy : 45.0%, Test Loss: 1.752829909324646\n",
      "Epoch Time (Training + Test) = 9.83 seconds\n",
      "epoch: 28 average loss: 1.760\n",
      "Test Accuracy : 46.7%, Test Loss: 1.705929484218359\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 29 average loss: 1.762\n",
      "Test Accuracy : 46.4%, Test Loss: 1.7274060100317001\n",
      "Epoch Time (Training + Test) = 9.85 seconds\n",
      "epoch: 30 average loss: 1.742\n",
      "Test Accuracy : 45.3%, Test Loss: 1.73522425070405\n",
      "Epoch Time (Training + Test) = 9.87 seconds\n",
      "epoch: 31 average loss: 1.735\n",
      "Test Accuracy : 46.5%, Test Loss: 1.709826786071062\n",
      "Epoch Time (Training + Test) = 9.85 seconds\n",
      "epoch: 32 average loss: 1.729\n",
      "Test Accuracy : 48.6%, Test Loss: 1.689197737723589\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.87 seconds\n",
      "epoch: 33 average loss: 1.712\n",
      "Test Accuracy : 46.9%, Test Loss: 1.6909508891403675\n",
      "Epoch Time (Training + Test) = 9.92 seconds\n",
      "epoch: 34 average loss: 1.717\n",
      "Test Accuracy : 47.9%, Test Loss: 1.6787830665707588\n",
      "Epoch Time (Training + Test) = 9.78 seconds\n",
      "epoch: 35 average loss: 1.722\n",
      "Test Accuracy : 49.1%, Test Loss: 1.6640980392694473\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 36 average loss: 1.708\n",
      "Test Accuracy : 48.6%, Test Loss: 1.658100239932537\n",
      "Epoch Time (Training + Test) = 9.83 seconds\n",
      "epoch: 37 average loss: 1.697\n",
      "Test Accuracy : 48.5%, Test Loss: 1.6688501983880997\n",
      "Epoch Time (Training + Test) = 9.89 seconds\n",
      "epoch: 38 average loss: 1.697\n",
      "Test Accuracy : 48.9%, Test Loss: 1.6591967716813087\n",
      "Epoch Time (Training + Test) = 9.80 seconds\n",
      "epoch: 39 average loss: 1.697\n",
      "Test Accuracy : 49.4%, Test Loss: 1.6302863284945488\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.93 seconds\n",
      "epoch: 40 average loss: 1.693\n",
      "Test Accuracy : 49.9%, Test Loss: 1.6221573390066624\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 41 average loss: 1.681\n",
      "Test Accuracy : 49.0%, Test Loss: 1.6449078395962715\n",
      "Epoch Time (Training + Test) = 9.82 seconds\n",
      "epoch: 42 average loss: 1.671\n",
      "Test Accuracy : 48.2%, Test Loss: 1.655748587101698\n",
      "Epoch Time (Training + Test) = 9.82 seconds\n",
      "epoch: 43 average loss: 1.669\n",
      "Test Accuracy : 50.5%, Test Loss: 1.604995358735323\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.90 seconds\n",
      "epoch: 44 average loss: 1.685\n",
      "Test Accuracy : 49.5%, Test Loss: 1.622209507972002\n",
      "Epoch Time (Training + Test) = 9.83 seconds\n",
      "epoch: 45 average loss: 1.675\n",
      "Test Accuracy : 48.3%, Test Loss: 1.6484125293791294\n",
      "Epoch Time (Training + Test) = 9.80 seconds\n",
      "epoch: 46 average loss: 1.673\n",
      "Test Accuracy : 48.5%, Test Loss: 1.6309171319007874\n",
      "Epoch Time (Training + Test) = 9.72 seconds\n",
      "epoch: 47 average loss: 1.656\n",
      "Test Accuracy : 49.0%, Test Loss: 1.6421664282679558\n",
      "Epoch Time (Training + Test) = 9.85 seconds\n",
      "epoch: 48 average loss: 1.662\n",
      "Test Accuracy : 51.4%, Test Loss: 1.6050100587308407\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.80 seconds\n",
      "epoch: 49 average loss: 1.666\n",
      "Test Accuracy : 48.9%, Test Loss: 1.6440630070865154\n",
      "Epoch Time (Training + Test) = 9.79 seconds\n",
      "epoch: 50 average loss: 1.665\n",
      "Test Accuracy : 49.8%, Test Loss: 1.6406493969261646\n",
      "Epoch Time (Training + Test) = 9.76 seconds\n",
      "epoch: 51 average loss: 1.653\n",
      "Test Accuracy : 49.5%, Test Loss: 1.6346596591174603\n",
      "Epoch Time (Training + Test) = 9.83 seconds\n",
      "epoch: 52 average loss: 1.650\n",
      "Test Accuracy : 49.4%, Test Loss: 1.6220963187515736\n",
      "Epoch Time (Training + Test) = 9.83 seconds\n",
      "epoch: 53 average loss: 1.631\n",
      "Test Accuracy : 49.6%, Test Loss: 1.603583224117756\n",
      "Epoch Time (Training + Test) = 9.85 seconds\n",
      "epoch: 54 average loss: 1.638\n",
      "Test Accuracy : 50.1%, Test Loss: 1.5766872428357601\n",
      "Epoch Time (Training + Test) = 9.71 seconds\n",
      "epoch: 55 average loss: 1.639\n",
      "Test Accuracy : 48.8%, Test Loss: 1.6134706102311611\n",
      "Epoch Time (Training + Test) = 9.83 seconds\n",
      "epoch: 56 average loss: 1.632\n",
      "Test Accuracy : 49.9%, Test Loss: 1.582069467753172\n",
      "Epoch Time (Training + Test) = 9.71 seconds\n",
      "epoch: 57 average loss: 1.617\n",
      "Test Accuracy : 50.0%, Test Loss: 1.5960964746773243\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 58 average loss: 1.624\n",
      "Test Accuracy : 50.0%, Test Loss: 1.5911534167826176\n",
      "Epoch Time (Training + Test) = 9.81 seconds\n",
      "epoch: 59 average loss: 1.622\n",
      "Test Accuracy : 49.0%, Test Loss: 1.5931141152977943\n",
      "Epoch Time (Training + Test) = 9.68 seconds\n",
      "epoch: 60 average loss: 1.630\n",
      "Test Accuracy : 50.2%, Test Loss: 1.5758855864405632\n",
      "Epoch Time (Training + Test) = 9.84 seconds\n",
      "epoch: 61 average loss: 1.614\n",
      "Test Accuracy : 51.5%, Test Loss: 1.585720393806696\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.81 seconds\n",
      "epoch: 62 average loss: 1.611\n",
      "Test Accuracy : 50.6%, Test Loss: 1.5783304125070572\n",
      "Epoch Time (Training + Test) = 9.72 seconds\n",
      "epoch: 63 average loss: 1.607\n",
      "Test Accuracy : 51.4%, Test Loss: 1.5652375593781471\n",
      "Epoch Time (Training + Test) = 9.84 seconds\n",
      "epoch: 64 average loss: 1.603\n",
      "Test Accuracy : 51.5%, Test Loss: 1.5853367783129215\n",
      "Epoch Time (Training + Test) = 9.85 seconds\n",
      "epoch: 65 average loss: 1.606\n",
      "Test Accuracy : 51.3%, Test Loss: 1.5964398421347141\n",
      "Epoch Time (Training + Test) = 9.76 seconds\n",
      "epoch: 66 average loss: 1.595\n",
      "Test Accuracy : 50.2%, Test Loss: 1.607695359736681\n",
      "Epoch Time (Training + Test) = 9.88 seconds\n",
      "epoch: 67 average loss: 1.597\n",
      "Test Accuracy : 51.4%, Test Loss: 1.593928873538971\n",
      "Epoch Time (Training + Test) = 9.82 seconds\n",
      "epoch: 68 average loss: 1.607\n",
      "Test Accuracy : 52.3%, Test Loss: 1.547181360423565\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.78 seconds\n",
      "epoch: 69 average loss: 1.595\n",
      "Test Accuracy : 51.0%, Test Loss: 1.5490880385041237\n",
      "Epoch Time (Training + Test) = 9.83 seconds\n",
      "epoch: 70 average loss: 1.580\n",
      "Test Accuracy : 50.3%, Test Loss: 1.5624956786632538\n",
      "Epoch Time (Training + Test) = 9.85 seconds\n",
      "epoch: 71 average loss: 1.596\n",
      "Test Accuracy : 52.0%, Test Loss: 1.5422211699187756\n",
      "Epoch Time (Training + Test) = 9.83 seconds\n",
      "epoch: 72 average loss: 1.584\n",
      "Test Accuracy : 52.0%, Test Loss: 1.5639505833387375\n",
      "Epoch Time (Training + Test) = 9.88 seconds\n",
      "epoch: 73 average loss: 1.570\n",
      "Test Accuracy : 51.6%, Test Loss: 1.5618566051125526\n",
      "Epoch Time (Training + Test) = 9.84 seconds\n",
      "epoch: 74 average loss: 1.583\n",
      "Test Accuracy : 50.2%, Test Loss: 1.576019011437893\n",
      "Epoch Time (Training + Test) = 9.80 seconds\n",
      "epoch: 75 average loss: 1.569\n",
      "Test Accuracy : 51.0%, Test Loss: 1.5709448903799057\n",
      "Epoch Time (Training + Test) = 9.71 seconds\n",
      "Data Saved to Grouped_Convolution_PW_Group_10.csv\n",
      "Finished Training: \n",
      "Total Time 0.410123 hours\n",
      " Average Time Per Epoch 19.69 seconds\n",
      "None 20\n",
      "No lr scheduler\n",
      "Optimizer Set to New: {'state': {}, 'param_groups': [{'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': False, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]}]}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:146snvt8) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Current Best Acc</td><td>▁▅▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇█████████████████████</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>epoch time (s)</td><td>▁████▇████████████▇▇▇█▇▇▇▇▇▇█▇█▇▇█▇▇▇▇█▇</td></tr><tr><td>lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_accuracy</td><td>▁▅▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇██▇█▇███████████████</td></tr><tr><td>test_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>training_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Best Accuracy</td><td>0.523</td></tr><tr><td>Current Best Acc</td><td>0.523</td></tr><tr><td>Total Time (hours)</td><td>0.41012</td></tr><tr><td>epoch</td><td>75</td></tr><tr><td>epoch time (s)</td><td>9.70901</td></tr><tr><td>lr</td><td>0.001</td></tr><tr><td>test_accuracy</td><td>0.51</td></tr><tr><td>test_loss</td><td>1.57094</td></tr><tr><td>training_loss</td><td>1.56879</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">wobbly-wood-6</strong>: <a href=\"https://wandb.ai/johnny_suu/UniAdapt_Grouped_Convolution/runs/146snvt8\" target=\"_blank\">https://wandb.ai/johnny_suu/UniAdapt_Grouped_Convolution/runs/146snvt8</a><br/>Synced 5 W&B file(s), 1 media file(s), 1 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230503_171205-146snvt8\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:146snvt8). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7e19788a6ab4395b1880f3d0cd3537b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01693333333435779, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\John Su\\Downloads\\SydneyUni\\2022\\thesis\\Thesis\\wandb\\run-20230503_172443-3je58h7y</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/johnny_suu/UniAdapt_Grouped_Convolution/runs/3je58h7y\" target=\"_blank\">autumn-universe-7</a></strong> to <a href=\"https://wandb.ai/johnny_suu/UniAdapt_Grouped_Convolution\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "Project Name: UniAdapt_Grouped_Convolution, Run Name Grouped_Convolution_PW_Group_20 \n",
      "\n",
      "\n",
      "Run Start : 2023-05-03 17-24-43\n",
      "start_epoch : 0\n",
      "initial_lr : 0.001\n",
      "batch_size : 64\n",
      "epochs : 75\n",
      "epoch_chkpts : []\n",
      "device : cuda:0\n",
      "criterion : CrossEntropyLoss()\n",
      "optimizer : Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: False\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "seed : None\n",
      "model_architecture : <class 'NN_Thesis.nn_classes.BNN_Resnet_UniAdapt'>\n",
      "binerised_training : True\n",
      "Number of Elements : 4389580\n",
      "Initial accuracy:\n",
      "Test Accuracy : 3.8%, Test Loss: 36.32751678661177\n",
      "best_acc.pth saved!\n",
      "Test Accuracy : 3.7%, Test Loss: 36.67568051815033\n",
      "epoch: 1 average loss: 2.610\n",
      "Test Accuracy : 28.7%, Test Loss: 2.3737443685531616\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.87 seconds\n",
      "epoch: 2 average loss: 2.359\n",
      "Test Accuracy : 33.1%, Test Loss: 2.2719016075134277\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 10.05 seconds\n",
      "epoch: 3 average loss: 2.285\n",
      "Test Accuracy : 34.5%, Test Loss: 2.2041749879717827\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 10.03 seconds\n",
      "epoch: 4 average loss: 2.216\n",
      "Test Accuracy : 35.6%, Test Loss: 2.1528124921023846\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.85 seconds\n",
      "epoch: 5 average loss: 2.177\n",
      "Test Accuracy : 35.8%, Test Loss: 2.1156199984252453\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 6 average loss: 2.148\n",
      "Test Accuracy : 38.0%, Test Loss: 2.085066322237253\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.83 seconds\n",
      "epoch: 7 average loss: 2.113\n",
      "Test Accuracy : 38.9%, Test Loss: 2.0504919178783894\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 8 average loss: 2.077\n",
      "Test Accuracy : 39.4%, Test Loss: 2.010234348475933\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.88 seconds\n",
      "epoch: 9 average loss: 2.062\n",
      "Test Accuracy : 38.6%, Test Loss: 2.040884170681238\n",
      "Epoch Time (Training + Test) = 9.83 seconds\n",
      "epoch: 10 average loss: 2.041\n",
      "Test Accuracy : 40.8%, Test Loss: 2.008843597024679\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 11 average loss: 2.019\n",
      "Test Accuracy : 40.9%, Test Loss: 1.9834410920739174\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 12 average loss: 2.000\n",
      "Test Accuracy : 40.6%, Test Loss: 1.9608434848487377\n",
      "Epoch Time (Training + Test) = 9.85 seconds\n",
      "epoch: 13 average loss: 1.993\n",
      "Test Accuracy : 40.9%, Test Loss: 1.950557254254818\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.84 seconds\n",
      "epoch: 14 average loss: 1.971\n",
      "Test Accuracy : 42.6%, Test Loss: 1.9274913482367992\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.85 seconds\n",
      "epoch: 15 average loss: 1.966\n",
      "Test Accuracy : 42.9%, Test Loss: 1.8886300772428513\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 16 average loss: 1.953\n",
      "Test Accuracy : 42.9%, Test Loss: 1.9201782383024693\n",
      "Epoch Time (Training + Test) = 9.82 seconds\n",
      "epoch: 17 average loss: 1.937\n",
      "Test Accuracy : 43.2%, Test Loss: 1.8852453529834747\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 18 average loss: 1.920\n",
      "Test Accuracy : 43.8%, Test Loss: 1.8915714919567108\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 19 average loss: 1.922\n",
      "Test Accuracy : 43.8%, Test Loss: 1.8815441131591797\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.79 seconds\n",
      "epoch: 20 average loss: 1.911\n",
      "Test Accuracy : 42.8%, Test Loss: 1.8524058125913143\n",
      "Epoch Time (Training + Test) = 9.80 seconds\n",
      "epoch: 21 average loss: 1.901\n",
      "Test Accuracy : 42.1%, Test Loss: 1.891821961849928\n",
      "Epoch Time (Training + Test) = 9.75 seconds\n",
      "epoch: 22 average loss: 1.893\n",
      "Test Accuracy : 45.0%, Test Loss: 1.850278027355671\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.87 seconds\n",
      "epoch: 23 average loss: 1.898\n",
      "Test Accuracy : 44.5%, Test Loss: 1.8421442247927189\n",
      "Epoch Time (Training + Test) = 9.82 seconds\n",
      "epoch: 24 average loss: 1.884\n",
      "Test Accuracy : 43.8%, Test Loss: 1.829375971108675\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 25 average loss: 1.869\n",
      "Test Accuracy : 44.5%, Test Loss: 1.820755735039711\n",
      "Epoch Time (Training + Test) = 9.82 seconds\n",
      "epoch: 26 average loss: 1.862\n",
      "Test Accuracy : 44.0%, Test Loss: 1.825112134218216\n",
      "Epoch Time (Training + Test) = 9.81 seconds\n",
      "epoch: 27 average loss: 1.856\n",
      "Test Accuracy : 44.9%, Test Loss: 1.8427048809826374\n",
      "Epoch Time (Training + Test) = 9.74 seconds\n",
      "epoch: 28 average loss: 1.848\n",
      "Test Accuracy : 44.3%, Test Loss: 1.8134912550449371\n",
      "Epoch Time (Training + Test) = 9.84 seconds\n",
      "epoch: 29 average loss: 1.848\n",
      "Test Accuracy : 45.1%, Test Loss: 1.8199988417327404\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.79 seconds\n",
      "epoch: 30 average loss: 1.833\n",
      "Test Accuracy : 44.4%, Test Loss: 1.7982295118272305\n",
      "Epoch Time (Training + Test) = 9.84 seconds\n",
      "epoch: 31 average loss: 1.848\n",
      "Test Accuracy : 45.8%, Test Loss: 1.806882318109274\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.87 seconds\n",
      "epoch: 32 average loss: 1.834\n",
      "Test Accuracy : 45.9%, Test Loss: 1.8017930202186108\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.72 seconds\n",
      "epoch: 33 average loss: 1.821\n",
      "Test Accuracy : 44.4%, Test Loss: 1.8017552644014359\n",
      "Epoch Time (Training + Test) = 9.81 seconds\n",
      "epoch: 34 average loss: 1.821\n",
      "Test Accuracy : 44.7%, Test Loss: 1.7987548112869263\n",
      "Epoch Time (Training + Test) = 9.87 seconds\n",
      "epoch: 35 average loss: 1.817\n",
      "Test Accuracy : 45.6%, Test Loss: 1.7998961620032787\n",
      "Epoch Time (Training + Test) = 9.78 seconds\n",
      "epoch: 36 average loss: 1.820\n",
      "Test Accuracy : 43.5%, Test Loss: 1.8167400173842907\n",
      "Epoch Time (Training + Test) = 9.73 seconds\n",
      "epoch: 37 average loss: 1.802\n",
      "Test Accuracy : 44.5%, Test Loss: 1.8230026438832283\n",
      "Epoch Time (Training + Test) = 9.85 seconds\n",
      "epoch: 38 average loss: 1.809\n",
      "Test Accuracy : 45.4%, Test Loss: 1.7749882563948631\n",
      "Epoch Time (Training + Test) = 9.77 seconds\n",
      "epoch: 39 average loss: 1.799\n",
      "Test Accuracy : 45.6%, Test Loss: 1.7882330417633057\n",
      "Epoch Time (Training + Test) = 9.75 seconds\n",
      "epoch: 40 average loss: 1.792\n",
      "Test Accuracy : 45.2%, Test Loss: 1.8058238290250301\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 41 average loss: 1.807\n",
      "Test Accuracy : 45.1%, Test Loss: 1.7996865883469582\n",
      "Epoch Time (Training + Test) = 9.81 seconds\n",
      "epoch: 42 average loss: 1.789\n",
      "Test Accuracy : 45.2%, Test Loss: 1.7757275775074959\n",
      "Epoch Time (Training + Test) = 9.71 seconds\n",
      "epoch: 43 average loss: 1.793\n",
      "Test Accuracy : 45.6%, Test Loss: 1.7745724022388458\n",
      "Epoch Time (Training + Test) = 9.80 seconds\n",
      "epoch: 44 average loss: 1.789\n",
      "Test Accuracy : 46.4%, Test Loss: 1.7638838812708855\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.88 seconds\n",
      "epoch: 45 average loss: 1.794\n",
      "Test Accuracy : 46.5%, Test Loss: 1.7595090828835964\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.70 seconds\n",
      "epoch: 46 average loss: 1.786\n",
      "Test Accuracy : 47.4%, Test Loss: 1.7351876832544804\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.82 seconds\n",
      "epoch: 47 average loss: 1.774\n",
      "Test Accuracy : 46.2%, Test Loss: 1.7570067048072815\n",
      "Epoch Time (Training + Test) = 9.83 seconds\n",
      "epoch: 48 average loss: 1.783\n",
      "Test Accuracy : 45.8%, Test Loss: 1.761619545519352\n",
      "Epoch Time (Training + Test) = 9.80 seconds\n",
      "epoch: 49 average loss: 1.782\n",
      "Test Accuracy : 45.6%, Test Loss: 1.7624681405723095\n",
      "Epoch Time (Training + Test) = 9.70 seconds\n",
      "epoch: 50 average loss: 1.770\n",
      "Test Accuracy : 46.1%, Test Loss: 1.755450826138258\n",
      "Epoch Time (Training + Test) = 9.69 seconds\n",
      "epoch: 51 average loss: 1.768\n",
      "Test Accuracy : 45.9%, Test Loss: 1.7597352713346481\n",
      "Epoch Time (Training + Test) = 9.77 seconds\n",
      "epoch: 52 average loss: 1.769\n",
      "Test Accuracy : 46.7%, Test Loss: 1.719592746347189\n",
      "Epoch Time (Training + Test) = 9.68 seconds\n",
      "epoch: 53 average loss: 1.772\n",
      "Test Accuracy : 44.4%, Test Loss: 1.7607911638915539\n",
      "Epoch Time (Training + Test) = 9.70 seconds\n",
      "epoch: 54 average loss: 1.754\n",
      "Test Accuracy : 46.7%, Test Loss: 1.7312301695346832\n",
      "Epoch Time (Training + Test) = 9.77 seconds\n",
      "epoch: 55 average loss: 1.756\n",
      "Test Accuracy : 46.6%, Test Loss: 1.7354349978268147\n",
      "Epoch Time (Training + Test) = 9.70 seconds\n",
      "epoch: 56 average loss: 1.757\n",
      "Test Accuracy : 45.3%, Test Loss: 1.726475115865469\n",
      "Epoch Time (Training + Test) = 10.07 seconds\n",
      "epoch: 57 average loss: 1.747\n",
      "Test Accuracy : 46.2%, Test Loss: 1.7163045145571232\n",
      "Epoch Time (Training + Test) = 9.84 seconds\n",
      "epoch: 58 average loss: 1.752\n",
      "Test Accuracy : 46.2%, Test Loss: 1.7447792813181877\n",
      "Epoch Time (Training + Test) = 9.80 seconds\n",
      "epoch: 59 average loss: 1.760\n",
      "Test Accuracy : 47.9%, Test Loss: 1.7229761555790901\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.72 seconds\n",
      "epoch: 60 average loss: 1.749\n",
      "Test Accuracy : 46.4%, Test Loss: 1.7281020320951939\n",
      "Epoch Time (Training + Test) = 9.76 seconds\n",
      "epoch: 61 average loss: 1.740\n",
      "Test Accuracy : 46.2%, Test Loss: 1.7365765497088432\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 62 average loss: 1.760\n",
      "Test Accuracy : 47.1%, Test Loss: 1.7031181901693344\n",
      "Epoch Time (Training + Test) = 9.70 seconds\n",
      "epoch: 63 average loss: 1.740\n",
      "Test Accuracy : 45.2%, Test Loss: 1.7430947907269\n",
      "Epoch Time (Training + Test) = 9.67 seconds\n",
      "epoch: 64 average loss: 1.743\n",
      "Test Accuracy : 47.9%, Test Loss: 1.7056978605687618\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.79 seconds\n",
      "epoch: 65 average loss: 1.731\n",
      "Test Accuracy : 47.6%, Test Loss: 1.752174962311983\n",
      "Epoch Time (Training + Test) = 9.73 seconds\n",
      "epoch: 66 average loss: 1.738\n",
      "Test Accuracy : 46.4%, Test Loss: 1.7525415532290936\n",
      "Epoch Time (Training + Test) = 9.79 seconds\n",
      "epoch: 67 average loss: 1.720\n",
      "Test Accuracy : 47.0%, Test Loss: 1.7120646871626377\n",
      "Epoch Time (Training + Test) = 9.75 seconds\n",
      "epoch: 68 average loss: 1.733\n",
      "Test Accuracy : 47.9%, Test Loss: 1.6972421780228615\n",
      "Epoch Time (Training + Test) = 9.76 seconds\n",
      "epoch: 69 average loss: 1.729\n",
      "Test Accuracy : 47.5%, Test Loss: 1.7011039890348911\n",
      "Epoch Time (Training + Test) = 9.72 seconds\n",
      "epoch: 70 average loss: 1.725\n",
      "Test Accuracy : 46.5%, Test Loss: 1.7240006253123283\n",
      "Epoch Time (Training + Test) = 9.97 seconds\n",
      "epoch: 71 average loss: 1.721\n",
      "Test Accuracy : 47.1%, Test Loss: 1.7242667973041534\n",
      "Epoch Time (Training + Test) = 9.74 seconds\n",
      "epoch: 72 average loss: 1.725\n",
      "Test Accuracy : 48.4%, Test Loss: 1.6747284308075905\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.85 seconds\n",
      "epoch: 73 average loss: 1.721\n",
      "Test Accuracy : 47.8%, Test Loss: 1.69195606559515\n",
      "Epoch Time (Training + Test) = 9.83 seconds\n",
      "epoch: 74 average loss: 1.723\n",
      "Test Accuracy : 47.0%, Test Loss: 1.6962257958948612\n",
      "Epoch Time (Training + Test) = 9.87 seconds\n",
      "epoch: 75 average loss: 1.712\n",
      "Test Accuracy : 49.7%, Test Loss: 1.6893258318305016\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.76 seconds\n",
      "Data Saved to Grouped_Convolution_PW_Group_20.csv\n",
      "Finished Training: \n",
      "Total Time 0.408779 hours\n",
      " Average Time Per Epoch 19.62 seconds\n",
      "None None\n",
      "No lr scheduler\n",
      "Optimizer Set to New: {'state': {}, 'param_groups': [{'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': False, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]}]}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3je58h7y) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Current Best Acc</td><td>▁▅▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇████████████████</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>epoch time (s)</td><td>▁▇█▇▇▇▇▇▇▇▇▇▇▇▇▇▇▆▇▆▇▇▆▇▇▇▆▇▆▆▇▆▇▆▆▇▆▇▇▇</td></tr><tr><td>lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_accuracy</td><td>▁▅▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇▇▇▇█▇█▇▇██████</td></tr><tr><td>test_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>training_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Best Accuracy</td><td>0.497</td></tr><tr><td>Current Best Acc</td><td>0.497</td></tr><tr><td>Total Time (hours)</td><td>0.40878</td></tr><tr><td>epoch</td><td>75</td></tr><tr><td>epoch time (s)</td><td>9.75635</td></tr><tr><td>lr</td><td>0.001</td></tr><tr><td>test_accuracy</td><td>0.497</td></tr><tr><td>test_loss</td><td>1.68933</td></tr><tr><td>training_loss</td><td>1.71154</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">autumn-universe-7</strong>: <a href=\"https://wandb.ai/johnny_suu/UniAdapt_Grouped_Convolution/runs/3je58h7y\" target=\"_blank\">https://wandb.ai/johnny_suu/UniAdapt_Grouped_Convolution/runs/3je58h7y</a><br/>Synced 5 W&B file(s), 1 media file(s), 1 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230503_172443-3je58h7y\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3je58h7y). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce0d32a2aac04dd282a1d372ddd2cb5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01693333333435779, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\John Su\\Downloads\\SydneyUni\\2022\\thesis\\Thesis\\wandb\\run-20230503_173720-1v9cfc4k</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/johnny_suu/UniAdapt_Grouped_Convolution/runs/1v9cfc4k\" target=\"_blank\">volcanic-thunder-8</a></strong> to <a href=\"https://wandb.ai/johnny_suu/UniAdapt_Grouped_Convolution\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "Project Name: UniAdapt_Grouped_Convolution, Run Name Grouped_Convolution_PW_Group_None \n",
      "\n",
      "\n",
      "Run Start : 2023-05-03 17-37-20\n",
      "start_epoch : 0\n",
      "initial_lr : 0.001\n",
      "batch_size : 64\n",
      "epochs : 75\n",
      "epoch_chkpts : []\n",
      "device : cuda:0\n",
      "criterion : CrossEntropyLoss()\n",
      "optimizer : Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: False\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "seed : None\n",
      "model_architecture : <class 'NN_Thesis.nn_classes.BNN_Resnet_UniAdapt'>\n",
      "binerised_training : True\n",
      "Number of Elements : 4382060\n",
      "Initial accuracy:\n",
      "Test Accuracy : 5.9%, Test Loss: 41.91401371075089\n",
      "best_acc.pth saved!\n",
      "Test Accuracy : 5.5%, Test Loss: 42.39794921875\n",
      "epoch: 1 average loss: 2.619\n",
      "Test Accuracy : 27.0%, Test Loss: 2.440483346581459\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.65 seconds\n",
      "epoch: 2 average loss: 2.405\n",
      "Test Accuracy : 30.9%, Test Loss: 2.302826948463917\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.67 seconds\n",
      "epoch: 3 average loss: 2.337\n",
      "Test Accuracy : 32.6%, Test Loss: 2.283533826470375\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.64 seconds\n",
      "epoch: 4 average loss: 2.298\n",
      "Test Accuracy : 33.6%, Test Loss: 2.219530873000622\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.89 seconds\n",
      "epoch: 5 average loss: 2.257\n",
      "Test Accuracy : 33.2%, Test Loss: 2.2172079533338547\n",
      "Epoch Time (Training + Test) = 10.21 seconds\n",
      "epoch: 6 average loss: 2.225\n",
      "Test Accuracy : 32.9%, Test Loss: 2.1905109770596027\n",
      "Epoch Time (Training + Test) = 10.11 seconds\n",
      "epoch: 7 average loss: 2.216\n",
      "Test Accuracy : 35.7%, Test Loss: 2.1440219171345234\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.63 seconds\n",
      "epoch: 8 average loss: 2.193\n",
      "Test Accuracy : 36.8%, Test Loss: 2.1114782467484474\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.63 seconds\n",
      "epoch: 9 average loss: 2.180\n",
      "Test Accuracy : 36.8%, Test Loss: 2.100733954459429\n",
      "Epoch Time (Training + Test) = 9.47 seconds\n",
      "epoch: 10 average loss: 2.166\n",
      "Test Accuracy : 35.4%, Test Loss: 2.0999242030084133\n",
      "Epoch Time (Training + Test) = 9.57 seconds\n",
      "epoch: 11 average loss: 2.151\n",
      "Test Accuracy : 38.3%, Test Loss: 2.0772407948970795\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.57 seconds\n",
      "epoch: 12 average loss: 2.138\n",
      "Test Accuracy : 38.1%, Test Loss: 2.066191576421261\n",
      "Epoch Time (Training + Test) = 9.61 seconds\n",
      "epoch: 13 average loss: 2.133\n",
      "Test Accuracy : 37.2%, Test Loss: 2.0698789544403553\n",
      "Epoch Time (Training + Test) = 9.48 seconds\n",
      "epoch: 14 average loss: 2.105\n",
      "Test Accuracy : 37.0%, Test Loss: 2.045967251062393\n",
      "Epoch Time (Training + Test) = 9.49 seconds\n",
      "epoch: 15 average loss: 2.099\n",
      "Test Accuracy : 37.1%, Test Loss: 2.096678152680397\n",
      "Epoch Time (Training + Test) = 9.59 seconds\n",
      "epoch: 16 average loss: 2.102\n",
      "Test Accuracy : 37.6%, Test Loss: 2.0324356891214848\n",
      "Epoch Time (Training + Test) = 9.48 seconds\n",
      "epoch: 17 average loss: 2.086\n",
      "Test Accuracy : 37.7%, Test Loss: 2.0613975264132023\n",
      "Epoch Time (Training + Test) = 9.59 seconds\n",
      "epoch: 18 average loss: 2.084\n",
      "Test Accuracy : 37.8%, Test Loss: 2.0168073512613773\n",
      "Epoch Time (Training + Test) = 9.50 seconds\n",
      "epoch: 19 average loss: 2.068\n",
      "Test Accuracy : 37.0%, Test Loss: 2.037160936743021\n",
      "Epoch Time (Training + Test) = 9.47 seconds\n",
      "epoch: 20 average loss: 2.068\n",
      "Test Accuracy : 37.2%, Test Loss: 2.0407378673553467\n",
      "Epoch Time (Training + Test) = 9.60 seconds\n",
      "epoch: 21 average loss: 2.056\n",
      "Test Accuracy : 39.6%, Test Loss: 1.9920920431613922\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.58 seconds\n",
      "epoch: 22 average loss: 2.049\n",
      "Test Accuracy : 39.5%, Test Loss: 2.009259793907404\n",
      "Epoch Time (Training + Test) = 9.55 seconds\n",
      "epoch: 23 average loss: 2.053\n",
      "Test Accuracy : 39.6%, Test Loss: 2.013067875057459\n",
      "Epoch Time (Training + Test) = 9.50 seconds\n",
      "epoch: 24 average loss: 2.051\n",
      "Test Accuracy : 38.3%, Test Loss: 2.0496871918439865\n",
      "Epoch Time (Training + Test) = 9.56 seconds\n",
      "epoch: 25 average loss: 2.041\n",
      "Test Accuracy : 39.4%, Test Loss: 1.9994900040328503\n",
      "Epoch Time (Training + Test) = 9.53 seconds\n",
      "epoch: 26 average loss: 2.019\n",
      "Test Accuracy : 40.4%, Test Loss: 1.9851443395018578\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.66 seconds\n",
      "epoch: 27 average loss: 2.026\n",
      "Test Accuracy : 40.6%, Test Loss: 1.975174393504858\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.60 seconds\n",
      "epoch: 28 average loss: 2.015\n",
      "Test Accuracy : 40.8%, Test Loss: 1.9608780853450298\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.66 seconds\n",
      "epoch: 29 average loss: 2.009\n",
      "Test Accuracy : 39.8%, Test Loss: 1.9706389121711254\n",
      "Epoch Time (Training + Test) = 9.67 seconds\n",
      "epoch: 30 average loss: 2.011\n",
      "Test Accuracy : 41.9%, Test Loss: 1.9806546121835709\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.48 seconds\n",
      "epoch: 31 average loss: 1.994\n",
      "Test Accuracy : 39.2%, Test Loss: 1.9918175637722015\n",
      "Epoch Time (Training + Test) = 9.64 seconds\n",
      "epoch: 32 average loss: 2.005\n",
      "Test Accuracy : 39.7%, Test Loss: 1.9802465215325356\n",
      "Epoch Time (Training + Test) = 9.47 seconds\n",
      "epoch: 33 average loss: 2.002\n",
      "Test Accuracy : 41.6%, Test Loss: 1.946899551898241\n",
      "Epoch Time (Training + Test) = 9.53 seconds\n",
      "epoch: 34 average loss: 1.999\n",
      "Test Accuracy : 40.6%, Test Loss: 1.9594986252486706\n",
      "Epoch Time (Training + Test) = 9.46 seconds\n",
      "epoch: 35 average loss: 2.002\n",
      "Test Accuracy : 39.8%, Test Loss: 1.9668501764535904\n",
      "Epoch Time (Training + Test) = 9.48 seconds\n",
      "epoch: 36 average loss: 1.998\n",
      "Test Accuracy : 41.1%, Test Loss: 1.9541287124156952\n",
      "Epoch Time (Training + Test) = 9.41 seconds\n",
      "epoch: 37 average loss: 1.976\n",
      "Test Accuracy : 41.0%, Test Loss: 1.9304338619112968\n",
      "Epoch Time (Training + Test) = 9.54 seconds\n",
      "epoch: 38 average loss: 1.978\n",
      "Test Accuracy : 40.3%, Test Loss: 1.9615158513188362\n",
      "Epoch Time (Training + Test) = 9.47 seconds\n",
      "epoch: 39 average loss: 1.990\n",
      "Test Accuracy : 39.2%, Test Loss: 1.9782817251980305\n",
      "Epoch Time (Training + Test) = 9.44 seconds\n",
      "epoch: 40 average loss: 1.979\n",
      "Test Accuracy : 40.5%, Test Loss: 1.9595241285860538\n",
      "Epoch Time (Training + Test) = 9.44 seconds\n",
      "epoch: 41 average loss: 1.982\n",
      "Test Accuracy : 39.4%, Test Loss: 1.9613126255571842\n",
      "Epoch Time (Training + Test) = 9.65 seconds\n",
      "epoch: 42 average loss: 1.983\n",
      "Test Accuracy : 39.8%, Test Loss: 1.9607842825353146\n",
      "Epoch Time (Training + Test) = 9.56 seconds\n",
      "epoch: 43 average loss: 1.979\n",
      "Test Accuracy : 41.9%, Test Loss: 1.9109970964491367\n",
      "Epoch Time (Training + Test) = 9.45 seconds\n",
      "epoch: 44 average loss: 1.972\n",
      "Test Accuracy : 41.5%, Test Loss: 1.9259330071508884\n",
      "Epoch Time (Training + Test) = 9.48 seconds\n",
      "epoch: 45 average loss: 1.964\n",
      "Test Accuracy : 40.6%, Test Loss: 1.9434900768101215\n",
      "Epoch Time (Training + Test) = 9.50 seconds\n",
      "epoch: 46 average loss: 1.963\n",
      "Test Accuracy : 40.8%, Test Loss: 1.9405087679624557\n",
      "Epoch Time (Training + Test) = 9.47 seconds\n",
      "epoch: 47 average loss: 1.958\n",
      "Test Accuracy : 42.8%, Test Loss: 1.8838859423995018\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.60 seconds\n",
      "epoch: 48 average loss: 1.967\n",
      "Test Accuracy : 43.1%, Test Loss: 1.8861465640366077\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.62 seconds\n",
      "epoch: 49 average loss: 1.954\n",
      "Test Accuracy : 42.5%, Test Loss: 1.8828252367675304\n",
      "Epoch Time (Training + Test) = 9.45 seconds\n",
      "epoch: 50 average loss: 1.970\n",
      "Test Accuracy : 42.2%, Test Loss: 1.9183035157620907\n",
      "Epoch Time (Training + Test) = 9.44 seconds\n",
      "epoch: 51 average loss: 1.959\n",
      "Test Accuracy : 41.5%, Test Loss: 1.899907924234867\n",
      "Epoch Time (Training + Test) = 9.51 seconds\n",
      "epoch: 52 average loss: 1.949\n",
      "Test Accuracy : 41.6%, Test Loss: 1.9211006499826908\n",
      "Epoch Time (Training + Test) = 9.46 seconds\n",
      "epoch: 53 average loss: 1.963\n",
      "Test Accuracy : 41.6%, Test Loss: 1.9168924614787102\n",
      "Epoch Time (Training + Test) = 9.46 seconds\n",
      "epoch: 54 average loss: 1.952\n",
      "Test Accuracy : 40.6%, Test Loss: 1.9131183214485645\n",
      "Epoch Time (Training + Test) = 9.47 seconds\n",
      "epoch: 55 average loss: 1.944\n",
      "Test Accuracy : 42.9%, Test Loss: 1.8671406656503677\n",
      "Epoch Time (Training + Test) = 9.57 seconds\n",
      "epoch: 56 average loss: 1.958\n",
      "Test Accuracy : 42.2%, Test Loss: 1.9020951166749\n",
      "Epoch Time (Training + Test) = 9.54 seconds\n",
      "epoch: 57 average loss: 1.952\n",
      "Test Accuracy : 42.9%, Test Loss: 1.8844169862568378\n",
      "Epoch Time (Training + Test) = 9.44 seconds\n",
      "epoch: 58 average loss: 1.948\n",
      "Test Accuracy : 43.0%, Test Loss: 1.9087023958563805\n",
      "Epoch Time (Training + Test) = 9.61 seconds\n",
      "epoch: 59 average loss: 1.942\n",
      "Test Accuracy : 41.9%, Test Loss: 1.924902357161045\n",
      "Epoch Time (Training + Test) = 9.61 seconds\n",
      "epoch: 60 average loss: 1.932\n",
      "Test Accuracy : 42.4%, Test Loss: 1.8791726306080818\n",
      "Epoch Time (Training + Test) = 9.49 seconds\n",
      "epoch: 61 average loss: 1.931\n",
      "Test Accuracy : 42.8%, Test Loss: 1.8835469149053097\n",
      "Epoch Time (Training + Test) = 9.49 seconds\n",
      "epoch: 62 average loss: 1.949\n",
      "Test Accuracy : 41.9%, Test Loss: 1.9019471257925034\n",
      "Epoch Time (Training + Test) = 9.51 seconds\n",
      "epoch: 63 average loss: 1.930\n",
      "Test Accuracy : 42.5%, Test Loss: 1.8948464505374432\n",
      "Epoch Time (Training + Test) = 9.60 seconds\n",
      "epoch: 64 average loss: 1.942\n",
      "Test Accuracy : 42.0%, Test Loss: 1.9150608703494072\n",
      "Epoch Time (Training + Test) = 9.56 seconds\n",
      "epoch: 65 average loss: 1.928\n",
      "Test Accuracy : 42.6%, Test Loss: 1.8797094859182835\n",
      "Epoch Time (Training + Test) = 9.51 seconds\n",
      "epoch: 66 average loss: 1.932\n",
      "Test Accuracy : 42.2%, Test Loss: 1.8892297595739365\n",
      "Epoch Time (Training + Test) = 9.49 seconds\n",
      "epoch: 67 average loss: 1.923\n",
      "Test Accuracy : 41.9%, Test Loss: 1.890447124838829\n",
      "Epoch Time (Training + Test) = 9.62 seconds\n",
      "epoch: 68 average loss: 1.938\n",
      "Test Accuracy : 42.3%, Test Loss: 1.8591949976980686\n",
      "Epoch Time (Training + Test) = 9.64 seconds\n",
      "epoch: 69 average loss: 1.933\n",
      "Test Accuracy : 43.1%, Test Loss: 1.8837722763419151\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.59 seconds\n",
      "epoch: 70 average loss: 1.921\n",
      "Test Accuracy : 41.1%, Test Loss: 1.9261214397847652\n",
      "Epoch Time (Training + Test) = 9.90 seconds\n",
      "epoch: 71 average loss: 1.925\n",
      "Test Accuracy : 43.1%, Test Loss: 1.8847985863685608\n",
      "Epoch Time (Training + Test) = 9.92 seconds\n",
      "epoch: 72 average loss: 1.928\n",
      "Test Accuracy : 42.7%, Test Loss: 1.8570269122719765\n",
      "Epoch Time (Training + Test) = 9.56 seconds\n",
      "epoch: 73 average loss: 1.923\n",
      "Test Accuracy : 43.5%, Test Loss: 1.8746892474591732\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.50 seconds\n",
      "epoch: 74 average loss: 1.912\n",
      "Test Accuracy : 44.9%, Test Loss: 1.8570145964622498\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.65 seconds\n",
      "epoch: 75 average loss: 1.923\n",
      "Test Accuracy : 41.1%, Test Loss: 1.9078620970249176\n",
      "Epoch Time (Training + Test) = 9.49 seconds\n",
      "Data Saved to Grouped_Convolution_PW_Group_None.csv\n",
      "Finished Training: \n",
      "Total Time 0.398859 hours\n",
      " Average Time Per Epoch 19.15 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "conv2_groups = [1,2,5,10,20,None]\n",
    "conv1_groups = [None]*len(conv2_groups)\n",
    "\n",
    "for conv1_group, conv2_group in zip(conv1_groups,conv2_groups):\n",
    "    print(conv1_group,conv2_group)\n",
    "    \n",
    "    BNN = BNN_Resnet_UniAdapt(80,freeze_pre_weights=True)\n",
    "    BNN.layer3[-1].do_bntan = True\n",
    "    BNN.load_state_dict(state,strict=False)\n",
    "    BNN.freeze()\n",
    "\n",
    "    BNN = BNN.to('cuda:0')\n",
    "    BNN.encoder_net = placeholder()\n",
    "\n",
    "    n1 = uniAdapt_Net([80,160,320],[80,160,320],block = thinBlock2,conv1_group = conv1_group,conv2_group = conv2_group)\n",
    "    n1 = n1.cuda()\n",
    "    n1.train()\n",
    "\n",
    "    adapt_net = nn.Sequential(n1,nn.Flatten())\n",
    "    \n",
    "    adapt_net.to('cuda:0')\n",
    "\n",
    "    BNN.head_bn =  nn.BatchNorm1d(20)\n",
    "    BNN.head =  BinarizeLinear(320 + 320,20)\n",
    "    BNN.add_UniAdapter(adapt_net)\n",
    "    BNN = BNN.to('cuda:0')\n",
    "    BNN.uniAdapt = True\n",
    "    \n",
    "    trainer = Trainer(BNN,model_name=f'Grouped_Convolution_PW_Group_{conv2_group}',project_name='UniAdapt_Grouped_Convolution',binarise=True,classes=train_20.classes)\n",
    "    trainer.lr = 1e-3\n",
    "    trainer.batch_size = 64\n",
    "    trainer.epochs =75\n",
    "    trainer.epoch_chkpts = []\n",
    "    trainer.start_epoch = 0\n",
    "    # trainer.amount = [0.8,0]\n",
    "    # trainer.pruning_rounds = 1 # Only need this for iterative pruning\n",
    "    # trainer.model_to_prune = trainer.model.uniAdaptNet[0]\n",
    "    params = nn.ModuleList([BNN.uniAdaptNet,BNN.head_bn,BNN.head]).parameters()\n",
    "    # params = nn.ModuleList([BNN.head_bn,BNN.head]).parameters()\n",
    "    trainer.set_scheduler(None)\n",
    "    trainer.set_optimizer(torch.optim.Adam,params,lr = trainer.lr)\n",
    "    trainer.train(train20_DL,test20_DL,prune_strat=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "205703fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "174160\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(174160, 1741.6000000000015)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "el = total_elem(adapt_net)\n",
    "el,el*(1-0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c43ca6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "BNN.freeze()\n",
    "BNN.head_bn =  nn.BatchNorm1d(20)\n",
    "BNN.head =  BinarizeLinear(320 + 320,20)\n",
    "BNN.add_UniAdapter(adapt_net)\n",
    "BNN = BNN.to('cuda:0')\n",
    "BNN.uniAdapt = True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e6c9ef46",
   "metadata": {},
   "source": [
    "## pruning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "98215a29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No lr scheduler\n",
      "Optimizer Set to New: {'state': {}, 'param_groups': [{'lr': 0.0001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': False, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]}]}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3gg5yxba) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Current Best Acc</td><td>▁▇██████████████████████████████████████</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>epoch time (s)</td><td>▁▇▆▇▇▆▇▇▇▇▆▇▇▇█▇▇▆▅▅▆▇▇▆▆▇▇▇█▇▇▅▆▆▅▆▆▅▅▅</td></tr><tr><td>lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_accuracy</td><td>▁▇█▇████████████████████████████████████</td></tr><tr><td>test_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>training_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Best Accuracy</td><td>0.6995</td></tr><tr><td>Current Best Acc</td><td>0.6995</td></tr><tr><td>Total Time (hours)</td><td>0.41555</td></tr><tr><td>epoch</td><td>75</td></tr><tr><td>epoch time (s)</td><td>9.65845</td></tr><tr><td>lr</td><td>0.001</td></tr><tr><td>test_accuracy</td><td>0.678</td></tr><tr><td>test_loss</td><td>0.99472</td></tr><tr><td>training_loss</td><td>1.04471</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">kind-sea-6</strong>: <a href=\"https://wandb.ai/johnny_suu/UniAdapt_Cifar100_Finetuning/runs/3gg5yxba\" target=\"_blank\">https://wandb.ai/johnny_suu/UniAdapt_Cifar100_Finetuning/runs/3gg5yxba</a><br/>Synced 5 W&B file(s), 1 media file(s), 1 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230421_103859-3gg5yxba\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3gg5yxba). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d76a97ac5b8d4938a6e1488b2901ad80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016916666666899498, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\John Su\\Downloads\\SydneyUni\\2022\\thesis\\Thesis\\wandb\\run-20230421_105144-1lsqv3f4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/johnny_suu/UniAdapt_Cifar100_Finetuning/runs/1lsqv3f4\" target=\"_blank\">deft-sea-7</a></strong> to <a href=\"https://wandb.ai/johnny_suu/UniAdapt_Cifar100_Finetuning\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "Project Name: UniAdapt_Cifar100_Finetuning, Run Name BNN_UniAdapt_finetune_thinblock2_80%prune_conv2only \n",
      "\n",
      "\n",
      "Run Start : 2023-04-21 10-51-44\n",
      "start_epoch : 0\n",
      "initial_lr : 0.0001\n",
      "batch_size : 64\n",
      "epochs : 75\n",
      "epoch_chkpts : []\n",
      "device : cuda:0\n",
      "criterion : CrossEntropyLoss()\n",
      "optimizer : Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: False\n",
      "    lr: 0.0001\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "seed : None\n",
      "model_architecture : <class 'NN_Thesis.nn_classes.BNN_Resnet_UniAdapt'>\n",
      "binerised_training : True\n",
      "Number of Elements : 4382060\n",
      "Initial accuracy:\n",
      "Test Accuracy : 4.5%, Test Loss: 54.62386307443023\n",
      "best_acc.pth saved!\n",
      "Test Accuracy : 4.0%, Test Loss: 51.46666193008423\n",
      "epoch: 1 average loss: 2.760\n",
      "Test Accuracy : 31.9%, Test Loss: 2.240475356578827\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.70 seconds\n",
      "epoch: 2 average loss: 2.111\n",
      "Test Accuracy : 45.0%, Test Loss: 1.910510253161192\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.75 seconds\n",
      "epoch: 3 average loss: 1.906\n",
      "Test Accuracy : 51.6%, Test Loss: 1.7894702516496181\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 10.26 seconds\n",
      "epoch: 4 average loss: 1.794\n",
      "Test Accuracy : 54.8%, Test Loss: 1.6879839561879635\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 10.28 seconds\n",
      "epoch: 5 average loss: 1.709\n",
      "Test Accuracy : 58.1%, Test Loss: 1.5979379676282406\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 10.16 seconds\n",
      "epoch: 6 average loss: 1.647\n",
      "Test Accuracy : 60.2%, Test Loss: 1.5570880435407162\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 10.49 seconds\n",
      "epoch: 7 average loss: 1.601\n",
      "Test Accuracy : 61.4%, Test Loss: 1.5072240121662617\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 10.81 seconds\n",
      "epoch: 8 average loss: 1.567\n",
      "Test Accuracy : 63.1%, Test Loss: 1.469915471971035\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 10.20 seconds\n",
      "epoch: 9 average loss: 1.536\n",
      "Test Accuracy : 63.8%, Test Loss: 1.451170314103365\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 10.23 seconds\n",
      "epoch: 10 average loss: 1.508\n",
      "Test Accuracy : 64.2%, Test Loss: 1.427487563341856\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.93 seconds\n",
      "epoch: 11 average loss: 1.483\n",
      "Test Accuracy : 62.3%, Test Loss: 1.4204634465277195\n",
      "Epoch Time (Training + Test) = 10.02 seconds\n",
      "epoch: 12 average loss: 1.476\n",
      "Test Accuracy : 65.3%, Test Loss: 1.3874026201665401\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.78 seconds\n",
      "epoch: 13 average loss: 1.465\n",
      "Test Accuracy : 64.5%, Test Loss: 1.3743031397461891\n",
      "Epoch Time (Training + Test) = 9.88 seconds\n",
      "epoch: 14 average loss: 1.451\n",
      "Test Accuracy : 65.9%, Test Loss: 1.350250493735075\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 15 average loss: 1.434\n",
      "Test Accuracy : 66.4%, Test Loss: 1.3345976695418358\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.90 seconds\n",
      "epoch: 16 average loss: 1.425\n",
      "Test Accuracy : 66.1%, Test Loss: 1.333022627979517\n",
      "Epoch Time (Training + Test) = 9.86 seconds\n",
      "epoch: 17 average loss: 1.414\n",
      "Test Accuracy : 66.0%, Test Loss: 1.3259269781410694\n",
      "Epoch Time (Training + Test) = 9.82 seconds\n",
      "epoch: 18 average loss: 1.402\n",
      "Test Accuracy : 68.1%, Test Loss: 1.3136142082512379\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.81 seconds\n",
      "epoch: 19 average loss: 1.393\n",
      "Test Accuracy : 67.0%, Test Loss: 1.3082431629300117\n",
      "Epoch Time (Training + Test) = 9.78 seconds\n",
      "epoch: 20 average loss: 1.377\n",
      "Test Accuracy : 67.5%, Test Loss: 1.3039239048957825\n",
      "Epoch Time (Training + Test) = 10.02 seconds\n",
      "epoch: 21 average loss: 1.369\n",
      "Test Accuracy : 66.5%, Test Loss: 1.279546644538641\n",
      "Epoch Time (Training + Test) = 10.18 seconds\n",
      "epoch: 22 average loss: 1.366\n",
      "Test Accuracy : 66.2%, Test Loss: 1.2913882806897163\n",
      "Epoch Time (Training + Test) = 11.27 seconds\n",
      "epoch: 23 average loss: 1.364\n",
      "Test Accuracy : 68.0%, Test Loss: 1.2756861187517643\n",
      "Epoch Time (Training + Test) = 10.15 seconds\n",
      "epoch: 24 average loss: 1.346\n",
      "Test Accuracy : 66.3%, Test Loss: 1.2651163674890995\n",
      "Epoch Time (Training + Test) = 10.17 seconds\n",
      "epoch: 25 average loss: 1.341\n",
      "Test Accuracy : 67.3%, Test Loss: 1.2649363093078136\n",
      "Epoch Time (Training + Test) = 9.93 seconds\n",
      "epoch: 26 average loss: 1.346\n",
      "Test Accuracy : 67.2%, Test Loss: 1.2565608695149422\n",
      "Epoch Time (Training + Test) = 11.25 seconds\n",
      "epoch: 27 average loss: 1.325\n",
      "Test Accuracy : 67.2%, Test Loss: 1.2579171285033226\n",
      "Epoch Time (Training + Test) = 11.43 seconds\n",
      "epoch: 28 average loss: 1.330\n",
      "Test Accuracy : 67.5%, Test Loss: 1.2186682000756264\n",
      "Epoch Time (Training + Test) = 10.23 seconds\n",
      "epoch: 29 average loss: 1.324\n",
      "Test Accuracy : 68.0%, Test Loss: 1.227983757853508\n",
      "Epoch Time (Training + Test) = 10.39 seconds\n",
      "epoch: 30 average loss: 1.308\n",
      "Test Accuracy : 67.2%, Test Loss: 1.2452318854629993\n",
      "Epoch Time (Training + Test) = 11.52 seconds\n",
      "epoch: 31 average loss: 1.314\n",
      "Test Accuracy : 67.2%, Test Loss: 1.234775710850954\n",
      "Epoch Time (Training + Test) = 10.25 seconds\n",
      "epoch: 32 average loss: 1.303\n",
      "Test Accuracy : 67.9%, Test Loss: 1.2292394116520882\n",
      "Epoch Time (Training + Test) = 10.84 seconds\n",
      "epoch: 33 average loss: 1.297\n",
      "Test Accuracy : 67.7%, Test Loss: 1.2202627770602703\n",
      "Epoch Time (Training + Test) = 10.46 seconds\n",
      "epoch: 34 average loss: 1.299\n",
      "Test Accuracy : 66.3%, Test Loss: 1.2230336926877499\n",
      "Epoch Time (Training + Test) = 10.96 seconds\n",
      "epoch: 35 average loss: 1.282\n",
      "Test Accuracy : 66.8%, Test Loss: 1.2177804000675678\n",
      "Epoch Time (Training + Test) = 10.85 seconds\n",
      "epoch: 36 average loss: 1.287\n",
      "Test Accuracy : 68.2%, Test Loss: 1.2212413363158703\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.85 seconds\n",
      "epoch: 37 average loss: 1.284\n",
      "Test Accuracy : 66.5%, Test Loss: 1.2085833586752415\n",
      "Epoch Time (Training + Test) = 9.68 seconds\n",
      "conv2\n",
      "conv3\n",
      "conv2\n",
      "conv3\n",
      "conv2\n",
      "conv3\n",
      "Pruned [0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0, 0.8, 0]% of weights at epoch 37\n",
      "epoch: 38 average loss: 1.311\n",
      "Test Accuracy : 67.2%, Test Loss: 1.2119153030216694\n",
      "Epoch Time (Training + Test) = 9.89 seconds\n",
      "epoch: 39 average loss: 1.299\n",
      "Test Accuracy : 66.1%, Test Loss: 1.2419662550091743\n",
      "Epoch Time (Training + Test) = 9.97 seconds\n",
      "epoch: 40 average loss: 1.295\n",
      "Test Accuracy : 66.2%, Test Loss: 1.230066318064928\n",
      "Epoch Time (Training + Test) = 9.91 seconds\n",
      "epoch: 41 average loss: 1.287\n",
      "Test Accuracy : 67.1%, Test Loss: 1.2353166099637747\n",
      "Epoch Time (Training + Test) = 9.77 seconds\n",
      "epoch: 42 average loss: 1.278\n",
      "Test Accuracy : 66.7%, Test Loss: 1.224669074639678\n",
      "Epoch Time (Training + Test) = 9.70 seconds\n",
      "epoch: 43 average loss: 1.272\n",
      "Test Accuracy : 66.5%, Test Loss: 1.2394430860877037\n",
      "Epoch Time (Training + Test) = 10.20 seconds\n",
      "epoch: 44 average loss: 1.273\n",
      "Test Accuracy : 65.4%, Test Loss: 1.2121774181723595\n",
      "Epoch Time (Training + Test) = 10.21 seconds\n",
      "epoch: 45 average loss: 1.270\n",
      "Test Accuracy : 67.0%, Test Loss: 1.1952095199376345\n",
      "Epoch Time (Training + Test) = 9.85 seconds\n",
      "epoch: 46 average loss: 1.259\n",
      "Test Accuracy : 66.6%, Test Loss: 1.2126495763659477\n",
      "Epoch Time (Training + Test) = 10.02 seconds\n",
      "epoch: 47 average loss: 1.275\n",
      "Test Accuracy : 67.0%, Test Loss: 1.2057682648301125\n",
      "Epoch Time (Training + Test) = 9.90 seconds\n",
      "epoch: 48 average loss: 1.259\n",
      "Test Accuracy : 67.5%, Test Loss: 1.201635006815195\n",
      "Epoch Time (Training + Test) = 9.75 seconds\n",
      "epoch: 49 average loss: 1.265\n",
      "Test Accuracy : 66.5%, Test Loss: 1.1952783167362213\n",
      "Epoch Time (Training + Test) = 9.59 seconds\n",
      "epoch: 50 average loss: 1.241\n",
      "Test Accuracy : 67.5%, Test Loss: 1.1886894181370735\n",
      "Epoch Time (Training + Test) = 10.07 seconds\n",
      "epoch: 51 average loss: 1.243\n",
      "Test Accuracy : 66.4%, Test Loss: 1.1894046775996685\n",
      "Epoch Time (Training + Test) = 10.02 seconds\n",
      "epoch: 52 average loss: 1.243\n",
      "Test Accuracy : 66.6%, Test Loss: 1.1934736892580986\n",
      "Epoch Time (Training + Test) = 10.13 seconds\n",
      "epoch: 53 average loss: 1.243\n",
      "Test Accuracy : 67.6%, Test Loss: 1.1750495322048664\n",
      "Epoch Time (Training + Test) = 10.03 seconds\n",
      "epoch: 54 average loss: 1.245\n",
      "Test Accuracy : 67.5%, Test Loss: 1.1672741267830133\n",
      "Epoch Time (Training + Test) = 10.26 seconds\n",
      "epoch: 55 average loss: 1.244\n",
      "Test Accuracy : 67.1%, Test Loss: 1.1841402407735586\n",
      "Epoch Time (Training + Test) = 10.25 seconds\n",
      "epoch: 56 average loss: 1.232\n",
      "Test Accuracy : 66.2%, Test Loss: 1.1836590338498354\n",
      "Epoch Time (Training + Test) = 10.31 seconds\n",
      "epoch: 57 average loss: 1.221\n",
      "Test Accuracy : 66.6%, Test Loss: 1.1726614367216825\n",
      "Epoch Time (Training + Test) = 9.87 seconds\n",
      "epoch: 58 average loss: 1.240\n",
      "Test Accuracy : 67.4%, Test Loss: 1.1660094615072012\n",
      "Epoch Time (Training + Test) = 10.08 seconds\n",
      "epoch: 59 average loss: 1.228\n",
      "Test Accuracy : 66.9%, Test Loss: 1.1547786500304937\n",
      "Epoch Time (Training + Test) = 10.40 seconds\n",
      "epoch: 60 average loss: 1.226\n",
      "Test Accuracy : 66.3%, Test Loss: 1.1763879526406527\n",
      "Epoch Time (Training + Test) = 10.22 seconds\n",
      "epoch: 61 average loss: 1.223\n",
      "Test Accuracy : 65.9%, Test Loss: 1.1610219161957502\n",
      "Epoch Time (Training + Test) = 9.95 seconds\n",
      "epoch: 62 average loss: 1.203\n",
      "Test Accuracy : 67.2%, Test Loss: 1.1543750930577517\n",
      "Epoch Time (Training + Test) = 10.05 seconds\n",
      "epoch: 63 average loss: 1.224\n",
      "Test Accuracy : 67.0%, Test Loss: 1.150638785213232\n",
      "Epoch Time (Training + Test) = 9.73 seconds\n",
      "epoch: 64 average loss: 1.217\n",
      "Test Accuracy : 66.3%, Test Loss: 1.1612347234040499\n",
      "Epoch Time (Training + Test) = 9.87 seconds\n",
      "epoch: 65 average loss: 1.221\n",
      "Test Accuracy : 68.8%, Test Loss: 1.1344867050647736\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 9.59 seconds\n",
      "epoch: 66 average loss: 1.202\n",
      "Test Accuracy : 67.2%, Test Loss: 1.1506572365760803\n",
      "Epoch Time (Training + Test) = 9.43 seconds\n",
      "epoch: 67 average loss: 1.209\n",
      "Test Accuracy : 65.2%, Test Loss: 1.1712078228592873\n",
      "Epoch Time (Training + Test) = 9.50 seconds\n",
      "epoch: 68 average loss: 1.201\n",
      "Test Accuracy : 67.8%, Test Loss: 1.1374430228024721\n",
      "Epoch Time (Training + Test) = 10.34 seconds\n",
      "epoch: 69 average loss: 1.205\n",
      "Test Accuracy : 67.4%, Test Loss: 1.1404081583023071\n",
      "Epoch Time (Training + Test) = 10.20 seconds\n",
      "epoch: 70 average loss: 1.207\n",
      "Test Accuracy : 67.2%, Test Loss: 1.1232229620218277\n",
      "Epoch Time (Training + Test) = 9.79 seconds\n",
      "epoch: 71 average loss: 1.193\n",
      "Test Accuracy : 68.7%, Test Loss: 1.1277260705828667\n",
      "Epoch Time (Training + Test) = 9.78 seconds\n",
      "epoch: 72 average loss: 1.199\n",
      "Test Accuracy : 66.9%, Test Loss: 1.1305162943899632\n",
      "Epoch Time (Training + Test) = 9.83 seconds\n",
      "epoch: 73 average loss: 1.196\n",
      "Test Accuracy : 67.2%, Test Loss: 1.1537618823349476\n",
      "Epoch Time (Training + Test) = 9.80 seconds\n",
      "epoch: 74 average loss: 1.195\n",
      "Test Accuracy : 66.1%, Test Loss: 1.1644332315772772\n",
      "Epoch Time (Training + Test) = 9.74 seconds\n",
      "epoch: 75 average loss: 1.193\n",
      "Test Accuracy : 66.8%, Test Loss: 1.1392162162810564\n",
      "Epoch Time (Training + Test) = 9.68 seconds\n",
      "Data Saved to BNN_UniAdapt_finetune_thinblock2_80%prune_conv2only.csv\n",
      "Finished Training: \n",
      "Total Time 0.420920 hours\n",
      " Average Time Per Epoch 20.20 seconds\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(BNN,model_name='BNN_UniAdapt_finetune_thinblock2_80%prune_conv2only',project_name='UniAdapt_Cifar100_Finetuning',binarise=True,classes=train_20.classes)\n",
    "trainer.lr = 1e-4\n",
    "trainer.batch_size = 64\n",
    "trainer.epochs =75\n",
    "trainer.epoch_chkpts = []\n",
    "trainer.start_epoch = 0\n",
    "trainer.amount = [0.8,0]\n",
    "# trainer.pruning_rounds = 1 # Only need this for iterative pruning\n",
    "trainer.model_to_prune = trainer.model.uniAdaptNet[0]\n",
    "params = nn.ModuleList([BNN.uniAdaptNet,BNN.head_bn,BNN.head]).parameters()\n",
    "# params = nn.ModuleList([BNN.head_bn,BNN.head]).parameters()\n",
    "trainer.set_scheduler(None)\n",
    "trainer.set_optimizer(torch.optim.Adam,params,lr = trainer.lr)\n",
    "trainer.train(train20_DL,test20_DL,prune_strat='oneshot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "586cab6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([80, 1, 3, 3])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BNN.uniAdaptNet[0].layer1.conv2.weight_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3949417d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No lr scheduler\n",
      "Optimizer Set to New: {'state': {}, 'param_groups': [{'lr': 0.0001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': False, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]}]}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1lsqv3f4) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Current Best Acc</td><td>▁▄▆▇▇▇▇█████████████████████████████████</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>epoch time (s)</td><td>▁▃▅▄▆▅▄▄▄▄▃▅▄▄▇▅█▆▇▄▄▄▃▅▄▃▄▄▄▅▄▅▄▃▃▃▅▃▄▃</td></tr><tr><td>lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_accuracy</td><td>▁▄▆▇▇▇▇█████████████████████████████████</td></tr><tr><td>test_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>training_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Best Accuracy</td><td>0.6885</td></tr><tr><td>Current Best Acc</td><td>0.6885</td></tr><tr><td>Total Time (hours)</td><td>0.42092</td></tr><tr><td>epoch</td><td>75</td></tr><tr><td>epoch time (s)</td><td>9.6822</td></tr><tr><td>lr</td><td>0.0001</td></tr><tr><td>test_accuracy</td><td>0.6685</td></tr><tr><td>test_loss</td><td>1.13922</td></tr><tr><td>training_loss</td><td>1.1935</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">deft-sea-7</strong>: <a href=\"https://wandb.ai/johnny_suu/UniAdapt_Cifar100_Finetuning/runs/1lsqv3f4\" target=\"_blank\">https://wandb.ai/johnny_suu/UniAdapt_Cifar100_Finetuning/runs/1lsqv3f4</a><br/>Synced 5 W&B file(s), 1 media file(s), 1 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230421_105144-1lsqv3f4\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1lsqv3f4). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35a0e5c4d7114985ab636282bdaed451",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016933333332417533, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\John Su\\Downloads\\SydneyUni\\2022\\thesis\\Thesis\\wandb\\run-20230421_110442-2g31vlmk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/johnny_suu/UniAdapt_Cifar100_Finetuning/runs/2g31vlmk\" target=\"_blank\">generous-music-8</a></strong> to <a href=\"https://wandb.ai/johnny_suu/UniAdapt_Cifar100_Finetuning\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "Project Name: UniAdapt_Cifar100_Finetuning, Run Name BNN_UniAdapt_finetune_thinblock3_99%prune_conv2only \n",
      "\n",
      "\n",
      "Run Start : 2023-04-21 11-04-42\n",
      "start_epoch : 0\n",
      "initial_lr : 0.0001\n",
      "batch_size : 64\n",
      "epochs : 75\n",
      "epoch_chkpts : []\n",
      "device : cuda:0\n",
      "criterion : CrossEntropyLoss()\n",
      "optimizer : Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: False\n",
      "    lr: 0.0001\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "seed : None\n",
      "model_architecture : <class 'NN_Thesis.nn_classes.BNN_Resnet_UniAdapt'>\n",
      "binerised_training : True\n",
      "Number of Elements : 4547660\n",
      "Initial accuracy:\n",
      "Test Accuracy : 4.3%, Test Loss: 49.52670742447968\n",
      "best_acc.pth saved!\n",
      "Test Accuracy : 5.0%, Test Loss: 47.793893575668335\n",
      "best_acc.pth saved!\n",
      "epoch: 1 average loss: 2.301\n",
      "Test Accuracy : 48.9%, Test Loss: 1.8532273843884468\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 10.47 seconds\n",
      "epoch: 2 average loss: 1.816\n",
      "Test Accuracy : 56.4%, Test Loss: 1.6535400748252869\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 10.39 seconds\n",
      "epoch: 3 average loss: 1.675\n",
      "Test Accuracy : 60.5%, Test Loss: 1.5308730080723763\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 10.41 seconds\n",
      "epoch: 4 average loss: 1.575\n",
      "Test Accuracy : 64.3%, Test Loss: 1.450400810688734\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 10.38 seconds\n",
      "epoch: 5 average loss: 1.507\n",
      "Test Accuracy : 66.8%, Test Loss: 1.3967422470450401\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 10.47 seconds\n",
      "epoch: 6 average loss: 1.466\n",
      "Test Accuracy : 66.9%, Test Loss: 1.3592250645160675\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 10.46 seconds\n",
      "epoch: 7 average loss: 1.426\n",
      "Test Accuracy : 67.7%, Test Loss: 1.3329875655472279\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 10.48 seconds\n",
      "epoch: 8 average loss: 1.391\n",
      "Test Accuracy : 69.2%, Test Loss: 1.2738969437777996\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 10.47 seconds\n",
      "epoch: 9 average loss: 1.364\n",
      "Test Accuracy : 69.4%, Test Loss: 1.2726339735090733\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 10.45 seconds\n",
      "epoch: 10 average loss: 1.340\n",
      "Test Accuracy : 70.7%, Test Loss: 1.2438020333647728\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 10.45 seconds\n",
      "epoch: 11 average loss: 1.314\n",
      "Test Accuracy : 71.5%, Test Loss: 1.2035347186028957\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 10.79 seconds\n",
      "epoch: 12 average loss: 1.297\n",
      "Test Accuracy : 70.5%, Test Loss: 1.1921475529670715\n",
      "Epoch Time (Training + Test) = 10.89 seconds\n",
      "epoch: 13 average loss: 1.271\n",
      "Test Accuracy : 72.0%, Test Loss: 1.1835936345160007\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 10.69 seconds\n",
      "epoch: 14 average loss: 1.257\n",
      "Test Accuracy : 71.6%, Test Loss: 1.1596614625304937\n",
      "Epoch Time (Training + Test) = 10.34 seconds\n",
      "epoch: 15 average loss: 1.238\n",
      "Test Accuracy : 70.9%, Test Loss: 1.1580099947750568\n",
      "Epoch Time (Training + Test) = 10.50 seconds\n",
      "epoch: 16 average loss: 1.229\n",
      "Test Accuracy : 72.3%, Test Loss: 1.1507213432341814\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 10.48 seconds\n",
      "epoch: 17 average loss: 1.211\n",
      "Test Accuracy : 72.5%, Test Loss: 1.1297673862427473\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 10.42 seconds\n",
      "epoch: 18 average loss: 1.196\n",
      "Test Accuracy : 71.9%, Test Loss: 1.1072790510952473\n",
      "Epoch Time (Training + Test) = 10.32 seconds\n",
      "epoch: 19 average loss: 1.188\n",
      "Test Accuracy : 71.4%, Test Loss: 1.119522226974368\n",
      "Epoch Time (Training + Test) = 10.40 seconds\n",
      "epoch: 20 average loss: 1.179\n",
      "Test Accuracy : 72.8%, Test Loss: 1.0998305976390839\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 10.45 seconds\n",
      "epoch: 21 average loss: 1.161\n",
      "Test Accuracy : 73.2%, Test Loss: 1.0982958171516657\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 10.75 seconds\n",
      "epoch: 22 average loss: 1.149\n",
      "Test Accuracy : 73.2%, Test Loss: 1.0687710717320442\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 10.37 seconds\n",
      "epoch: 23 average loss: 1.139\n",
      "Test Accuracy : 73.2%, Test Loss: 1.063169401139021\n",
      "Epoch Time (Training + Test) = 10.51 seconds\n",
      "epoch: 24 average loss: 1.131\n",
      "Test Accuracy : 73.7%, Test Loss: 1.0552459936589003\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 10.46 seconds\n",
      "epoch: 25 average loss: 1.119\n",
      "Test Accuracy : 73.6%, Test Loss: 1.0515920501202345\n",
      "Epoch Time (Training + Test) = 10.47 seconds\n",
      "epoch: 26 average loss: 1.110\n",
      "Test Accuracy : 72.7%, Test Loss: 1.0541795119643211\n",
      "Epoch Time (Training + Test) = 10.43 seconds\n",
      "epoch: 27 average loss: 1.104\n",
      "Test Accuracy : 72.9%, Test Loss: 1.0234040580689907\n",
      "Epoch Time (Training + Test) = 10.44 seconds\n",
      "epoch: 28 average loss: 1.089\n",
      "Test Accuracy : 74.3%, Test Loss: 1.0154590848833323\n",
      "best_acc.pth saved!\n",
      "Epoch Time (Training + Test) = 10.43 seconds\n",
      "epoch: 29 average loss: 1.072\n",
      "Test Accuracy : 72.9%, Test Loss: 1.0276722870767117\n",
      "Epoch Time (Training + Test) = 10.36 seconds\n",
      "epoch: 30 average loss: 1.075\n",
      "Test Accuracy : 73.5%, Test Loss: 1.0104122161865234\n",
      "Epoch Time (Training + Test) = 10.33 seconds\n",
      "epoch: 31 average loss: 1.061\n",
      "Test Accuracy : 72.9%, Test Loss: 1.0087078474462032\n",
      "Epoch Time (Training + Test) = 10.46 seconds\n",
      "epoch: 32 average loss: 1.055\n",
      "Test Accuracy : 73.8%, Test Loss: 0.993908355012536\n",
      "Epoch Time (Training + Test) = 10.38 seconds\n",
      "epoch: 33 average loss: 1.048\n",
      "Test Accuracy : 74.2%, Test Loss: 1.0008622109889984\n",
      "Epoch Time (Training + Test) = 10.29 seconds\n",
      "epoch: 34 average loss: 1.043\n",
      "Test Accuracy : 73.9%, Test Loss: 0.9966563191264868\n",
      "Epoch Time (Training + Test) = 10.33 seconds\n",
      "epoch: 35 average loss: 1.043\n",
      "Test Accuracy : 74.2%, Test Loss: 0.9912078101187944\n",
      "Epoch Time (Training + Test) = 10.42 seconds\n",
      "epoch: 36 average loss: 1.018\n",
      "Test Accuracy : 73.4%, Test Loss: 0.9913077466189861\n",
      "Epoch Time (Training + Test) = 10.46 seconds\n",
      "epoch: 37 average loss: 1.017\n",
      "Test Accuracy : 73.3%, Test Loss: 0.9888039994984865\n",
      "Epoch Time (Training + Test) = 10.44 seconds\n",
      "conv2\n",
      "conv3\n",
      "conv2\n",
      "conv3\n",
      "conv2\n",
      "conv3\n",
      "Pruned [0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99, 0.8, 0.99]% of weights at epoch 37\n",
      "epoch: 38 average loss: 1.669\n",
      "Test Accuracy : 60.2%, Test Loss: 1.4215077459812164\n",
      "Epoch Time (Training + Test) = 10.35 seconds\n",
      "epoch: 39 average loss: 1.460\n",
      "Test Accuracy : 63.3%, Test Loss: 1.3468205258250237\n",
      "Epoch Time (Training + Test) = 10.31 seconds\n",
      "epoch: 40 average loss: 1.403\n",
      "Test Accuracy : 64.3%, Test Loss: 1.3130598030984402\n",
      "Epoch Time (Training + Test) = 10.57 seconds\n",
      "epoch: 41 average loss: 1.389\n",
      "Test Accuracy : 62.7%, Test Loss: 1.292608167976141\n",
      "Epoch Time (Training + Test) = 10.26 seconds\n",
      "epoch: 42 average loss: 1.373\n",
      "Test Accuracy : 64.3%, Test Loss: 1.2905049659311771\n",
      "Epoch Time (Training + Test) = 10.42 seconds\n",
      "epoch: 43 average loss: 1.354\n",
      "Test Accuracy : 65.8%, Test Loss: 1.2764052338898182\n",
      "Epoch Time (Training + Test) = 10.62 seconds\n",
      "epoch: 44 average loss: 1.349\n",
      "Test Accuracy : 66.0%, Test Loss: 1.251880917698145\n",
      "Epoch Time (Training + Test) = 10.47 seconds\n",
      "epoch: 45 average loss: 1.333\n",
      "Test Accuracy : 65.1%, Test Loss: 1.2410471066832542\n",
      "Epoch Time (Training + Test) = 10.29 seconds\n",
      "epoch: 46 average loss: 1.320\n",
      "Test Accuracy : 65.3%, Test Loss: 1.2522605061531067\n",
      "Epoch Time (Training + Test) = 10.43 seconds\n",
      "epoch: 47 average loss: 1.325\n",
      "Test Accuracy : 65.4%, Test Loss: 1.2329754885286093\n",
      "Epoch Time (Training + Test) = 10.40 seconds\n",
      "epoch: 48 average loss: 1.312\n",
      "Test Accuracy : 65.0%, Test Loss: 1.232769438996911\n",
      "Epoch Time (Training + Test) = 10.30 seconds\n",
      "epoch: 49 average loss: 1.309\n",
      "Test Accuracy : 65.4%, Test Loss: 1.2174787428230047\n",
      "Epoch Time (Training + Test) = 10.30 seconds\n",
      "epoch: 50 average loss: 1.291\n",
      "Test Accuracy : 66.3%, Test Loss: 1.1910057328641415\n",
      "Epoch Time (Training + Test) = 10.34 seconds\n",
      "epoch: 51 average loss: 1.304\n",
      "Test Accuracy : 67.2%, Test Loss: 1.2084922138601542\n",
      "Epoch Time (Training + Test) = 10.36 seconds\n",
      "epoch: 52 average loss: 1.292\n",
      "Test Accuracy : 66.0%, Test Loss: 1.2103497479110956\n",
      "Epoch Time (Training + Test) = 10.37 seconds\n",
      "epoch: 53 average loss: 1.288\n",
      "Test Accuracy : 65.5%, Test Loss: 1.1869170237332582\n",
      "Epoch Time (Training + Test) = 10.29 seconds\n",
      "epoch: 54 average loss: 1.284\n",
      "Test Accuracy : 67.4%, Test Loss: 1.187783744186163\n",
      "Epoch Time (Training + Test) = 10.34 seconds\n",
      "epoch: 55 average loss: 1.271\n",
      "Test Accuracy : 66.8%, Test Loss: 1.1856135837733746\n",
      "Epoch Time (Training + Test) = 10.38 seconds\n",
      "epoch: 56 average loss: 1.275\n",
      "Test Accuracy : 66.1%, Test Loss: 1.2166213560849428\n",
      "Epoch Time (Training + Test) = 10.32 seconds\n",
      "epoch: 57 average loss: 1.270\n",
      "Test Accuracy : 65.8%, Test Loss: 1.1979326959699392\n",
      "Epoch Time (Training + Test) = 10.37 seconds\n",
      "epoch: 58 average loss: 1.265\n",
      "Test Accuracy : 66.1%, Test Loss: 1.208151975646615\n",
      "Epoch Time (Training + Test) = 10.37 seconds\n",
      "epoch: 59 average loss: 1.264\n",
      "Test Accuracy : 64.8%, Test Loss: 1.2038591857999563\n",
      "Epoch Time (Training + Test) = 10.28 seconds\n",
      "epoch: 60 average loss: 1.256\n",
      "Test Accuracy : 65.8%, Test Loss: 1.1792550291866064\n",
      "Epoch Time (Training + Test) = 10.42 seconds\n",
      "epoch: 61 average loss: 1.262\n",
      "Test Accuracy : 66.0%, Test Loss: 1.1763394642621279\n",
      "Epoch Time (Training + Test) = 10.32 seconds\n",
      "epoch: 62 average loss: 1.259\n",
      "Test Accuracy : 65.9%, Test Loss: 1.1837735828012228\n",
      "Epoch Time (Training + Test) = 10.27 seconds\n",
      "epoch: 63 average loss: 1.256\n",
      "Test Accuracy : 65.2%, Test Loss: 1.185876740142703\n",
      "Epoch Time (Training + Test) = 10.37 seconds\n",
      "epoch: 64 average loss: 1.244\n",
      "Test Accuracy : 66.8%, Test Loss: 1.161489725112915\n",
      "Epoch Time (Training + Test) = 10.35 seconds\n",
      "epoch: 65 average loss: 1.240\n",
      "Test Accuracy : 65.3%, Test Loss: 1.1870012804865837\n",
      "Epoch Time (Training + Test) = 10.27 seconds\n",
      "epoch: 66 average loss: 1.239\n",
      "Test Accuracy : 67.2%, Test Loss: 1.1754757519811392\n",
      "Epoch Time (Training + Test) = 10.42 seconds\n",
      "epoch: 67 average loss: 1.233\n",
      "Test Accuracy : 66.0%, Test Loss: 1.1761312559247017\n",
      "Epoch Time (Training + Test) = 10.48 seconds\n",
      "epoch: 68 average loss: 1.238\n",
      "Test Accuracy : 66.0%, Test Loss: 1.1809830069541931\n",
      "Epoch Time (Training + Test) = 10.30 seconds\n",
      "epoch: 69 average loss: 1.226\n",
      "Test Accuracy : 66.8%, Test Loss: 1.153961779549718\n",
      "Epoch Time (Training + Test) = 10.42 seconds\n",
      "epoch: 70 average loss: 1.232\n",
      "Test Accuracy : 66.2%, Test Loss: 1.1573265884071589\n",
      "Epoch Time (Training + Test) = 10.31 seconds\n",
      "epoch: 71 average loss: 1.229\n",
      "Test Accuracy : 67.0%, Test Loss: 1.1584334131330252\n",
      "Epoch Time (Training + Test) = 10.30 seconds\n",
      "epoch: 72 average loss: 1.219\n",
      "Test Accuracy : 66.3%, Test Loss: 1.1461952421814203\n",
      "Epoch Time (Training + Test) = 10.40 seconds\n",
      "epoch: 73 average loss: 1.224\n",
      "Test Accuracy : 67.0%, Test Loss: 1.1405848562717438\n",
      "Epoch Time (Training + Test) = 10.34 seconds\n",
      "epoch: 74 average loss: 1.228\n",
      "Test Accuracy : 65.0%, Test Loss: 1.179587872698903\n",
      "Epoch Time (Training + Test) = 10.38 seconds\n",
      "epoch: 75 average loss: 1.221\n",
      "Test Accuracy : 65.7%, Test Loss: 1.1675816904753447\n",
      "Epoch Time (Training + Test) = 10.38 seconds\n",
      "Data Saved to BNN_UniAdapt_finetune_thinblock3_99%prune_conv2only.csv\n",
      "Finished Training: \n",
      "Total Time 0.433832 hours\n",
      " Average Time Per Epoch 20.82 seconds\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(BNN,model_name='BNN_UniAdapt_finetune_thinblock3_99%prune_conv2only',project_name='UniAdapt_Cifar100_Finetuning',binarise=True,classes=train_20.classes)\n",
    "trainer.lr = 1e-4\n",
    "trainer.batch_size = 64\n",
    "trainer.epochs =75\n",
    "trainer.epoch_chkpts = []\n",
    "trainer.start_epoch = 0\n",
    "trainer.amount = [0.8,0.99]\n",
    "# trainer.pruning_rounds = 1 # Only need this for iterative pruning\n",
    "trainer.model_to_prune = trainer.model.uniAdaptNet[0]\n",
    "params = nn.ModuleList([BNN.uniAdaptNet,BNN.head_bn,BNN.head]).parameters()\n",
    "# params = nn.ModuleList([BNN.head_bn,BNN.head]).parameters()\n",
    "trainer.set_scheduler(None)\n",
    "trainer.set_optimizer(torch.optim.Adam,params,lr = trainer.lr)\n",
    "trainer.train(train20_DL,test20_DL,prune_strat='oneshot')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "11938c6bc6919ae2720b4d5011047913343b08a43b18698fd82dedb0d4417594"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
